\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage[utf8x]{inputenc}
\usepackage{multirow}
\usepackage{url}
\usepackage{amsmath}

\begin{document}
%
\title{Using~HFST~for~Creating Computational~Linguistic~Applications}
%
\author{Krister Lind\'{e}n \and Erik Axelson \and Senka Drobac \and\\
  Sam Hardwick \and Miikka Silfverberg \and Tommi A Pirinen }

\institute{University of Helsinki\\
  Department of Modern Languages\\
  Unioninkatu 40 A\\
  FI-00014 Helsingin yliopisto, Finland\\
  \email{\{krister.linden, erik.axelson, senka.drobac, sam.hardwick,\\
    miikka.silfverberg,tommi.pirinen\}@helsinki.fi}}

\maketitle

\begin{abstract}
  \sloppy HFST – Helsinki Finite-State Technology (\url{hfst.sf.net})
  is a framework for compiling and applying linguistic descriptions
  with finite-state methods. HFST currently connects some of the most 
  important finite-state tools for creating morphologies and spellcheckers into 
  one open-source platform and supports extending and improving 
  the descriptions with weights to accommodate the modeling of 
  statistical information. HFST offers a path from language descriptions 
  to efficient language applications.

  \keywords{finite-state applications, morphology, tagging, HFST}
\end{abstract}

\section{Introduction}
%Krister

HFST – Helsinki Finite-State Technology (\url{hfst.sf.net}) is designed 
for creating and compiling morphologies, which has been documented in, e.g.,  
\cite{linden/2009/sfcm,linden/2011/sfcm}. In this article we focus on the applications 
created with HFST and some of their theoretical motivations. HFST
contains open-source replicas of xfst, lexc and twolc which are
well-known and well-researched tools for morphology building, see \cite{beesley/2003}. The tools
support both parallel and cascaded application of transducers. 
This is outlined in Section~\ref{LexiconTools} along with some new and 
previously undocumented extensions to the Xerox tools in HFST.  

There are a number of tools for describing morphologies. Many of them 
start with the item-and-arrangement approach in which an arrangement of 
sublexicons contain lists of items that may continue in other sublexicons. A formula
for compiling such lexical descriptions was documented in \cite{linden/2009/sfcm}.
In Section~\ref{MorphTools}, we demonstrate a general process for how such morphotactic descriptions 
can be compiled into finite-state lexicons using finite-state operations. 
To realize the morphological processes, rules may be applied to the finite-state lexicon. 
In addition, HFST offers the capability to train and apply part-of-speech
taggers on top of the morphologies using parallel weighted finite-state transducers, 
which is outlined and evaluated in Section~\ref{PosTools}.

Using compiled morphologies, a number of applications
have been created, e.g. spellcheckers for close to 100 languages and
hyphenators for approximately 40 different languages. The spellcheckers were
derived from open-source dictionaries and integrated with OpenOffice and
LibreOffice, e.g. a full-fledged Greenlandic spellchecker, which is a
polyagglutinating language, is currently available for OpenOffice via HFST. 
By adding the tagger capability, we have also created an
improved spelling suggestion mechanism for words in context.
The spellchecker applications and some of their theoretical underpinnings are described 
in Section~\ref{Applications}.

Finally  in Section~\ref{Discussion}, we discuss some additional applications such as synonym and translation dictionaries 
as well as a framework for recognizing multi-word expressions for information extraction and how this can be created 
using finite-state technologies.

\section{Building Morphologies}\label{LexiconTools}


%Miikka

One of the earliest and most important goals of the HFST project has
been to provide open-source utilities for compiling full-fledged
morphological analyzers, which can be used in constructing spellcheckers,
taggers and other language technological utilities of good
quality. The Xerox toolkit \cite{beesley/2003} is among the most
widely used frameworks for constructing morphological utilities. The
toolkit is used to compile linguistic description into morphological
analyzers. More specifically, Xerox tools include the finite-state
lexicon compiler lexc, the two-level rule compiler \verb|twolc| and the
cascading replace rule compiler xfst. HFST includes the tools
\textbf{hfst-lexc}, \textbf{hfst-twolc} and \textbf{hfst-xfst}, which provide full backward
compatibility with Xerox tools and augment their functionality.

Lexicon files for the lexicon compiler \textbf{hfst-lexc} describe the morphotactics of a
language using sub-lexicons containing lists of morphs. A rule
component can be used for realizing phonological changes occurring 
within the morphs or at their boundaries.

The rule compilers \textbf{hfst-xfst} and \textbf{hfst-twolc} provide almost the same
functionality. Both are used to realize morphophonological variations
on a lexicon compiled with \textbf{hfst-lexc}. The difference between the tools is that
\textbf{hfst-xfst} rules are applied in succession gradually altering the
lexicon whereas \textbf{hfst-twolc} rules are applied as parallel constraints limiting
the realizations of morphophonemes used in the \textbf{hfst-lexc} lexicon.

\subsection{Parallel Rules with Negative Contexts and Regular Expression Centers}

The rule compiler \textbf{hfst-twolc} provides backward compatibility with the
Xerox tool twolc, but it augments the twolc functionality by providing two
new types of rules. In \textbf{hfst-twolc}, rules can have centers which are
regular expressions and can have negative contexts.
\begin{figure}
{\footnotesize
\begin{verbatim}
! Change x to y after the string "a b" unless "d" or 
! "c d" follows.
"Rule with negative context"
x:y <=> a b _     ;
        except
            _ ( c ) d ;

"Rule without negative context"
x:y <=> a b _ [ ? - [ c | d ] | c [ ? - d ] | .#. ] ;
\end{verbatim}
}
\caption{Negative context rule and corresponding traditional two-level
  rule}\label{negative-context-rule}
\end{figure}

In traditional two-level rules, it can be very difficult to express
that a certain alternation should not occur in a given
context. Sometimes conflict resolution~\cite{silfverberg/2009/2} of
two-level rules handles this, but conflict resolution works only if
rules can be ordered into chains of sub cases. Often this can be
difficult to accomplish, especially for grammar writers who are not
especially well acquainted with writing regular expressions.

Using an extension to the Xerox two-level rule syntax in
\verb|hfst-twolc|, it is possible to formulate rules with negative
context, i.e. context which prohibit the triggering of the
rule. Figure~\ref{negative-context-rule} shows a schematic example of
a negative context rule and a traditional Xerox style rule, which has
the same effect as the rule with the negative context.

Rules with negative contexts are used in the Kyrgyz morphology
developed in the Apertium project~\footnote{http://www.apertium.org/}.
They significantly shorten the two-level
grammar~\footnote{Personal communication with Francis Tyers.}.

\subsection{Cascaded Rules: Explanations and Examples}
% Senka

HFST replace rules provide backward compatibility with XFST replace rules, 
described in \cite{Kempe96parallelreplacement}\cite{beesley/2003}. 
Although they share the same notation, the HFST replace rules differ in that
they were developed from the concept of Generalized Lenient Composition, 
described in \cite{ylijyra/2008b}.

We present here a general account of using replace rules with HFST.
for a detailed description of each rule, see
The Finite State Morphology book \cite{beesley/2003}.

% Simple rules

\textbf{Simple rules.} A simple right arrow replace rule,
\begin{equation}
  A \rightarrow\ B
\end{equation}

where A and B are regular expressions, 
expresses that A in the upper language maps to B in the lower language.

Replace rules can be compiled into a transducer using
\verb!hfst-regexp2fst!. This tool takes a regular expresion as input 
and gives a corresponding transducer written in a binary file as output. 
To convert transducers from binary format to text (ATT) format, 
there is an HFST tool \verb!hfst-fst2txt!. Therefore, the upper rule 
could be compiled as in Figure~\ref{fig:simple_replace}.

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo "A -> B ;" | hfst-regexp2fst | hfst-fst2txt
\end{verbatim}
}
\caption{Compiling simple replace rule}
\label{fig:simple_replace}
\end{figure}


The regular expression can be read from a file, or saved to a file (Figure~\ref{fig:read_from_file}). 

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo "A -> B ;" > regex
$ hfst-regexp2fst -i regex -o transducer
$ hfst-fst2txt transducer
\end{verbatim}
}
\caption{Reading from, and writing to a file}
\label{fig:read_from_file}
\end{figure}


Rules can be applied to any language by using composition.  By
composing a word \verb!ABCD! with the previous rule \verb!A -> B!, the
result is the transducer in Figure~\ref{fig:replace_compose}.  It has
5 states (0 -- 4), and only state 4 is final. The default output of
\verb!hfst-regexp2fst! is weighted. Since weights are
not used in replace rules, the weights get the value 0.0 when
compiled. 

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo "A B C D .o. A -> B ;" | hfst-regexp2fst | hfst-fst2txt
0       1       A       B       0.000000
1       2       B       B       0.000000
2       3       C       C       0.000000
3       4       D       D       0.000000
4       0.000000
\end{verbatim}
}
\caption{Composing rule to a word}
\label{fig:replace_compose}
\end{figure}


%\begin{figure} [h!]
%{\footnotesize
%\begin{verbatim}
%$ echo "c a t .o. c a t -> c a t s  ;" |
%> hfst-regexp2fst -f sfst | hfst-fst2txt
%0       1       c       c
%1       2       a       a
%2       3       t       t
%3       4       @0@     s
%4
%% \end{verbatim}
%% }
%% \caption{Using SFST transducer format}
%% \label{fig:sfst_format}
%% \end{figure}


% Context

\textbf{Context.} Every rule can have a context in which the replacement is made. 
Here, A will be mapped to B if and only if it is between regular expressions
L and R.
\begin{equation}
  A \rightarrow\ B\ ||\ L \_\  R
\end{equation}

Also, multiple context pairs are supported, when separated by comma.
\begin{equation}
  A \rightarrow\ B\ ||\ L_1 \_\  R_1 ,\ \ldots\ ,\ L_i \_\  R_i
\end{equation}

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo " m a n a a b .o. a -> b || m _ n , _ b;" |
hfst-regexp2fst | hfst-fst2txt
0       1       m       m       0.000000
1       2       a       b       0.000000
2       3       n       n       0.000000
3       4       a       a       0.000000
4       5       a       b       0.000000
5       6       b       b       0.000000
6       0.000000
\end{verbatim}
}
\caption{Replace rule between two contexts}
\label{fig:multiple_contexts}
\end{figure}

In Figure~\ref{fig:multiple_contexts}, the rule expression says that \verb!a! in the 
upper language is mapped to \verb!b! in lower language 
when between \verb!m! and \verb!n!, or in front of \verb!b!. 
In the word \verb!manaab!, the first and last \verb!a! are mapped to \verb!b!, 
because they occur between corresponding contexts, but the second \verb!a! is kept unchanged.


\begin{table} [h!]
  \centering
  \caption{Different context directions in Replace Rules}
  \begin{tabular}{ | c | c | }
    \hline
    \ \verb!||!\ & \ both contexts are taken from the upper language \\ \hline
    \ \verb!//!\ & \ the left context is taken from the lower language, the right from the upper \\ \hline
    \ \verb!\\!\ & \ the left context is taken from the upper language, the right from the lower \\ \hline
    \ \verb!\/!\ & \ both contexts are taken from the lower language. \\ \hline
  \end{tabular}
  \label{tab:context_directions}
\end{table}

In a replace rule where A is the upper and B the lower language, 
there are four contextual directions that can be used with replace rules.
For example, when the \ \verb!//!\ sign is used as context orientation operator, 
the left context will be taken from the lower language. 
It is thus possible for the replace function to write its own context. 
This is shown in Figure~\ref{fig:context_orientation}. 

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
  $ echo " b a a .o. a -> b // b _ ;" | ./hfst-regexp2fst | hfst-fst2txt
  0       1       b       b       0.000000
  1       2       a       b       0.000000
  2       3       a       b       0.000000
  3       0.000000
\end{verbatim}
}
\caption{Replace rule writes its own context}
\label{fig:context_orientation}
\end{figure}

% Other replace functions

\textbf{Other replace functions.} We have hitherto only used the right arrow replace operator,
but there are many other operators that can be used. 
All the operators listed in Table~\ref{tab:replace_operators} have their left arrow version, 
which is the inversion of the right operator. Furthermore, all the rules can be used 
with epenthesis \verb![. .]! and markup \verb![...]! operators. 
The epenthesis operator should be used with empty strings to avoid replacing infinitely 
many epsilons, while the markup operator is used to insert markers around a word.

\begin{table} [h!]
  \centering
  \caption{List of right replace operators that can be used in HFST}
  \begin{tabular}{| c | c |} 
    \hline
    Right replace operators & Replace function \\ \hline\hline
    \verb!->!   & Replace \\ \hline
    \verb!(->)! & Replace optional \\ \hline
    \verb!@->!  & Longest match from left to right \\ \hline
    \verb!->@!  & Longest match from right to left \\ \hline
    \verb!@>!   & Shortest match from left to right \\ \hline
    \verb!>@!   & Shortest match from right to left \\ \hline
  \end{tabular}
  \label{tab:replace_operators}
\end{table}



\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
  $ echo " a a a .o. [.a*.] @-> b ;" | hfst-regexp2fst > a.fst
  $ echo " b a a b .o. a @-> %[ ... %] ;" | hfst-regexp2fst > b.fst
\end{verbatim}
}
\caption{Epenthesis and markup operators}
\label{fig:epenthesis_markup}
\end{figure}


% Parallel rules

\textbf{Parallel rules.} Parallel rules are used when there are 
multiple replacements at the same time. The general parallel replace rule expression 
consists of individual replace rules separated by two commas.
\begin{equation}
  A_1 \rightarrow\ B_1\ ||\ L_1 \_\  R_1\ ,,\ \ldots\ ,,\ A_i \rightarrow\ B_i\ ||\ L_i \_\ R_i
\end{equation}

In XFST, all rules in a parallel rule expression have to have the same format. 
They need to have the same arrow and the same context layout. 
In HFST, the constraint that all the rules should have the same arrow is kept, 
but the context formats can freely differ. 
Therefore, the rule expression in Figure~\ref{fig:parallel_rules} 
would not be allowed in XFST, but is valid in HFST.
\begin{figure}
{\footnotesize
\begin{verbatim}
  $  echo " a -> b || m _ n ,, c -> d ;" | ./hfst-regexp2fst | hfst-fst2txt
\end{verbatim}
}
\caption{Parallel rules with different contexts}
\label{fig:parallel_rules}
\end{figure}

\section{Morphological Descriptions}\label{MorphTools}


% Krister

Popular formalisms for describing the morphotactics of a language tend to be some variation 
of the item and arrangement scheme. We build on this to generalize from \verb!lexc! into other
item-and-arrangement notations such as Hunmorph and Apertium. This gives us the option to describe 
morpholgoy in a notation that is compiled into finite-state transducers which we can continue to process with other
HFST tools. We demonstrate how compilation could proceed when reducing a \verb!lexc! lexicon into a sequence of
finite-state operations. A morphological programming language like \verb!SFST-PL! forgoes a standard lexicon interface
like \verb!lexc! and only offers the end user an option to construct the lexicon using finite-state operations. 

\subsection{Morphotax and Morphological Formul\ae}
%Tommi & Krister

Morphotax is the component dealing with morphological combinations in language
description. This concerns word-formation processes, such as affixation and
compounding.  There are numerous formalisms for describing morphotactics in natural
language processing applications, such as
hunspell\footnote{\url{http://hunspell.sf.net}} (and its older *spell
relatives), the Apertium lttoolbox\footnote{\url{http://apertium.sf.net}} or
the Xerox lexc\cite{beesley/2003}. The HFST toolset contains parsers and
compilers for reading descriptions of morphologies written in these formalisms,
which can be used for compiling a finite-state automaton for the other HFST
tools to process \cite{pirinen/2010/il,linden/2009/sfcm}.

Consider a trivial lexicon consisting of the English nouns \emph{cat} and
\emph{ax} with the empty string as the singular marker and \emph{s} and
\emph{es} as plural markers, respectively,  forming a lexicon as outlined in
Figure~\ref{fig:morph0} using lexc notation.

\begin{figure} [h]
{\footnotesize
\begin{verbatim}
  LEXICON Root
  Nouns;

  LEXICON Nouns
  cat NumberS;
  ax NumberES;

  LEXICON NumberS
  END;
  s END;

  LEXICON NumberES
  END;
  es END;
\end{verbatim}
}
\caption{Lexicon in lexc notation}
\label{fig:morph0}
\end{figure}

Many contemporary notations for describing morphotactics tend to use the same item-and-arrangement paradigm for
combining sets of morphs or morphemes, i.e. they use sublexicons for lists of morphs that may 
have continuations in other sublexicons. 
In its most general form, such a paradigm can be defined using a combination of two components expressing local restrictions:
(1) a disjunction of morphs in local lexical context repeated infinitely and 
(2) a morphotactic filter describing the permitted sequence of sublexicons.
Both the disjunction of morphs and the morphotactic filter can be described in finite-state form.

In Figure~\ref{fig:morphology1}, we demonstrate how the lexicon in
Figure~\ref{fig:morph0} is compiled into a loop of a disjunction of any of the
morphs. Note that we customarily encode special symbols in at-signs `@'. In
this case we use special joiner symbols to calculate morphotactics, and
at-signs give end user a hint that they are not supposed to end up in the final
result. In naming, the \texttt{@NumberES@} style markers are used to match up
the morphs, and \texttt{@LEX@} to bound morphemes, which simplifies the filter
algorithm a bit\footnote{As in \cite{linden/2009/sfcm} the algorithm without
boundaries requires a term complement of the disjunctive closure of the special
symbols.}.

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
  $ echo "@Root@ @Nouns@ @LEX@" > strings
  $ echo "@Nouns@ c a t @NumberS@ @LEX@" >> strings
  $ echo "@Nouns@ a x @NumberES@ @LEX@" >> strings
  $ echo "@NumberS@ @END@ @LEX@" >> strings
  $ echo "@NumberS@ s @END@ @LEX@" >> strings
  $ echo "@NumberES@ @END@ @LEX@" >> strings
  $ echo "@NumberES@ e s @END@ @LEX@" >> strings
  $ hfst-strings2fst --has-spaces --disjunct-strings < strings  | 
  hfst-repeat -f 1 > bag_of_morphs
\end{verbatim}
}
\caption{Compiling a disjunction of morphs}
\label{fig:morphology1}
\end{figure}

In Figure~\ref{fig:morphology2}, we show the command-line simulation for
creating the morphotactic filter and how this filter is composed with the
disjunction of morphs to create the lexicon. In effect we here create filter
that ensures that e.g. the \emph{es} morph only follows the morph that
was asking for it on right, i.e. we match the \texttt{@NumberES@} on both
sides. For full algorithm refer to e.g.~\cite{linden/2009/sfcm}.

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
  $ echo "%@Root%@ [\%@LEX%@]*"  |  hfst-regexp2fst > start
  $ echo "%@END%@ %@LEX%@"  |  hfst-regexp2fst > end

  # We create one continuation restriction for each lexicon
  $ echo "%@NumberS%@ %@LEX%@ %@NumberS%@ [\%@LEX%@]*"  |  hfst-regexp2fst > cont1
  $ echo "%@NumberES%@ %@LEX%@ %@NumberES%@  [\%@LEX%@]*"  |  hfst-regexp2fst > cont2

  # All restrictions are disjuncted and then anchored globally
  $ hfst-disjunct cont1 cont2  |  hfst-repeat -f 1 > conts
  $ hfst-concatenation start conts end > morphotactics

  # Local morphotactic restrictions together with locally anchored morphs 
  # create the lexicon
  $ hfst-compose bag_of_morphs  morphotactics > lexicon
\end{verbatim}
}
\caption{Composing the morphotactics with the disjunction of morphs into a lexicon}
\label{fig:morphology2}
\end{figure}

Since many morphotactic formalisms tend to be some variation of the item and arrangement scheme,
we can build on this to generalize into other notations, since the morphotactics itself
does not depend on the description language of the morphotactics. 
This also makes additions like compound-based weighting of the language-model 
\cite{linden/2009/fsmnlp} generally applicable.

\subsubsection{For Lexicographers} this approach gives the option to
choose their favorite notation, e.g. hfst-lexc, Hunspell or Apertium and then continue with other
HFST tools. In theory, it would also be possible to use this morphotactic formula
to write implementations for any item-and-arrangement morphology, but in practice
it's easier to simply use some high-level scripting language to convert a lexical database into 
e.g. lexc notation.

\subsection{Performance}
% Erik

The HFST toolkit has been implemented using three back-end libraries:
SFST, OpenFst and foma. Usually one library is chosen
and used throughout the compilation of a given morphology.
In this way, we can compare how different back-end libraries perform
in the same task.

In Table~\ref{tab:compilation_times}, we show compilation times for different 
morphologies using different HFST back-ends. The morphologies are OMorFi 
\cite{pirinen/2008} for Finnish, Morphisto \cite{zielinski/2009} for German,
Morph-it \cite{Zanchetta/2005} for Italian, Swelex 
\footnote{\url{https://kitwiki.csc.fi/twiki/bin/view/KitWiki/HFSTSwelex}} 
for Swedish and TrMorph \cite{Coltekin/2010} for Turkish. 
OMorFi, Morphisto and TrMorph have several rules for
inflection and compounding, Morph-it and Swelex are basically word-lists.
We use HFST version 3.3.4, with SFST, OpenFst and foma as
back-ends. The times are averages from runs of 10 compilations.

\begin{table} [h!]
  \centering
  \caption{Compilation times for different morphologies with
    different HFST back-ends. The times are given in minutes and seconds
    and averaged over 10 compilations. HFST version is 3.3.4.}
  \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    Back-End & Finnish & German & Italian & Swedish & Turkish \\ \hline\hline
    SFST & 2:48 & 2:12 & 0:30 & 0:13 & 0:12 \\ \hline
    OpenFst & 7:52 & 7:45 & 2:24 & 0:49 & 0:40 \\ \hline
    foma & 1:52 & 1:33 & 0:31 & 0:13 & 0:05 \\ \hline
  \end{tabular}
  \label{tab:compilation_times}
\end{table}


In Table~\ref{tab:compilation_times_versions}, we show both the current compilation 
times and the ones that we achieved in an earlier benchmarking \cite{linden/2011/sfcm} for 
Finnish and German. We also show the back-end versions used.

\begin{table} [h!]
  \centering
  \caption{Compilation times for different morphologies with
    different HFST back-ends and their versions. 
    The times are given in minutes and seconds.}
  \begin{tabular}{| c | c | c | c |}
    \hline
    Back-End                 & version  & Finnish  & German \\ \hline\hline
    \multirow{2}{*}{SFST}    & 1.4.2    & 5:02     & 6:39 \\
    & 1.4.6    & 2:48     & 2:12 \\ \hline
    \multirow{2}{*}{OpenFst} & 1.2.7    & 6:51     & 6:28 \\
    & 1.2.10   & 7:52     & 7:45 \\ \hline
    \multirow{2}{*}{foma}    & 0.1.14   & 1:49     & 1:29 \\
    & 0.1.16   & 1:52     & 1:33 \\
    \hline
  \end{tabular}
  \label{tab:compilation_times_versions}
\end{table}

It can be clearly seen that the performance of HFST with SFST as a back-end 
has improved: the compilation time of the Finnish morphology has almost
halved and the compilation time of the German morphology is only one third of the
time at the previous benchmarking. 
This improvement comes from the newer version of SFST that features more optimized
composition and Hopcroft minimization functions. The improved functions 
were developed by Helmut Schmid in cooperation with the HFST team.

% The compilation times of HFST with OpenFst as back-end have become slightly slower.
% We are still looking into this fact trying to find out whether
% the problem lies in the HFST code or if we are not using the features of 
% OpenFst's newer version right.
% The performance of HFST with foma has stayed almost the same, only a small
% growth of a couple of seconds can be seen.


\section{Building Taggers}\label{PosTools}


% Miikka

The HFST library includes tools for constructing statistical
part-of-speech (POS) taggers which resemble Hidden Markov Models (HMM) from
tagged training data. HFST taggers differ from other HMM taggers such
as Tnt~\cite{Brants:2000} and Hunpos~\cite{Halascy:2007} in that HFST
allows combining different estimates for tag probabilities during
tagging. It is possible to use e.g. the surrounding word forms in
estimating the probability of a given tag. This is more thoroughly
explained in \cite{silfverberg/2010/icetal,silfverberg/2011/nodalida} The
accuracy for basic HMM taggers implemented using HFST tools is
comparable to the accuracy of Tnt and Hunpos as demonstrated below.

HMM-type taggers include a lexical model and a tag sequence model. The
lexical model is needed for determining probabilities for the
co-occurrence of tags and word forms disregarding context, and the tag
sequence model is used for determining the probabilities for the
co-occurrence of tags of neighboring words. The lexical models in
HFST taggers include suffix guessers which can be modified to suit
the needs of particular languages. Additionally HFST supports using
morphological analyzers in tagging.

\subsection{The structure of HFST Taggers}

HMM taggers are statistical models which determine the most likely
POS tag from some tag set for each word in a sentence. For
determining the most likely tags, the taggers use
lexical probabilities ${\rm p}(w|t)$ for each word $w$ and each tag
$t$ together with \emph{transition probabilities} for the tag of a
word at a given position given the tags of the preceding words ${\rm
  p}(t_i | t_{i - 1},\ ...\, t_{i-n})$. The integer $n$ determines how
many preceding tags are considered when estimating the probability of
tags. It is called the \emph{order} of the HMM tagger. Second order HMM
taggers are the most common in POS tagging.

HFST taggers extend traditional HMM taggers by allowing modifications
to the traditional estimate ${\rm p}(t_i | t_{i - 1},\ ...\,
t_{i-n})$. Together with preceding tags, succeeding tags and word
forms can be used when deciding the probability of a tag at a given
position. These estimates are combined using finite-state calculus.

In HFST taggers the lexical probabilities ${\rm p}(w|t)$ are given by
the lexical component of the tagger and the transition probabilities
${\rm p}(t_i | t_{i - 1},\ ...\, t_{i-n})$ are given by the sequence
component of the tagger. Both models are trained using tagged
training data.

Both components can be modified to suit the needs of a particular
language. In \cite{silfverberg/2011/nodalida} it is explained how the sequence
model can be modified to include preceding and succeeding words in
the estimates of the probabilities of tags and how suffix guessers
can be modified to better suit agglutinative languages. In the present paper
we show how a morphological analyzer can be integrated with an HFST
tagger to improve the accuracy of the tagger when there are a lot of
out-of-vocabulary words.


\subsection{The Lexical Model of HFST Taggers}
% Miikka

The accuracy of traditional POS taggers suffers greatly
because of out-of-vocabulary (OOV) words, i.e. word forms which were
not observed during training of the tagger. For languages like
English with few morphological phenomena and where compound words are written separately, this
is not a big problem when the genres of the training data and the data
that is tagged are sufficiently similar. When the genres differ
considerably there are more OOV words and consequently accuracy
is reduced. When building taggers for morphologically rich languages
such as Turkish, Finnish or Hungarian OOV words become a major
problem even when no change of genre is involved. In these cases
OOV words arise from inflection, derivation and
compounding.

HFST taggers offer two ways to tackle the problem of OOV words: (1) the
tagger builder can adjust the way the probability distribution for
different length suffixes are combined to form estimates for the
probabilities of tags, and (2) the tagger builder has the option to combine taggers with
morphological analyzers.

E.g. in Finnish the suffix guesser proposed by \cite{Brants:2000} gives
poor results. Such a suffix guesser computes tag probabilities for
each suffix of a word which is shorter than a given threshold and
combines the estimates to give a probability for the OOV word and
each tag. In \cite{silfverberg/2011/nodalida}, it was observed that a guesser
which only uses the longest suffix of the OOV word found in the
training data gives better results for Finnish. It thus appears that the type of suffix
guesser which gives the best results is language dependent. HFST
allows for changing the way the suffix probabilities are combined.

Research in tagging morphologically complex languages has utilized
morphological analyzers e.g. \cite{Tzoukerman:1996,Oravecz:2002}, but
the tag set of the POS tagger and the tag set of the
morphological analyzer has remained the same. In such approaches, a
lexical model is constructed from the analysis sets given by the
morphological analyzer for each word. E.g. the English word "man" is
ambiguous between the interpretation singular noun "+Noun+Sg" and
infinitive verb "+Verb+Inf". In the training data, each occurrence of
the word "man" could thus be substituted by the set $S_{man} = \{$
"+Noun+Sg", "+Verb+Inf" $\}$. The lexical model would thus give
probabilities ${\rm p}(S_{man}|t)$ for each tag $t$. 

Previous research has focused on the case where the tag set of the
morphological analyzer and the tag set of the training corpus are the
same. We note here that this is not necessary.
It is still possible to train a lexical model based on
ambiguity classes. When an OOV word is encountered, its ambiguity
class is constructed using the morphological analyzer and the lexical
probabilities of the ambiguity class are retrieved.

It is a practical problem if the tag sets are required to be the
same, since conventions for tagging a
language vary and there is not always a straightforward conversion
between the tag set of the training corpus and the tag set of the
POS tagger. Such a conversion can be difficult if the tag
set of either resource is coarser than that of the other.

\subsection{Experiments with HFST taggers}

We demonstrate HFST taggers by constructing POS taggers for Finnish
and English. For English we construct a regular second order HMM
tagger. For Finnish we construct two taggers: one regular second
order HMM tagger and another second order HMM tagger which utilizes
the OMorFi morphological analyzer for Finnish~\cite{pirinen/2008}.

The English training data and test data come from the Penn
Treebank. Sections 1 to 18 were used for training and sections 22 to
24 for testing. The Finnish training and test data come from Finnish
newspaper text which has been automatically tagged using the
Textmorfo parser~\footnote{http://www.csc.fi/kielipankki/}. The data
is described in Table~\ref{data-taggers}.

\begin{table}
  \caption{Data used for training and testing the English and Finnish taggers.}\label{data-taggers}
  \begin{center}
    \begin{tabular}{lrrr}
      \hline
      Language       & ~Training data size (tokens)~& Test data size (tokens)~& Distinct tags\\
      \hline
      English        &   912,344~~~~~~~~~~~~~~    & 129,654~~~~~~~~~~ &  45~~~~~~~ \\
      Finnish        & 1,027,511~~~~~~~~~~~~~~    & 156,572~~~~~~~~~~ & 764~~~~~~~ \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

As Table~\ref{eng-tagging-acc} shows, HFST taggers achieve comparable
results to the well known second order HMM tagger TNT. The accuracy
for unknown words is slightly worse using the HFST tagger, but for
known words the accuracies are nearly identical.

\begin{table}
  \caption{Results for second order HMM taggers of English. The accuracy
    figures for TNT can be found in~\cite{Halascy:2007}.}\label{eng-tagging-acc}
  \begin{center}
    \begin{tabular}{lccc}
      \hline
      Model       & ~~~~Seen & ~~~~Unseen & ~~~~All \\
      \hline
      TNT         & 96.77\%  &    85.19\% & 96.46\% \\
      Basic HFST  & 96.68\%  &    80.71\% & 96.23\% \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

For Finnish, the accuracy for the basic second order HMM tagger is
poorer than for English as seen in Table~\ref{fin-tagging-acc}. This
is mostly caused by words in the test data that are
missing from the training data, i.e. OOV words. In the Finnish test
data 11.51\% of the words are OOV words. For comparison, only 2.81\%
of the words in the English test data are OOV words.

To reduce the number of OOV words in the Finnish test data, a
morphological analyzer was used as explained above. Using the
morphological analyzer, only 2.73\% of the words in the test data
were OOV words. Consequently a significant increase in total tagging
accuracy is seen. See Table~\ref{fin-tagging-acc}. The increase is
negligible for known words, but significant for unknown words.

\begin{table}
  \caption{Results for Finnish taggers. The model Basic HFST is a
    regular second order HMM tagger. The model With Morph HFST is
    augmented with a morphological analyzer.}\label{fin-tagging-acc}
  \begin{center}
    \begin{tabular}{lccc}
      \hline 
      Model            & ~~~~Seen & ~~~~Unseen & ~~~~All \\
      \hline 
      Basic HFST       &  97.51\% &    77.51\% & 95.23\% \\
      With Morph HFST  &  97.53\% &    83.65\% & 95.90\% \\
      \hline
    \end{tabular}
  \end{center}
\end{table}


\section{Transducer applications}\label{Applications}


% Tommi

Automata technology is a general framework for describing language models
and phenomena from a wide range of linguistic fields from phonology to
morphology, as well as certain areas of syntax and semantics (in POS tagging and
machine translation).  The practical applications cover spell-checking, 
as demonstrated in Voikko\footnote{\url{voikko.sf.net}} with
bindings to LibreOffice, Gnome desktop, Mozilla and Mac OS X Spell Service, to
morphological and syntactic analysis as demonstrated in native HFST tools
from HFST downloads\footnote{\url{hfst.sf.net}}, and to machine translation as demonstrated in
several released language pairs in the machine translation system
Apertium\footnote{\url{www.apertium.org}}. This demonstrates a very important feature of HFST,
alluded to in previous chapters, i.e. language models can be
described with one tool in one theoretical and practical framework. For example,
existing morphological analyzers for the Sámi languages found on the
Internet\footnote{\url{divvun.no}}, written with the Xerox formalism were converted into a spell-checker 
in one evening and evaluated in \cite{pirinen/2010/lrec} with additional training from likewise freely
available Sámi Wikipedia. Similar results of using machine translation
dictionaries from the free/libre open source project
Apertium to create not only
dictionaries, but morphological analyzers and spell-checkers, are also
demonstrated on our web page.\footnote{The examples are available at
  \url{http:/www.ling.helsinki.fi/cgi-bin/omor/omordemo.bash}}

It can be noted about the general applicability of finite-state
language models in practical applications that we can now generate
e.g. spell-checkers for languages that lack such tools but have machine
readable dictionaries (such as Manx) or morphological analyzers (such as
Greenlandic, which is not easily implemented in other spell-checking formalisms). 
The adaptation of such models to baseline spell-checkers (with a homogeneous 
Damerau-Levenshtein edit distance as error model) can be performed without 
feedback from linguists or native speakers.

\subsection{Spellcheckers}

The task of finite-state spell-checking is well researched and documented. It
consists mainly of two phases, identifying incorrect word forms and creating
suggestions for corrections. For incorrect word forms there are two types of
mistakes, non-word spelling errors, such as writing \emph{cta} where \emph{cat}
is meant, and real-word spelling errors, such as writing \emph{there} where
\emph{their} is intended. The method for finding the former in finite-state systems
is simply applying the dictionary to the text word by word. Any unrecognized
string not belonging to the language of the dictionary automaton is a non-word
spelling error. For real-word errors a statistical n-gram model or syntactic
parser is typically required. To correct a spelling error in a finite-state
system, a two-tape automaton modeling the typing errors should be applied to
the misspelt string to get set of potential corrections\cite{pirinen/2010/lrec}.
For practical purposes the error model can also be implemented as a
fuzzy finite-state traversal algorithm or similar methods\cite{oflazer/1996}. The result
of the correction step is a set of word-forms that are correct in the
language of the spell-checking dictionary. Another related task is to rank this
set to provide the most likely corrections first.
A trivial way to perform such ranking would be to use
unigram \cite{pirinen/2010/lrec} or n-gram probabilities of the words 
\cite{mays/1991} or word-form analyses \cite{pirinen/2012/cicling}.

\subsubsection{Creating Spellcheckers.}
% Tommi

The creation of a finite-state spellchecker requires compiling of (at least) two
automata: a dictionary that contains the correctly spelled word-forms and the
error model that can rewrite misspelt word-forms into correctly spelled ones.
The former automaton can be as simple as a reference corpus, containing larger
quantities of correctly spelled words and (possibly) smaller quantities of
misspelt ones\cite{norvig/2010}. Also more elaborate dictionaries, such as
morphological analyzers described in Section~\ref{MorphTools} can be used, and also
trained for spell-checking purposes with reference corpora without any big
modifications to underlying implementation\cite{pirinen/2010/lrec}.

For the error-model we can trivially construct an automaton corresponding to
the Levenshtein edit distance algorithm\cite{oflazer/1996,agata/2002}. For more elaborate error models
it is possible to use hunspell algorithms as automata \cite{pirinen/2010/il} or
construct further extensions by hand \cite{pirinen/2010/lrec}. Given an
aligned error corpus, it is also possible to construct a weighted error model
automaton automatically \cite{brill/2000}.

\subsubsection{Checking Strings and Generating Suggestions.}
% Sam

String checking is a straightforward process of matching against the lexical
automaton, or calculating the output language of $I \circ L$, where $I$ is the
input and $L$ is the lexical automaton. If the result is empty, the
string is absent from the lexicon and the correction set must be calculated.

A corrected string is a string that can be generated by transforming the
input string with the error source and is present in the lexicon.
These strings are thus the output language of the composition
$I \circ \ E \circ L$, where $E$ is the error source.

The desired behavior, or result set $R$, of a spellchecker given input $I$, a
lexicon $L$ and an error source $E$ is thus given by equation \ref{result_set_eqn},
where $\pi_2$ is the output projection.

\begin{equation}
  \label{result_set_eqn}
  R = \begin{cases}
    \pi_2(I \circ L), & \mbox{if } \pi_2(I \circ L) \neq \emptyset \\
    \pi_2(I \circ E \circ L) & \mbox{otherwise}\\
  \end{cases}
\end{equation}

It is undesirable to compute either of the intermediate compositions
$I \circ E$ and $E \circ L$; the former will require futile work (producing
strings that are not in the lexicon) and the latter, though possible to
precompute, will be very large (see table \ref{composed_error_table}).

\begin{table}
  \centering
  \caption{Size of two lexical transducers composed with
    Damerau-Levenshtein edit distance transducers, all results minimized}
  \label{composed_error_table}
  \begin{tabular}{ |c| c c c| }
    \hline
    Transducer               & states   & transitions & SFST file size \\ \hline
    Morphalou (French)       & 77.0K    & 190.7K   & 1.7MB \\
    With edit distance 1     & 9.6K     & 733.6K   & 5.7MB \\
    With edit distance 2     & 18.2K    & 344.7K  & 28MB \\ \hline
    OMorFi (Finnish)         & 203.8K   & 437.9K   & 4.0M \\
    With edit distance 1     & 17.7K    & 16186.6K & 120MB \\
    With edit distance 2     & 30.5K    & 53738.2K & 410MB \\ \hline
  \end{tabular}

\end{table}

To circumvent these problems, a three-way on-line composition of $I$, $E$ and
$L$ was implemented (distributed as \verb!hfst-ospell!
under the GPL and Apache licenses) (for a more general and complex algorithm,
see \cite{allauzen/2009}).

We write $I = (Q^I, 0^I, T^I, \Delta^I)$, where $Q$
is the set of states, $0$ is the starting state, $T$ is the set of terminal
states and $\Delta$ is the set of transitions (triples of (state, symbol pair
(input and output), state)), and similarity for $E$ and $L$.

Eliding for the time being weights and flag diacritics, states in the
composition transducer are triples of states of the component transducers.
Analogously with two-way composition, the starting state is $(0^I, 0^E, 0^L)$
and the edge $\delta = (q_1, (\sigma_1, \sigma_2), q_2)$ exists if edges
$(q^I_1, (\sigma_1, \sigma'), q^I_2)$,
$(q^E_1, (\sigma', \sigma''), q^E_2)$ and
$(q^L_1, (\sigma'', \sigma_2), q^L_2)$ exist in the respective transducers
for some
$\sigma' \in \Sigma^I \cap \Sigma^E, \sigma'' \in \Sigma^E \cap \Sigma^L$,
where $q_n = (q^I_n, q^E_n, q^L_n)$ and $\sigma_1 \neq \epsilon \neq \sigma_2$.

The set of the states of the composition, $Q$, are the reachable subset
(in the sense of having a path from $0$ to the state) of
$Q^I \times Q^E \times Q^L$.

If in a given state any of the component transducers has an edge involving
epsilon and the other symbols match as above, the resulting composed edges
go to states such that any state to the left of an input epsilon or to the
right of an output epsilon is unchanged, eg. if we have
$(q^I_1, (\sigma, \sigma'), q^{I'})$, $(q^E_1, (\epsilon, \sigma''), q^{E'})$ and
$(q^L_1, (\sigma'', \sigma'''), q^{L'})$, a successor state of $q_1$ will be
$(q^I_1, q^E_2, q^L_2)$.

The final states are the reachable subset of $T = T^I \times T^E \times T^L$.

It can trivially be verified that this is equivalent to taking the two
compositions $(I \circ E) \circ L$.

This three-way composition is computed in a breadth-first manner with a
double-ended queue of states. The queue is initialized with the starting state,
and the target of every edge is computed and pushed onto the
queue. The starting state is then discarded, and the process is repeated
with a new state popped from the queue until the queue is empty.

Conceptually, the state space of $E \circ L$, which contains all the
misspellings (in the sense of $E$) of all the entries in the lexicon, is
explored in such a way that only the states visited when looking up $I$ are
generated.

For this process to be guaranteed to terminate, it is sufficient that none of
the component transducers have input-epsilon loops and that the input
transducer accepts strings of only finite length. This is because every newly
generated state will either have a shorter sequence of edges to traverse in
$I$ ("increment $q^I$") or be closer to requiring an edge in $I$ to be traversed
(due to a finite sequence of epsilon edges becoming shorter), establishing a
loop variant.

Weights representing the probability of a particular correction being the
correct one are a natural extension, and in \verb!hfst-ospell! correspond to
multiplication in the tropical semiring (for details see \cite{openfst/2007})
of the weight each edge
traversed. Multiplication in this semiring is the standard addition operation
of positive real numbers, which we approximate by addition of \verb!float!s.
Each state in the queue is recorded with an accumulated weight, and its
successor states have this weight incremented by the sum of the weights of
the edges traversed in the component transducers.

The alphabet of $I$ cannot be determined in advance, and in practice is taken
to be the set of Unicode code points. To allow the error model to correct
unexpected symbols in this large space, \verb!hfst-ospell! uses a special
symbol, \verb!@_UNKNOWN_SYMBOL_@! which is taken to be equal to any symbol
that is otherwise absent.

\subsubsection{Error model tool and optimizations.}
For the most common case of generating Levenshtein and
Damerau-Levenshtein (in which transposition of adjacent symbols constitutes
one operation) distance error models, a tool (\verb!editdist.py!) and
definition format was developed.

The definition format serves the purposes of minimizing the number of symbols
used in the error source (the number of transitions is $\mathrm{O}(|\Sigma|^2)$)
and introducing weights for edits. Typical
morphologies have a number of unusual characters (punctuation, special symbols)
or internally used symbols (eg. flag diacritics) that should be filtered for
more efficient correction. This is accomplished by providing a facility for
reading the alphabet from a transducer, ignoring any symbols of more than
one Unicode code point, and reading further ignorable symbols from a
configuration file.

The configuration file allows specifying weights to be added to any edit
involving a particular character or a particular edit operation (for example,
assigning a low weight to the edit o $\rightarrow$ ö for an OCR application).

Certain characteristics of the spelling correction task permit
efficiency-oriented improvements to error models. A naive Levenshtein error
model with edit distance 2 in \verb!ospell! would, when given the word
word \verb!arrivew! where the French word \verb!arriver! was intended,
do the following useless work:

{\footnotesize
\begin{verbatim}
  a:0 0:a r r i v e    [failure]
  0:a a:0 r r i v e    [failure]
  a r:0 0:r r i v e    [failure]
  a 0:r r:0 r i v e    [failure]
  ...
  a r r i v e w:0      [success]
  a r r i v e w:z      [success]
  a r r i v e w:r      [success]
  ...
\end{verbatim}
}

When the correctable error is near the end, almost every symbol is deleted and
inserted with no effect. This may be circumvented by adding, for each deletion
and insertion, a special successor state from which its inverse is absent.

\subsubsection{Ranking Suggestions.}
% Tommi & Miikka

When applying the error model and the language model to input with spelling errors,
a result is typically an ordered set of corrected strings with some probability
associated with each correction. After applying the contextless error correction
described earlier, it is possible to use context words and their potential
analyses to re-rank the corrections \cite{pirinen/2012/cicling}.


\section{Discussion and Future Work}\label{Discussion}

\subsection{Synonym and Translation Dictionaries}
% Krister

Other finite-state applications created with HFST include inflecting thesauri and translation dictionaries.
These applications have been created from the bilingual parallel Princeton WordNet and FinnWordNet. 
The creation of FinnWordNet is documented in \cite{linden/2010}. FinnWordNet contains roughly 150,000 
word meainings in Finnish with their English translations. The synonym dictionaries for Finnish and English and the Finnish-English and English-Finnish translation dictionaries as well as their demos can be found on \url{hfst.sf.net}.

They inflecting synonym dictionaries were created as a composition of three transducers: (1) a morphological analyzer, (2) a transducer that replaces one word with another while coping the inflection tags and (3) a morphological generator as an inversion of the morphological analyzer. The translation dictionaries only have components (1) and (2). In the future, we intend to take advantage of the weighted transducers to introduce contexts so as to be able to suggest the most likely synonym or the most likely translation in context.

\subsection{Extending Transducers for Pattern Matching}
% Sam

Advanced, fast and flexible pattern matching is a major requirement for a
variety of tasks in information extraction, such as named entity recognition.
An approach to this task was presented by Lauri Karttunen in
\cite{karttunen/2011}, and an outline for implementing it in HFST is given here.

\subsubsection{Some desiderata for pattern matching.} Several patterns,
including nested matches, should be able to operate in parallel, and
it should be possible to impose contextual requirements (rules) on
the patterns to be matched. Matching should be efficient in space and
time --- in particular, it should be possible to avoid long-range
dependencies which are awkward for FST transformations.
A powerful system should also have a facility for referring
to common subpatterns by name.

\subsubsection{The pmatch Approach.}

For a more detailed overview the reader is directed to \cite{karttunen/2011}.
Here we focus on the aspects of \verb!pmatch! that necessitate extensions
to a FST formalism from the point of view of the implementation.

\verb!pmatch! is presented as a tool for general-purpose pattern matching,
tokenizing and parsing. It is given a series of definitions and a
specialized regular expression, and it then operates on strings, passing
through unmodified any parts of them that fail to match the expression,
and applying transformations to any matched parts. If several matches beginning
from a common point in the input can be made, matches of less than maximal
length are considered invalid.

The expressions may refer to other expressions (possibly combined with each
other by operations on regular languages), contextual conditions
(left or right side, with negation, \verb!OR! and \verb!AND!) and certain
built-in transformation rules. The most interesting of these transformation
rules is \verb!EndTag()!, which triggers a wrap-around XML tag to be inserted
on either side of the match.

Referencing other regexes (including self-reference) is unrestricted, so
the complete system has the power of a recursive transition network (RTN),
and matching is therefore context-free.

The crucial extension-demanding features for an HFST utility
with similar applications are:

\begin{itemize}
\item A distinction between an augmented universal transducer (the top level
  which echoes all input in the absence of matches) and sub-transducers
\item Ignoring non-maximal matching, ie. a left-to-right longest-distance
  matcher
\item An unrestricted system for referencing other transducers by name
\item Special handling of \verb!EndTag()! and instructions for context
  checking
\item Reserved symbols for implementing transducer insertion/reference,
  \verb!EndTag()! and contexts
\end{itemize}

The referencing system is apparently the only one of these that would be
impossible to implement in a strict FST framework; the other extensions suggest
compilations to larger, possibly less efficient transducers.

% \subsection{Implementation in a FST library}

% Whereas \verb!pmatch! offers a unified interface with various built-in
% transformations, sets and a Xerox-inherited syntax and environment,
% our present effort is focused on an API that supports the development
% of more specific applications.

% The call stack is a convenient metaphor for the referential system. When a
% transducer is loaded for matching in the initial position, a referential space
% of transducers is populated by recursively traversing the alphabet, looking
% for references to other transducers. This process is repeated until all
% possible references have been located.

% At every symbol in the input, the top level attempts to transform the longest
% possible continuation (or, if the same maximal continuation can be transformed
% in multiple ways, returns one of them, typically with a weighting system).
% Failures to do this are interpreted as successful identity matches.

% The encountering of an insertion symbol (e.g. \verb!@I.FinnishAdjective@!)
% triggers the calling of a subnetwork transducer with the transducer in
% question. These transducers share the full input buffer (for checking
% an arbitrary amount of context) but have independent spaces for flag
% diacritics, possible alphabet-dependent special symbols (such as \verb!IDENTITY!
% or \verb!UNKNOWN!).

\section{Conclusion}
HFST---Helsinki Finite-State Technology (\url{hfst.sf.net})
is a framework for compiling and applying linguistic descriptions
with finite-state methods. We have demonstrated how HFST uses finite-state techniques 
for creating runtime morphologies, taggers, spellcheckers, inflecting synonym dictionaries as well as 
translation dictionaries using one open-source platform which supports extending  
the descriptions with statistical information to allow the applications 
to take advantage of context. HFST offers a path from language descriptions 
to efficient language applications.

\subsection*{Acknowledgments}
We wish to acknowledge the FIN-CLARIN and META-NORD projects for their financial support 
as well as HFST users for their many constructive suggestions. 

\bibliographystyle{splncs03}

\bibliography{hfst2012}

\end{document}
% vim: set spell:
