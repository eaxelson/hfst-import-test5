\documentclass[11pt,a4paper]{article}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{xltxtra}

%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

\title{Improving Finite-State Spell-Checker's Corrections with Combinations of Word-Form and Morphosyntactic n-grams}

\author{Tommi A Pirinen\\
  University of Helsinki\\
  Helsinki, Finland\\
  {\tt \{tommi.pirinen,} And
  Miikka Silfverberb\\
  University of Helsinki\\
  Helsinki, Finland\\
  {\tt miikka.silfverberg,} And
  Krister Lind\'{e}n\\
  University of Helsinki\\
  Helsinki, Finland\\
  {\tt krister.linden\}@helsinki.fi}
}

\date{\today}

\begin{document}
\maketitle
\begin{abstract}
  In this paper we demonstrate an finite-state implementation of context-aware
  spell checking utilising both surface and analysis n-grams to improve the
  quality of suggestions by simple edit-distance based spell-checker. We
  demonstrate the context-aware spell-checking for English and Finnish and
  suggest the modifications that are necessary to have traditional n-gram
  models working for morphologically more complex languages, such as Finnish.
\end{abstract}

\section{Introduction}

Spell-checking by computer is perhaps one of the oldest most researched
application in the field of language technology, starting from the mid 20th
century~\cite{damerau/1964}. One of the crucial parts of spell-checking---both
from interactive user-interface point of view and for unsupervised correction
of errors---is the production of spelling suggestions.  In this article we test
various finite-state methods for using context and shallow morphological
analysis to improve the suggestions generated by traditional edit distance
measures or unigram frequencies (such as \cite{pirinen/2010/lrec}).

The task of spell-checking can be split to two parts of \emph{detection} and
actual \emph{correction} of the spelling errors. The spelling errors can be
detected from text as such word forms that are unlikely to belong to the
natural language in question, such as writing `cta' instead of `cat'. This form
of spelling errors is commonly called \emph{non-word (spelling) errors}.
Another form of spelling errors is word forms that do not belong to the given
context under certain syntactic or semantic requirements, such as writing
`their' instead of `there'. This form is called correspondingly \emph{real-word
(spelling) errors}. The non-word type of spelling errors can be easily detected
using a dictionary, whereas the detection of latter type of errors typically
requires syntactic analysis or probabilistic methods~\cite{mitton/2009}. For
purpose of this article we treat both forms of errors as same, as the same
methods can be applied for their correction.

The correction of spelling errors usually means generating list of word forms
belonging to the language. The mechanism of correcting the erroneous word-forms
is called an \emph{error-model}. The purpose of error-model is to act as a
filter to revert the mistakes the user typing the erroneous word-form has made.
The most simple and traditional model for making such corrections is called
Levenshtein-Damerau edit distance algorithm, attributed initially to
\cite{levenshtein/1966} and especially in context of spell-checking to
\cite{damerau/1964}. The Levenshtein-Damerau edit distance assumes spelling
errors are one of insertion, deletion or changing of a single character to
another, or swapping two adjacent characters, which models well the spelling
errors caused by accidental slip of finger on a keyboard. It was originally
discovered, that for most languages and spelling errors, this simple method
already covers 80~\% of all spelling errors~\cite{damerau/1964}. This model is
also language-independent, ignoring the differences in character repertoires of
given language. Various other error models have also been developed, ranging
from confusion sets to phonemic folding~\cite{kukich/1992}.

In this paper we evaluate the use of context for further fine-tuning of the
spelling error corrections. The context is still not commonly used in
spell-checkers, according to \cite{kukich/1992} it was lacking in majority
of spell-checkers and while situation may have improved slightly for some
commercial office suite products, the main spell-checkers for open source
environments are still primarily context-ignorant, such as
hunspell\footnote{\url{http://hunspell.sf.net}} widely in use in open source
world.  In case of English, the surface word-form trigrams has been discovered
to be reasonably efficient \cite[for non-word cases]{church/1991} and
\cite[for real-word cases]{mays/1991}. Another form of statistical
improvement of the correcting suggestions is to use morphosyntactically
relevant analyses in stead of surface forms to improve the corrections. In this
article we evaluate a hybrid model combining a shallow statistical
morphological parser~\cite{silfverberg/2010} with the surface word-form
n-grams.

Furthermore we test the context-based spelling methods for both English and
Finnish language materials, to ensure the applicability of the method for
morphologically different languages. The reason to do this is two-fold; firstly
the fact that English has rather low morphological productivity may result that
it behaves differently statistically than other languages. On the other hand
English has the largest amount of freely available text corpora, for other
languages the availability of free corpora, especially annotated material is
often seen as a problem.

The article is laid out as follows: in section~\ref{sec:material} we introduce
the corpora and dictionaries used for spell-checking and training material, as
well as the corpora used for obtaining the spelling errors with context. In
section~\ref{sec:methods} we show the implementation of finite-state
context-aware spell-checker and describe the statistical methods used. In
section~\ref{sec:evaluation} we show how the created spelling correctors
improve the results and explain the errors left. In
section~\ref{sec:future-work} we compare our work to other current systems and
enumerate possible improvements for both.

\section{Material}
\label{sec:material}

To train the spell-checker lexicons, word-form probabilities can be acquired
from arbitrary running text. By using these unigram frequencies we can assign
all word-forms initial probabilities in isolated spell-checking context alone.
The unigram-trained models we used were acquired from existing isolated
spell-checker systems~\cite{norvig/2010,pirinen/2010/lrec}, but we shortly
describe the used corpora here as well.

To train the various n-gram models, corpora are required. For surface-form
training material it is sufficient to capture running n-grams from the text.
For training the statistical parser parts with annotations, we also require a
readily disambiguated readings, ideally of course this means hand-annotated
tree banks or similar gold corpora. 

For evaluation two forms of error corpora are needed. One error corpus is based
on actual spelling errors from real sources, with correct corrections
written by linguists. Another, larger error corpus was generated from Wikipedia
using automatic error source to generate Damerau-Levenshtein type error at
roughly the probability of $\frac{1}{33}$ per character in running text, each
error type has exactly quarter of this probability\footnote{with exception of
swapping of two adjacent characters at the word-final character}. 

The corpora used are summarised in table~\ref{table:corpora}, the sizes are
provided for use of reconstruction of the systems, in practice they are newest
available versions of the respective corpora at the time of testing. In the
table, the first row is the training material used for the finite-state
lexicon, i.e. the extracted surface word-forms without the analyses for unigram
training. The second row is for the analysed and disambiguated material for the
n-gram based parsers and suggestion improvement. The third line is the corpora
of spelling errors used only for the evaluation of the systems.  As we can see
from the figures of English compared to Finnish there's a significant
difference in freely available corpora such as Wikipedia. When going further to
lesser resourced languages the number will drop enough to make such statistical
approaches less useful (e.g. the example of Northern S\'{a}mi in
\cite{pirinen/2010/lrec}).

\begin{table}[h]
  \begin{center}
    \caption{Sizes of training corpora
    \label{table:corpora}}
    \begin{scriptsize}
      \begin{tabular}{l|rrr}
        \hline
        Count unit$\rightarrow$ & Sentences & Tokens & Word-forms \\
        $\downarrow$Language    & (lines)   &        &  \\
        \hline
        \multicolumn{4}{c}{\textbf{English}} \\
        \hline
        Unigrams & --- & 2,110,728,338 & 128,457  \\
        (Wikipedia & & & \\
        etc.) & & & \\
        N-grams & & & \\
        (WSJ) & & & \\
        Errors & 3,229 & 20,237 & 2,397 \\
        (real) & & & \\
        (generated) & 9,409 & 1,047,089 & 8656 \\
        \hline
        \multicolumn{4}{c}{\textbf{Finnish}} \\
        \hline
        Unigrams & --- & 17,479,297 & 968,996 \\
        (Wikipedia) & & & \\
        N-grams & & & \\
        (europarl) & & & \\
        Errors & 333 & 4,177 & 2,762 \\
        (real) & & & \\
        (generated) & 9,640 & 723,526 & 14,983 \\
        \hline
      \end{tabular}
    \end{scriptsize}
  \end{center}
\end{table}

\subsection{English corpora}

The English dictionary is based on simple frequency weighted word-form list of
English language, as proposed in \cite{norvig/2010}. The word-forms were
collected from the English Wiktionary\footnote{\url{http://en.wiktionary.org}},
the English EBooks from project
Gutenberg\footnote{\url{http://www.gutenberg.org/browse/languages/en}} and the
British National
Corpus\footnote{\url{http://www.kilgarriff.co.uk/bnc-readme.html}}. This
word-list is used in effect as unigram lexicon for spell-checking.

To train English morphosyntactic parsers, we use the WSJ corpus. In
this corpus each word is annotated by single tag that encodes some
morphosyntactic information, such as part-of-speech and inflectional form. The
total number of tags in this corpus is XYZ.

The real world spelling errors of English were acquired by extracting the ones
with context from Birkbeck error
corpus\footnote{\url{http://ota.oucs.ox.ac.uk/headers/0643.xml}}. In this
corpus the errors are from variety of sources, including errors made by
children and language-learners. For the purpose of this experiment we picked
the subset of errors there which included context and also removed the cases of
word joining and splitting to simplify the implementation of parsing and
suggestion.

\subsection{Finnish Corpora}

For Finnish dictionary we selected the freely available open source
finite-state implementation of Finnish morphological
analyser\footnote{\url{http://home.gna.org/omorfi}}. The analyser had the
frequency-weighted word-form list of Finnish
Wikipedia\footnote{\url{http://download.wikipedia.org/fiwiki/}} making it in
practice an extended unigram lexicon for Finnish. The Finnish morphological
analyser, however, is capable of infinite compounding and derivation, which
makes it notably different approach to spell checking than English finite
word-form list. 

The Finnish morphosyntactic n-gram model was trained using europarl corpus
analysed with fdg. In this format the annotation is based on sequence of
tags, encoding part of speech and inflectional form. The total number of
different tag sequences for this annotation is ABCDEF! So obviously the
statistic distribution for these is likely to be different than for English.

For Finnish spelling errors, we ran the Finnish unigram spell-checker through
Wikipedia, europarl and a corpus of Finnish EBooks from the project
Gutenberg\footnote{\url{http://www.gutenberg.org/browse/languages/fi}}, to
acquire the non-word spelling errors, and picked at random the errors having
frequencies in range 1 to 8 instances; majority of higher frequency non-words
were actually proper nouns or neologisms missing from the dictionary. Using all
Wikipedia, europarl and Gutenberg provides the reasonable variety of both
contemporary and old texts in wide range of styles.



\section{Methods}
\label{sec:methods}

In this article we use the finite-state formulation of
spell-checking~\cite{pirinen/2010/cla}. We assume the standard notation for
finite-state algebra and define the language model as weighted finite-state
automaton assigning a weight to each correctly spelled word-form of a language,
and an error model automaton mapping a misspelled string to set of corrected
strings and their weights.

The only methodological modification that is needed to compile frequency based
word-form list into a weighted finite-state automaton is transforming the
weights, in this case we use the tropical semi-ring by transforming the
frequencies into penalty weights with formula $-\log\frac{f}{CS}$, where $f$ is
the frequency and $CS$ the corpus size in number of tokens.

The spelling suggestion lists from these unigram lexicon-based spell-checkers
are initially generated by composing a edit distance
automaton~\cite{agata/2002} with error weight of $-\log\frac{1}{CS+1}$ per edit
distance type of error. In practice this means that the corrections are
initially ordered primarily by the edit distance of the correction, and
secondarily by the unigram frequency of the word-form in reference corpus.
This order is implicitly encoded in the weighted paths of the resulting
automaton; to list the corrections we use the n best paths
algorithm~\cite{mohri/2002}. This ordering is also used as our baseline.

For context-based ordering of the corrections we use both the probabilities of
the surface word-form n-grams and the probabilities of the n-grams of the
analysis annotations. The implementation of analysis n-gram probability
estimation is similar as described in \cite{silfverberg/2010} with following
adaptations for the spelling correction. The suggestion which gives the highest
ranking, most likely analysis is selected.  The n-gram probability is estimated
separately with each spelling suggestion and then combined with the baseline
probability given by the unigram probability and the edit distance weight using
scaling formula of $1+2=3$

For example when correcting the misspelling of `an' as `anx' in sentence ``this
is anx example sentence'', as shown in the figure~\ref{fig:example}, we have
the surface trigrams \{this, is, \_\}, \{is, \_, example\}, \{\_, example,
sentence\}, and corresponding analysis trigrams \{DET, VVBZ, \_\}, \{VVBZ, \_,
NN\}, \{\_, NN, NN\}. The suggestions for anx at edit distance of one include
`ax' (one deletion), `ant', `and', `any' (one change) and so on. To rank
possible suggestions we substitute s3 for the suggestions, and estimate its
likely analysis, then use the ones with highest combined likelihood of n-gram
formula $P(s1:s5)$ etc.

\begin{figure}[h]
\begin{centering}
\caption{Example trigram combinations\label{fig:example}}
\begin{scriptsize}
\begin{tabular}{llcrr}
\hline
this$_{s1}$ & is$_{s2}$ & \_$_{s3}$ & example$_{s4}$ & sentence$_{s5}$\\
DET$_{a1}$ & VVBZ$_{a2}$ & \_ $_{a3}$& NN$_{a4}$ & NN$_{a5}$\\
\hline
\end{tabular}
\end{scriptsize}
\end{centering}
\end{figure}

The resulting Finite-state system consists of three automata, the dictionary
for spell-checking and error-modelling as described in
\cite{pirinen/2010/lrec}, and the new n-gram model automata, of which
we present four different variants in this paper. These automata sizes are given
in the table~\ref{table:sizes} for reference. The sizes also give some estimate
on memory usage of the spell-checking system, although the actual memory-usage
during correction will raise depending on the actual extents of search space
during correction phase.

\begin{table}
\begin{centering}
\caption{Automata sizes\label{table:sizes}}
\begin{scriptsize}
\begin{tabular}{lrrr}
    Automaton & States & Transitions & Bytes \\
    \hline
    \multicolumn{4}{c}{\textbf{English}} \\
    \hline
    Dictionary & 102,393 & 102,392 & 2.7 MiB \\
    Error model & 5,727 & 11,826 & 208 KiB \\
    \hline
    Analysis 3-grams & & & \\
    Surface 3-grams & & & \\
    Some Hybrid & & & \\
    Another Hybrid & & & \\
    \hline
    \multicolumn{4}{c}{\textbf{Finnish}} \\
    \hline
    Dictionary & 460,227 & 1,700,978 & 38 MiB \\
    Error model & 7,324 & 18,838 & 353 KiB \\
    \hline
    Analysis 3-grams & & & \\
    Surface 3-grams & & & \\
    Some Hybrid & & & \\
    Another Hybrid & & & \\
    \hline
\end{tabular}
\end{scriptsize}
\end{centering}
\end{table}

\subsection{English-Specific Finite-State Weighting Methods}

The language model of English was created as described in \cite{norvig/2010}.
It consists of only the word-form probabilities in the corpora. Similarly for
English n-gram material, the initial analyses found in the WSJ corpora were
taught to the finite-state parser verbatim. The scaling of surface and analysis
models and the unigram model and the edit-distance models requires complex
formulae $a^3 + b^3 = c^3$.


\subsection{Finnish-Specific Finite-State Weighting Methods}

The Finnish language model was based on a readily-available morphological
weighted FST analyser of Finnish language~\cite{pirinen/2008}.  We furthermore
modified the automaton to discount suggestions of newly created compounds and
derivations by weight greater than maximum weight of automaton before that,
i.e. $-10\log\frac{1}{CS+1}$ for the automatons training material. This has
nearly the same effect as using separate dictionary for suggestions, that
excludes the heavily weighted forms, without requiring the extra space.

In the initial Finnish parser model there were relatively large tagset, all of
which did not contain information we found necessary for task of
spell-checking, such as discourse particles, which are relatively
context-agnostic~\cite{visk}, so we opted to simplify the tagging on these
parts. Furthermore the parser used for training produced heuristic readings for
unrecognised word-forms, which we also removed.

\section{Tests and Evaluation}
\label{sec:evaluation}

The evaluation of the correction suggestion quality is descrcribed in
tables~\ref{table:real-eval}~and~\ref{table:fake-eval}. The
table~\ref{table:real-eval} contains precision values for the spelling errors
from real texts, and table~\ref{table:fake-eval} for the automatically
introduced spelling errors. The precision is measured by ranked suggestions. In
the tables, we give the results separately for ranks 1---4, and for the
remaining lower ranks.  In the last column, we have the cases where a correctly
written word could not be found with the tested suggestion algorithm.  The rows
of table represent different combinations of the n-gram models. The first row
is a baseline score achieved by the unigram dictionary and the weighted edit
distance model alone.

\begin{table*}[h]
  \begin{center}
    \caption{Precision of suggestion algorithms with real spelling errors
    \label{table:real-eval}}
    \begin{scriptsize}
      \begin{tabular}{l|llll|l|l|l}
        \hline
        Algorithm & 1 & 2 & 3 & 4 & Lower & None & Total \\
        \hline
        \multicolumn{8}{c}{\textbf{English}} \\
        \hline
        Unigrams
 & 1429      & 382       & 217       & 110       & 748       & 1105      & 3991     \\
        (baseline) 
 &  35.81 \% &   9.57 \% &   5.44 \% &   2.76 \% &  18.74 \% &  27.69 \% & 100.00 \%\\
        \hline
        Analysis
 & 1100      & 344       & 225       & 107       & 687       & 1373      & 3991     \\
        trigrams
 &  30.25 \% &   9.46 \% &   6.19 \% &   2.94 \% &  18.89 \% &  32.26 \% & 100.00 \%\\
        \hline
        Surface  &       &      &      &      &       &       &     \\
        trigrams &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Some     &       &      &      &      &       &       &     \\
        hybrid   &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Another  &       &      &      &      &       &       &     \\
        hybrid   &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        \multicolumn{8}{c}{\textbf{Finnish}} \\
        \hline
        Unigrams  
 & 232       & 26        & 8         & 8         & 24        & 25        & 322      \\
        (baseline) 
 &  72.05 \% &   8.07 \% &   2.48 \% &   2.48 \% &   7.45 \% &   7.76 \% & 100.00 \%\\
        \hline
        Analysis
 & 109       & 44        & 11        & 11        & 30        & 117       & 322      \\
        trigrams
 &  33.96 \% &  13.71 \% &   3.43 \% &   3.43 \% &   9.35 \% &  36.14 \% & 100.00 \%\\
        \hline
        Surface  
&       &      &      &      &       &       &     \\
        trigrams 
&    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Some    
&       &      &      &      &       &       &     \\
        hybrid  
&    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Another  
&       &      &      &      &       &       &     \\
        hybrid   
&    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
      \end{tabular}
    \end{scriptsize}
  \end{center}
\end{table*}

\begin{table*}[h]
  \begin{center}
    \caption{Precision of suggestion algorithms with automated spelling errors
    \label{table:fake-eval}}
    \begin{scriptsize}
      \begin{tabular}{l|rrrr|r|r|r}
        \hline
        Algorithm & 1 & 2 & 3 & 4 & Lower & None & Total \\
        \hline
        \multicolumn{8}{c}{\textbf{English}} \\
        \hline
        Unigrams   &     &       &      &      &       &       &     \\
        (baseline) &  \% &    \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Analysis &       &      &      &      &       &       &     \\
        trigrams &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Surface  &       &      &      &      &       &       &     \\
        trigrams &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Some     &       &      &      &      &       &       &     \\
        hybrid   &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Another  &       &      &      &      &       &       &     \\
        hybrid   &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        \multicolumn{8}{c}{\textbf{Finnish}} \\
        \hline
        Unigrams   &     &       &      &      &       &       &     \\
        (baseline) &  \% &    \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Analysis &       &      &      &      &       &       &     \\
        trigrams &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Surface  &       &      &      &      &       &       &     \\
        trigrams &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Some     &       &      &      &      &       &       &     \\
        hybrid   &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
        Another  &       &      &      &      &       &       &     \\
        hybrid   &    \% &   \% &   \% &   \% &    \% &    \% & 100 \% \\
        \hline
      \end{tabular}
    \end{scriptsize}
  \end{center}
\end{table*}

We note that using the hybrid model X we get uniform improvement regardless
of language used, whereas the traditional methods of surface or analysis n-grams
result in deterioration for morphologically complex Finnish material. A reason
for this may be the relative sparseness of statistical data in Finnish
material. We present further analysis for both languages in the following 
subsections.

\subsection{English Error-Analysis}

The English is entirely uninteresting in the first place.

\subsection{Finnish Error-Analysis}

For Finnish material the traditional methods of using analysis n-grams results
in approximately 50 \% of the good suggestions. Looking at the examples, this
most commonly happens in nominals where the expected correction in oblique
cases is shadowed by other correction of grammatical cases; since word-ordering
of verbal complements and adjuncts is relatively free it is expected that
statistically more common cases are statistically more common in all positions.

In the surface n-gram model we discover just the same.

The combination of surface and analysis n-grams like so works better, leaving
only residue of following hard cases to be solved.

\subsection{Performance Evaluation}

We did not perform any work to optimize the trigram analysis and selection,
but we found that the speed of the system is reasonable---even in its current
form. The table~\ref{table:speed-eval} summarises the average
speed of performing the experiments in tables~\ref{table:real-eval}~and~\ref{table:fake-eval}

\begin{table}[h]
  \begin{center}
    \caption{The speed of ranking the errors
    \label{table:speed-eval}}
    \begin{scriptsize}
      \begin{tabular}{l|rrrr}
        \hline
        Material  & English & automatic & Finnish & automatic \\
        Algorithm & real    &           & real    &           \\
        \hline
        Unigrams   &    10.0 s &      s & 138.2 s &     s  \\
        (baseline) & 399.1 wps &    wps & 2.3 wps &   wps  \\
        \hline
        Analysis &   377.4 s &     s & 3593.3 s &     s  \\
        trigrams & 10.6  wps &   wps & 0.08 wps &   wps  \\
        \hline
        Surface  &      s &     s &     s &     s  \\
        trigrams &    wps &   wps &   wps &   wps  \\
        \hline
        Some     &      s &     s &     s &     s  \\
        hybrid   &    wps &   wps &   wps &   wps  \\
        \hline
        Another  &      s &     s &     s &     s  \\
        hybrid   &    wps &   wps &   wps &   wps  \\
        \hline
      \end{tabular}
    \end{scriptsize}
  \end{center}
\end{table}


\section{Future Work and Discussion}
\label{sec:future-work}

We have shown that combined n-gram models are suitable for improving the
spelling corrections for both morphologically more complex languages such as
Finnish and the further improving the simpler languages like English. To
further verify the suitability of the method it still needs to be tested on
typologically more wide spectrum of languages.

In this article we used readily available and hand-made error corpora to test
the method for error correction. The same method as correction should typically
be possible to be used for error detection as well, especially for real-word
errors~\cite{mays/1991}. For future research an obvious development is to
integrate the n-gram system as a part of real spell-checker system for both
detection and correction of the spelling errors, as is already done for the
unigram based spell checker demonstrated in~\cite{pirinen/2010/lrec}.

\section{Conclusion}

In this paper we demonstrated the use of finite-state methods for trigram based
generation of spelling suggestions. We have showed that the basic trigram
methods suggested for languages like English~\cite{wilcox-ohearn/2008} are not
useful without modification for morphologically more complex languages like
Finnish.  Instead a more elaborate n-gram scheme combining surface and analysis
n-grams is successful for Finnish as well as English, at improvement rate of
more than 0~\%.

%\section*{Acknowledgements}

% We are grateful to Sam Hardwick for the spell checking software and the
% HFST research group for fruitful discussions.

\bibliographystyle{unsrt}
\bibliography{cicling2011}


\end{document}
