\documentclass{llncs}
\usepackage{epsf}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}

\begin{document}

\title{Predictive Text Entry of Agglutinative Languages using Morphological Segmentation and Phonological Restrictions}

\author{\ldots\\\ldots\\\ldots}
\institute{\ldots}

\maketitle

\begin{abstract}

Linguistic models for predictive text entry on ambiguous keyboards
typically rely on large dictionaries including word frequenies, which
are used to disambiguate between words matching user input. This
approach is insufficient for heavily agglutinative languages, like
Finnish or Turkish, where morphological phenomena such as inflection
and compounding increase the rate of out-of-vocabulary words. We
propose a method for text entry, which circumvents the problem of
out-of-vocabulary words, by replacing the dictionary with a Markov
chain on morpheme sequences constructed from morphologically segmented
training data. The Markov chain is combined with a third order hidden
Markov model (HMM) mapping key sequences to letter
sequences. Additionally we use rules, which enforce phonotactic
restrictions such as vowel harmony in Finnish. We evaluate our method
by constructing text entry systems for Finnish and Turkish mobile
phone keypads. We compare the Turkish text entry system with an
existing system, which is based on an HMM of letter sequences
\cite{Tantug:2010} and show that we achieve superior results measured
by the keystrokes per character ratio (KPC)
\cite{MacKenzie02kspc}. We also compare the Finnish text entry system
to an existing system, which utilizes a morphological analyzer
combined with a colloquial dictionary \cite{silfverberg/2011/cla} and
show that we achieve superior KPC. For segmenting the training data,
we use Morfessor, a system for unsupervised morphological segmentation
\cite{Creutz07ACMTSLP}. For constructing the probabilistic models
needed for the text entry systems, we use tools for POS tagging from
the HFST interface \cite{Silfverberg/2011}, which is an open-source
interface for weighted finite-state calculus. We also utilize an
open-source two-level phonology rule compiler, hfst-twolc, for
implementing the vowel harmony rules needed for text entry of Finnish
\cite{hfst/2011}.

\end{abstract}

\section{Introduction}

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=1.2in]{Nappaimet.pdf}
\caption{The 12-key keypad of a typical Finnish mobile phone. There
  are three letters in the Finnish alphabet "\"{a}", "å" and "\"{o}",
  which are not shown on the keypad. The letters "\"{a}" and "å" are
  entered pressing key "2" four times and five times respectively. The
  letter "\"{o}" is entered by pressing the key "6" four
  times.}\label{keypad}
\end{center}
\end{figure}

Mobile phone text messages are a hugely popular means of
communication, but mobile phones are not especially well suited for
inputting text because of their small size and often limited
keyboard. There exist several technological solutions for inputting
text on mobile phones and other limited keyboard devices. This paper
is concerned with a technology called predictive text entry, which
utilizes redundancy in natural language in order to enable efficient
text entry using limited keyboards (typically having 12 keys).

The subject of predictive text entry has been extensively studied, but
the studies have mainly concerned on predictive text entry of
English. Because of the limited morphological complexity of English,
these approaches have usually been able to rely on an extensive
dictionary along with word frequencies, since a sufficiently large
English dictionary nearly eliminates the problem of out-of-vocabulary
words. 

For morphologically complex languages like Finnish or Turkish,
productive inflection, derivation and compounding raise the number of
out-of-vocabulary words regardless of the size of the dictionary,
which means that out-of-vocabulary words present a serious problem for
dictionary based approaches to predictive text entry.

In this paper we present an approach to predictive text entry which is
based upon a morphologically segmented training corpus and a
probabilistic model on phonotax. We additionally use a probabilistic
model on letter sequences and phonological rules governing the vowel
harmony phenomenon in Finnish. We show that this model delivers
superior results compared to an existing dictionary based model for
text entry of Finnish when evaluated on actual text-message data using
the keystroke per character ratio (KPC). We also compare our method to
the predictive text entry in three commercially available mobile
phones and show that our approach gives superior KPC.

Apart from phonological rules, our approach is entirely unsupervised
and data-driven, since we us the unsupervised morphological
segmentation system Morfessor \cite{Creutz07ACMTSLP} for segmenting
the training corpus and the tools for constructing POS-taggers form
the HFST interface~\cite{hfst/2011}. We show that it can be applied to
another agglutinative language besides Finnish, namely Turkish. We
compare the Turkish text entry system to an existing text entry
system, which is based on a hidden Markov model and show that our
approach gives a substantial improvement in KPC.

The paper is structured as follows. In section~\ref{earlier-work} we
present some earlier approaches to predictive text entry. In
section~\ref{model}, we present our probabilistic model for text entry
together with the phonological rules which are used to realize vowel
harmony and explain how these models are combined into a system for
predictive text entry. Section~\ref{data} describes the training and
test corpora used in constructing and testing predictive text entry
systems for Finnish and Turkish. Evaluation of the systems is
presented in section~\ref{evaluation} and the results are discussed in
section~\ref{discussion}. Finally we present some concluding remarks
and future work directions in section~\ref{conclusion}.

\section{Earlier Approaches to Predictive Text Entry}\label{earlier-work}

Lore ipsum...

\section{Inputting text on a Mobile Phone}

\section{A Probabilistic Model of Word Structure}\label{model}

Predictive text entry can be seen as a labeling task, where every key
in a sequence of keys is assigned its most likely letter. An obvious
approach to modeling such correspondences of keystroke sequences and
letters is using a stochastic model with hidden variables, such as a
hidden Markov model.

Though predictive text entry can be modeled fairly well using HMMs on
key sequences and even using simply n-grams of letter sequences as
demonstrated by~\cite{Tantug:2010}, there are problems with these
approaches for agglutinative languages. A second or third order HMM
cannot encode very long dependencies inside words. This leads to
difficulties in processing agglutinative languages, since it is
difficult to separate stems from affixes in an adequate manner and to
handle arbitrarily long phonological dependencies like vowel harmony
in Finnish and Turkish. Using higher order HMMs could in theory lead
to better accuracy for longer dependencies, but higher order HMMs
suffer both from the data sparseness problem and efficiency problems,
which limit their usability.

We observed earlier that dictionary based methods produce poor results
for agglutinative languages. Still the number of word stems seen in
training corpora of comparable sizes for agglutinative and isolating
languages are comparable. The reasons that make dictionary based
models fare poorly for agglutinative languages are thus inflection,
compounding and derivation. 

In order to get a general enough model of words and still be able to
model word structure at a global level, we model words as sequences of
morphs, which are extracted from an automatically segmented training
corpus. To illustrate our approach we need to look at some Finnish
training data. There will probably be some occurrences of the word
form "taloa" (sg. partitive case of the word house). Automatic
segmentation of the training corpus might give the segmentation "talo
+ a", where the stem "talo" and the ending "a" are segmented into
different morphs. If the word form "taloakin" (sg. partitive case of
the word house with the clitic "kin") doesn't occur in the training
data, we can still esitmate its probability by looking at how probable
the morph combinations "talo + a" and "a + kin" are in the training
corpus.

The model we have chosen is a Markov chain of morph sequences. A
Markov chain on morph sequences is likely to suffer from the data
sparseness problem especially when trying to estimate the probability
of word forms whose stem has occurred few times (perhaps only once) in
the training corpus. Also productive compounding presents a problem,
because (for practical purposes) there are infinitely many ways in
which word stems can combine to form new word forms. We therefore
combine the Markov chain of morph sequences with an HMM which maps key
sequences to letter sequences.

For the Finnish example in the previous paragraph, the HMM gives an
estimate for the probability of the sequence of letters "t-a-l-o",
given the input key-sequence "8-2-5-6". The HMM doesn't include
morph boundaries, so it gives some estimate for the probability of
the wordform "a-l-a-t-a-l-o" (a common Finnish surname) given its keystring
"2-5-2-8-2-5-6", even though the combination of morphs "ala + talo"
would never have been observed in the training data and the morph
sequence model would therefore be unable to give a good estimate for
the probability of the compound word.

Finally many agglutinative languages like Finnish and Turkish
incorporate phonological phenomena, such as vowel harmony, which span
over entire word forms consisting of several morphs. E.g. the Finnish
inessive case ending is "ssA", where "A" is either "a" or "\"{a}"
depending on whether the stem of the wordform has front vowels "ä",
"ö", "y" or back vowels "a", "o", "u" (usually front vowels and back
vowels cannot be mixed). Since "a" and "\"{a}" are typed using the
same key "2", the vowel is always ambiguous in the ending. Consider
the segmented form "talo + i + ssa" (pl. inessive case of the word
house), the final is "a" which can be deduced from the vowel "o" in
the stem, but neither the morph sequence model nor the letter HMM can
disambiguate between "a" and "ä". The morpheme sequence model doesn't
aid in disambiguation, since it looks at adjacent morphs and the stem
and ending "ss\"{a}" are separated by the plural morph "i" (the vowel
"i" is neutral with regard to the vowel harmony phenomenon). Also the
letter sequence HMM fails to disambiguate between the front and back
vowel, since it can only consider the co-occurrence of letters that
are separated by less than three letters and the letters "o" and "a"
in the word form "taloissa" are separated by exactly three letters
(including "a" and "o").

Using a second order morph sequence model, might possibly handle some
cases of vowel harmony, but data sparseness is a serious problem with
the morph sequence model, since there are tens of thousands of
distinct morphs. Therefore long range phenomena like vowel harmony
cannot be adequately captured using n-gram models of morphs or
letters, which has prompted us to use phonological rules which realize
the constraints as part of our system.

The statistical models and phonological rules are implemented as
weighted finite-state transducers, which allows combining them using
the algebraic operations for finite-state transducers. Transducers are
a natural choice for coding arbitrarily long dependencies such as
vowel harmony, which cannot be captured by Markov models.

\subsection{A Hidden Markov Model for Predicting Letter Sequences from Key Sequences}

We denote a sequence of mobile phone keypad keys of length $n$ by $K =
(k_i)_{i=1}^{n}$. Here each $k_i$ corresponds to a key on the mobile
phone keypad. For the key $k$, we denote the set of letters
corresponding to the key $k$ by ${\rm M}(k)$. E.g. ${\rm M}(2) =
\{$a,~b,~c,~\"{a}$\}$ on a typical Finnish mobile phone
keyboard. Correspondingly, we denote a sequences of letter of length
$m$ by $L = (l_i)_{i=1}^{m}$.

The task of the letter model is to give the probability of a letter
sequence $L$ given a sequence of keys $K$. Naturally ${\rm P}(L|K) > 0$, iff
$L \in {\rm M}(K)$. The probability ${\rm P}(L|K)$ is given in equation (\ref{letter-chain-eqn}).  
\begin{equation}\label{letter-chain-eqn}
1
\end{equation}

\subsection{A Markov Chain of Morphs}

A morph of $n$ letters in the training-data is just a sequence of $n$
letters, so we denote it by $L = (l_i)_{i=1}^{n}$. A key sequence $K =
(k_i)_{i=1}^{m}$ corresponds to a sequence of morphs $L_1{\rm
  ... }L_k$ , where each $L_j = (l_{j_i})_{i=1}^{n_j}$, iff $\Sigma_{j
  = 1}^{k} n_j = m$ and $l_{j_i} \in {\rm M}(k_{m_1 + {\rm ... } +
  m_{j - 1} + i})$ for all $l_{j_i}$. We denote the set of morph
sequences that correspond to a key sequence $K$ by ${\rm M}(K)$.

The task of the morph model is to assign a probability for each
sequence of morphs in ${\rm M}(K)$ for a key sequence $K$. The
probability of a sequence of morphs $L_1{\rm ... }L_k \in {\rm M}(K)$
is given by the chain rule of probabilities in equation
(\ref{chain-eqn}).
\begin{equation}\label{chain-eqn}
{\rm P}(L_1{\rm, ..., }\ L_k) = {\rm P}(L_1){\rm P}(L_2|L_1)\ {\rm
  ... }\ P(L_k| L_1{\rm, ..., }\ L_{k - 1})
\end{equation}

We make the standard assumptions for a first order Markov model,
namely that ${\rm P}(L_i | L_1{\rm, ..., }\ L_{i-1}) = {\rm P}(L_i |
L_{i-1})$, which means that we assume that the probability of a morph
occurring depends only on its neighboring morphs and the morph
itself. This allows us to approximate equation (\ref{chain-eqn}) by
equation (\ref{markov-eqn}).
\begin{equation}\label{markov-eqn}
{\rm P}\big(L_1{\rm, ..., }L_k\big) = {\rm P}(L_1){\rm P}\big(L_2 |
L_1){\rm P}\big(L_3 | L_2)\ {\rm ... }\ {\rm P}(L_{k} | L_{k-1})
\end{equation}

In practice we use a training corpus for estimating the probability
${\rm P}(L_i | L_{i-1})$. For the morphs $L_i$ and $L_{i-1}$ we
use the estimate in equation~(\ref{count-estimate}), where ${\rm
  C}(L_{i-1},\ L_i)$ is the number of times that the morph $L_{i-1}$
was followed by the morph $L_i$ in the training corpus and ${\rm
  C}(L_{i-1})$ is the count of the morph $L_{i-1}$ in the training
corpus.
\begin{equation}\label{count-estimate}
{\rm \hat{P}}(L_i | L_{i-1}) = {\rm C}(L_{i - 1},\ L_i) / {\rm
  C}(L_{i-1})
\end{equation}

Since many morphs $L_i$ and $L_{i-1}$ do not occur adjacently anywhere
in the training corpus, we also utilize the unigram estimates ${\rm
  \hat{P}}(L_i) = {\rm C}(L_i)/M$ when estimating the probabilities
${\rm P}(L_i | L_{i-1})$. Here $M$ is the size of the training
corpus. The actual estimate for the probability ${\rm P}(L_i |
L_{i-1})$ is given in equation~(\ref{bigram-estimate}). The
coefficient $a$ is determined by deleted interpolation (see
\cite{Brants:2000}).
\begin{equation}\label{bigram-estimate}
{\rm P}(L_i | L_{i-1}) = {\rm \hat{P}}(L_i | L_{i-1})^a{\rm
  \hat{P}}(L_i)^{1 - a}{\rm,\ where }\ 0\leq a \leq 1\ {\rm.}
\end{equation}

It should be noted that we are dealing with morph sequences and not
letter strings. This is important, since several morph sequences can
correspond to the same string of letters.

\subsection{Phonological Constraints}

We use phonological constraints which are used to filter the results
given by the statistical components of the system. The result given by
the system is thus the most probable string, which satisfies the
phonological constraints. 

The phonological constraints apply on the output of the morph sequence
model and can thus refer to morpheme boundaries. Formally they are
two-level constraints, which can be implemented using the two-level
compiler hfst-twolc.

\subsection{Combining Models using Weighted Finite-State Calculus}

\section{Data and Linguistic Resources}\label{data}

We trained predictive text entry systems for Finnish and Turkish to
evaluate our method. We compare our results with two existing text
entry systems by \cite{silfverberg/2011/cla} and
\cite{Tantug:2010}. There are no standardized test materials for
predictive text entry for Finnish or Turkish, but we were able to
obtain the training materials and test materials used in the previous
systems.

The training materials and test materials for both Finnish and Turkish
were processed in the same way. All uppercase letters were transformed
into lowercase letters and all words that included non-alphabetical
characters were removed. This included among other characters all
numbers and punctuation except the symbol "'" in Turkish, which is
used to signify the boundary between the stem and affix in some word
forms.

\subsection{Finnish}

For training and testing the Finnish text entry system, we use the
same data as \cite{silfverberg/2011/cla}, though they use a
morphological analyzer, which we do not utilize. The training material
is extracted from Finnish IRC logs and contains some 350000 words. The
test material is actual text message data and contains $6663$
words~\footnote{The original test data contains $10851$ words, but it
  turned out that the latter part of the test data file is actually a
  list of unique words, which would skew test results, so we decided
  to only use the earlier half of the material}.

\subsubsection{Phonological Constraints for Finnish}

In Finnish a word form, which is not a compound word, cannot contain
both back-vowels ("a", "o", "u") and front-vowels ("ä", "ö", "y"). We
implemented two-level rules, which realize this constraint on a
morphologically segmented word form.

Figure \ref{fi-constraints} shows the rules we use for Finnish. The
rules disallow an affix with back-vowels, which joins to a stem with
front-vowels. The named regular expressions \verb|Affix|,
\verb|FrontAffix| and \verb|BackAffix| are sets of know inflective and
derivational affixes in Finnish. The expressions \verb|FrontStem| and
\verb|BackStem| denote an arbitrary morph, whose length exceeds three
characters.

\begin{figure}
\begin{verbatim}
"Front Vowel Harmony"
  <[ FrontAffix ]> /<== BackStem Affix* _ ; 

"Back Vowel Harmony"
  <[ BackAffix ]>  /<== FrontStem Affix* _ ; 
\end{verbatim}
\caption{Rules for Finnish vowel harmony.}\label{fi-constraints}
\end{figure}

\subsection{Turkish}

For training and testing the Turkish text entry system, we use the
same materials as \cite{Tantug:2010}. The material is divided into a
test corpus containing $2597$ words and a training corpus which
includes the rest of the words in the material. Thus the training data
and test data are disjoint.

\section{Evaluation}\label{evaluation}

In this section, we present the results of experiments using the
Finnish and Turkish training data and test data presented in the
previous section. For Finnish we examine the impact of varying the
amount of training data on the performance of the predictive text
entry system. For Turkish we present results on the whole test
material.

\subsection{The Keystrokes Per Characters Ratio}

Many factors influence the efficiency of mobile phone text entry in
practice. E.g. the general user interface design of the phone and
specifically the design of the keyboard have a large
impact. Nevertheless such factors are in some sense separate from the
predictive text entry algorithm itself, which makes it plausible to
evaluate the algorithms in isolation from the rest of the user
interface of mobile phones. In this paper we use the keystrokes per
character (KPC) ratio~\cite{MacKenzie02kspc} for measuring the
efficiency of text entry. The KPC ratio for a text entry method is
computed as the average number of keystrokes required to input one
letter in a test corpus. Following \cite{Tantug:2010}, we do not
consider space characters as a part of the test data.

\begin{figure}[htb!]
\begin{center}
\begin{tabular}{llllll}
\texttt{k} & \texttt{u} & \texttt{k} & & \texttt{k} & \texttt{a} \\ 
\hline
\texttt{5-5} & \texttt{8-8} & \texttt{5-5} & \texttt{<NEXT>} &
\texttt{5-5} & \texttt{2}
\end{tabular}
\caption{Inputting the Finnish word "kukka" using the multitap method
  for text entry requires $10$ keystrokes.}\label{kukka-kpc}
\end{center}
\end{figure}

Figure~\ref{kukka-kpc} shows how to enter the word "kukka" (flower) on
a mobile phone with Finnish keypad. Since there are two consecutive
letters "k" in "kukka", a special {\tt <NEXT>}-symbol needs to be
pressed after entering the first "k", in order to tell the text entry
that the next press of key {\tt 5} starts a new symbol, namely the
second "k". This increases the number of keystrokes from $9$ to
$10$. On a training data consisting solely of the word "kukka" the KPC
ratio would thus be "10/5 = 2.0".


When computing the KPC ratio for predictive text entry methods, we assume
that multitap is used as a fallback method when entering OOV words,
i.e. words that are not found among the suggestions given by the
system. In detail, entering an OOV word requires:
\begin{enumerate}
\item Entering the keys for the letters used to write the word (one
  keystroke per letter).
\item Scrolling through the suggestions ($9$ keystrokes in our system,
  since $10$ suggestions are given).
\item Deleting the last suggestion one letter at a time using a
  backspace key {\tt <C> } (one keystroke per letter).
\item Switching to multitap mode using a special key {\tt <MOD> } (one
  keystroke).
\item Inputting the word in multitap mode (keystroke count varies
  depending on the word).
\item Switching back to predictive text entry mode using the key {\tt
  <MOD> } (one keystroke).
\end{enumerate}

\subsection{Results for Finnish}

We constructed $12$ text entry systems using different portions of the
training data for Finnish presented in section~\ref{data}. We used the
first $1000$, $35000$, $69000$, $103000$, $137000$, $171000$,
$205000$, $239000$, $273000$, $307000$, $341000$ and $345337$ words
respectively. The impact of the size of the training data is shown in
figure~\ref{fi-kpc-graph}. The minimum KPC ratio $1.3818$ was attained
for the entire training data consisting of $345337$ words.

We also evaluated the effect of the different components on the KPC of
the predictive text entry system. The results are shown in
figure~\ref{Finnish-kpc-table}.

We compared our system to another published Finnish text-entry system
by~\cite{silfverberg/2011/cla}.  They do not evaluate their system
using the KPC ratio, but we were able to obtain their test results and
according to our experiments they achieve a KPC ratio of
$1.6120$.~\footnote{When examining the test data used by
  \cite{silfverberg/2011/cla}, we discovered, that the latter half of
  the data consisted of a uniquefied wordlist, which effected their
  results negatively. We have computed the KPC ratio for both our own
  system and the system of \cite{silfverberg/2011/cla} using only the
  $6663$ first words in the test data.}

\begin{table}
\begin{center}
\caption{KPC for Finnish Multitap and using the different component of
  our system. The third column shows the improvement over
  multitap.}\label{Finnish-kpc-table}
\begin{tabular}{lll}
\hline
Method &~~~~KPC &~~~~Improvement\\
\hline
Multitap                                       &~~~~2.4018 &~~~~0 \%\\
Letter n-grams                                 &~~~~1.7368 &~~~~27.7 \%\\
Letter n-grams and morph sequence model        &~~~~1.3825 &~~~~43.4 \%\\
Letter n-grams, morph sequence model and rules &~~~~1.3818 &~~~~43.5 \%\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[hbt!]
\includegraphics[width=4in]{finnish_kpc_figure.pdf}
\caption{The effect of the amount of training data on the KPC ratio of
  the Finnish predictive text entry system. The dashed line shows the
  KPC for multitap.}\label{fi-kpc-graph}
\end{figure}

\subsection{Results for Turkish}


\begin{table}
\caption{KPC for Turkish using different input methods. the multitap
  method. The third column shows the improvement over
  multitap}\label{Turkish-kpc-table}
\begin{center}
\begin{tabular}{lll}
\hline
Method &~~~~KPC &~~~~Improvement\\
\hline
Multitap                          &~~~~2.4386 &~~~~0 \%\\
Letter n-grams~\cite{Tantug:2010} &~~~~1.4382 &~~~~41.0 \%\\
Our method                        &~~~~1.1768 &~~~~51.7 \%\\
\hline
\end{tabular}
\end{center}
\end{table}

\section{Discussion}\label{discussion}

\section{Conclusions and future work}\label{conclusion}

\section{Acknowledgments}

\bibliographystyle{splncs03}
\bibliography{cicling2011.bib}
\end{document}
