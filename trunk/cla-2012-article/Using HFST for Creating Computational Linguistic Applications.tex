\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage[utf8x]{inputenc}
\usepackage{multirow}
\usepackage{caption}
\usepackage{url}
\usepackage{amsmath}

\usepackage[all]{xy}
\entrymodifiers={++[o][F-]}
\SelectTips{cm}{}

%
\begin{document}
%
\title{Using HFST for Creating Computational Linguistic Applications}
%
\author{Krister Lind\'{e}n \and Erik Axelson \and Senka Drobac \and\\
Sam Hardwick \and Tommi A Pirinen \and Miikka Silfverberg}

\institute{University of Helsinki\\
Department of Modern Languages\\
Unioninkatu 40 A\\
FI-00014 Helsingin yliopisto, Finland\\
\email{\{krister.linden, erik.axelson, senka.drobac, sam.hardwick,\\
tommi.pirinen, miikka silfverberg\}@helsinki.fi}}

\maketitle

\begin{abstract}
\sloppy HFST – Helsinki Finite-State Technology (\url{hfst.sf.net})
is a framework for compiling and applying linguistic descriptions
with finite-state
methods. HFST currently connects some of the most important finite-state
tools for creating morphologies and spellers into one open-source
platform and supports extending and improving the descriptions with
weights to accommodate the modeling of statistical information. HFST
offers a path from language descriptions to efficient language
applications.

HFST is designed for compiling morphologies and for that purpose it
contains open-source replicas of xfst, lexc and twolc which are
well-known and well-researched tools for morphology building. They
support both parallel and cascaded application of transducers. In
addition, HFST offers the capability to train and apply part-of-speech
taggers on top of the morphologies using weighted finite-state transducers.

With the morphology and tagger capabilities a number of applications
have been created, e.g. spellers for close to 100 languages and
hyphenators for approximately 40 different languages. The spellers were
derived from open-source dictionaries and integrated with OpenOffice and
LibreOffice. E.g. a full-fledged Greenlandic speller, which is a
polyagglutinating language, is currently available only via HFST for
OpenOffice. Using the tagger capability of HFST, we have created an
improved spelling suggestion mechanism for words in context as well as
better predictive text input for mobile phones for highly inflecting
languages like Finnish. Other writer’s tools created with HFST include
inflecting thesauri and translation dictionaries. We also offer lexicon
compilation and preprocessing for the Apertium machine translation
software.

For the processing of large corpora, e.g. for text indexing purposes, a
high-performing look-up facility is provided with HFST offering
110.000-440.000 words per second of morphological processing. A recent
extension of the look-up facility offers a utility to create named
entity recognizers for information extraction purposes on top of the
lookup. 
\keywords{no, keywords, yet}
\end{abstract}

\section*{Introduction}
% Krister

\section{Building Morphologies}

\subsection{Introduction}

One of the earliest and most important goals of the HFST project has
been to provide open-source utilities for compiling full-fledged
morphological analyzer, which can be used in constructing spellers,
taggers and other language technological utilities of good
quality. The Xerox toolkit \cite{beesley/2003} is among the most
widely used frameworks for constructing morphological utilities. The
toolkit is used to compile linguistic description into morphological
analyzers. More specifically, Xerox tools include the finite-state
lexicon compiler lexc, the two-level rule compiler twolc and the
cascading replace rule compiler xfst. HFST includes the tools
hfst-lexc, hfst-twolc and hfst-xfst, which provide full backward
compatibility with Xerox tools and augment their functionality.

An lexicon-file for the lexicon compiler hfst-lexc the morphotax of a
language using sub-lexicons which are lists of morphs. Later a rule
component is used to realize phonological changes which occur at the
boundaries of morphs.

The rule compilers hfst-xfst and hfst-twolc provide partially the same
functionality. Both are used to realize morphophonological variations
on an hfst-lexc lexicon. The difference between the tools is that
hfst-xfst rules are applied in succession gradually altering the
lexicon and hfst-twolc rules are applied as parallel constraints,
which limit the realizations of morphophonemes used in the hfst-lexc
lexicon..

\subsection{Parallel Rules with Negative Contexts and Regular Expression Centers}

The rule compiler hfst-twolc provides backward compatibility with the
Xerox tool twolc, but it augments twolc functionality by providing two
new types of rules. In hfst-twolc rules can have centers which are
regular expressions and the can have negative contexts.

\begin{figure}
\begin{verbatim}
"Rule with negative context"
x:y <=> a b _     ;
        except
            _ ( c ) d ;

"Rule without negative context"
x:y <=> a b _ [ ? - [ c | d ] | c [ ? - d ] | .#. ] ;
\end{verbatim}
\caption{Negative context rule and corresponding traditional two-level
  rule}\label{negative-context-rule}
\end{figure}

In traditional two-level rules it can be very difficult to express
that a certain alternation should not occur in a given
context. Sometimes conflict resolution of two-level rules handles
this, but conflict resolution works only if rules can be ordered into
chains of sub cases. Often this can be difficult to accomplish,
especially for grammar writers who are not especially well acquainted
to writing regular expressions.

Using an extension to the xerox two-level rule syntax in hfst-twolc,
it is possible to formulate rules with negative context, i.e. context
which prohibit the triggering of the
rule. Figure~\ref{negative-context-rule} shows a schematic example of
a negative context rule and a traditional xerox style rule, which has
the same effect as the rule with the negative context.

\begin{figure}
\begin{verbatim}
"ZZ normally realized as xy except between two a:s"
<[ Z:x Z:y ]> <==> _      ;
                   except
                   a _ a  ;

"Swap x and y between two a:s"
<[ Z:y Z:x ]> <==> a _ a ; 
\end{verbatim}
\caption{Rule with regular expression center and two auxiliary rules which}
\end{figure}

Regular expression centers allow rules to govern longer segments than
single symbols. This can be useful when describing languages, with
complex allomorphy.

\subsection{Cascaded Rules: Explanations and Examples}
% Senka
HFST replace rules provide backward compatibility with XFST replace rules, described in Kempe and Karttunen \cite{Kempe96parallelreplacement} and the Finite State Morphology book \cite{beesley/2003}. Although they share the same notation, HFST replace rules were developed from the concept of Generalized Lenient Composition, described in Yli-Jyr\"{a} \cite{YliJyra/2008b}. 

In this paper section, we will describe how to use Replace Rules with HFST tools. For a detailed description of each rule, see Beesley and Karttunen \cite{beesley/2003}.



% Simple rules

\textbf{Simple rules.} A simple right arrow replace rule,
\begin{equation}
A \rightarrow\ B
\end{equation}

where A and B are regular expressions that describe languages, states that A in the upper language maps to B in the lower language.

Replace rules can be compiled into transducer using HFST tool \textbf{hfst-regexp\-2fst}.
This tool takes a regular expresion as input and gives a corresponding transducer written in a binary file as output. To convert transducers from binary format to text format, there is a HFST tool \textbf{hfst-fst2txt}. Therefore, the upper rule could be compiled as in Figure~\ref{fig:simple_replace}.

\begin{figure} [h!]
\begin{verbatim}
$ echo "A -> B ;" | hfst-regexp2fst | hfst-fst2txt
\end{verbatim}
\caption{Compiling simple replace rule}
\label{fig:simple_replace}
\end{figure}


The regular expression can be read from a file, or saved to a file (Figure~\ref{fig:read_from_file}). 

\begin{figure} [h!]
\begin{verbatim}
$ echo "A -> B ;" > regex
$ hfst-regexp2fst -i regex -o transducer
$ hfst-fst2txt transducer
\end{verbatim}
\caption{Reading from, and writing to a file}
\label{fig:read_from_file}
\end{figure}


Rules can be applied to any language by using composition. By composing a word \verb!ABCD! with the previous rule \verb!A -> B!, the result is the transducer in Figure~\ref{fig:replace_compose}. It has 5 states (0 -- 4), and only state 4 is final. The \textbf{hfst-regexp2fst's} default output transducer format is \textbf{tropical\_openfst}, which has a support for weights. Since weights aren't used in replace rules, in this example all of them have value 0.000000. Other formats that can be used with this, and other HFST tools, are \textbf{sfst} and \textbf{foma}.

\begin{figure} [h!]
\begin{verbatim}
$ echo "A B C D .o. A -> B ;" | hfst-regexp2fst | hfst-fst2txt
0       1       A       B       0.000000
1       2       B       B       0.000000
2       3       C       C       0.000000
3       4       D       D       0.000000
4       0.000000
\end{verbatim}
\caption{Composing rule to a word}
\label{fig:replace_compose}
\end{figure}


\begin{figure} [h!]
\begin{verbatim}
$ echo "c a t .o. c a t -> c a t s  ;" |
> hfst-regexp2fst -f sfst | hfst-fst2txt
0       1       c       c
1       2       a       a
2       3       t       t
3       4       @0@     s
4
\end{verbatim}
\caption{Using sfst transducer format}
\label{fig:sfst_format}
\end{figure}



% Context

\textbf{Context.} Every rule can have a context in which the replacement is made. Just like A and B, L and R are also regular expresions that describe languages. Therefore, A will be mapped to B if, and only if, it is between L and R.
\begin{equation}
A \rightarrow\ B\ ||\ L \_\  R
\end{equation}
 
Also, multiple context pairs are supported, when separated by comma.
\begin{equation}
A \rightarrow\ B\ ||\ L_1 \_\  R_1 ,\ \ldots\ ,\ L_i \_\  R_i
\end{equation}

In Figure~\ref{fig:multiple_contexts}, the rule expression says that \verb!a! in the upper language is mapped to \verb!b! in lower language when between \verb!m! and \verb!n!, or in front of \verb!b!. In the word \verb!manaab!, the first and last \verb!a! are mapped to \verb!b!, because they are betwwen corresponding contexts, but the second \verb!a! is kept unchanged.

\begin{figure} [h!]
\begin{verbatim}
$ echo " m a n a a b .o. a -> b || m _ n , _ b;" |
> hfst-regexp2fst | hfst-fst2txt
0       1       m       m       0.000000
1       2       a       b       0.000000
2       3       n       n       0.000000
3       4       a       a       0.000000
4       5       a       b       0.000000
5       6       b       b       0.000000
6       0.000000
\end{verbatim}
\caption{Replace rule between two contexts}
\label{fig:multiple_contexts}
\end{figure}

\begin{table} [h!]
\centering
\caption{Different context directions in Replace Rules}
\begin{tabular}{ | c | c | }
\hline
\ \verb!||!\ & \ both contexts are taken from the upper language \\ \hline
\ \verb!//!\ & \ left context is taken from the lower language, right from the upper \\ \hline
\ \verb!\\!\ & \ left context is taken from the upper language, right from the lower \\ \hline
\ \verb!\/!\ & \ both contexts are taken from the lower language. \\ \hline
\end{tabular}
\label{tab:context_directions}
\end{table}

In every replace rule, where A is the upper, and B the lower language, there are four context directions that can be used with replace rules. For example, when the \ \verb!//!\ sign is used as context orientation operator, the left context will be taken from the lower language. It means that it is possible for the replace function to write it's own context. This is shown in Figure~\ref{fig:context_orientation}. 

\begin{figure} [h!]
 \begin{verbatim}
$ echo " b a a .o. a -> b // b _ ;" | ./hfst-regexp2fst | hfst-fst2txt
0       1       b       b       0.000000
1       2       a       b       0.000000
2       3       a       b       0.000000
3       0.000000
\end{verbatim}
\caption{Replace rule writes it's own context}
\label{fig:context_orientation}
\end{figure}




% Other replace functions

\textbf{Other replace functions.} In all examples, we used the right arrow replace operator, but there are many other operators that can be used. All the operators listed in Table~\ref{tab:replace_operators} have their left arrow version, which are the inversion of the right operator. Furthermore, all the rules can be used with epenthesis \verb![. .]! and markup \verb!...! operators. The epenthesis operator should be used with empty strings to avoid replacing infinitely many epsilons, while the markup operator is used to insert markers around a word.

\begin{table} [h!]
\centering
\caption{List of right replace operators that can be used in HFST}
\begin{tabular}{| c | c |} 
\hline
Right replace operators & Replace function \\ \hline\hline
\verb!->!   & Replace \\ \hline
\verb!(->)! & Replace optional \\ \hline
\verb!@->!  & Longest match from left to right \\ \hline
\verb!->@!  & Longest match from right to left \\ \hline
\verb!@>!   & Shortest match from left to right \\ \hline
\verb!>@!   & Shortest match from right to left \\ \hline
\end{tabular}
\label{tab:replace_operators}
\end{table}



\begin{figure} [h!]
 \begin{verbatim}
$ echo " a a a .o. [.a*.] @-> b ;" | hfst-regexp2fst > a.fst
$ echo " b a a b .o. a @-> %[ ... %] ;" | hfst-regexp2fst > b.fst
\end{verbatim}
\caption{Epenthesis and markup operators}
\label{fig:epenthesis_markup}
\end{figure}


% Parallel rules

\textbf{Parallel rules.} Parallel rules are used when there are multiple replacements at the same time. The general parallel replace rule expression consists of individual replace rules divided with two commas.
\begin{equation}
A_1 \rightarrow\ B_1\ ||\ L_1 \_\  R_1\ ,,\ \ldots\ ,,\ A_i \rightarrow\ B_i\ ||\ L_i \_\ R_i
\end{equation}

In XFST, all rules in a parallel rule expression have to have the same format. They need to have the same arrow and the same context layout. In HFST, the restraint that all the rules need to have the same arrow is kept, but the context format for each rule can differ from each other. Therefore, the rule expression in Figure ~\ref{fig:parallel_rules} would not be allowed in XFST, but is valid in HFST.
\begin{figure}
 \begin{verbatim}
$  echo " a -> b || m _ n ,, c -> d ;" | ./hfst-regexp2fst | hfst-fst2txt
\end{verbatim}
\caption{Parallel rules with different contexts}
\label{fig:parallel_rules}
\end{figure}




\subsection{Morphosyntax and Morphological Formul\ae}

Morphosyntax is the component dealing with morphological combinatorics in
language descriptions. This concerns word-formation processes, such as
affixation, compounding and more. There are numerous more or less widely
accepted formalisms for describing the morphosyntax in natural language
processing applications, such as
hunspell\footnote{\url{http://hunspell.sf.net}} (and its older *spell
relatives), apertium lttoolbox\footnote{\url{http://apertium.sf.net}} or Xerox
lexc\cite{beesley/2003}. The HFST tool set contains parsers and compilers to
read descriptions in those languages and compile a finite-state automaton for
rest of the HFST tools to process out from them
\cite{pirinen/il/2010,pirinene/2009/sfcm,pirinen/2012/lrec}. In this sections
we draw shortly summarise the common features of the algorithms used to compile
all the descriptions and generalise the formula over morphosyntax for
finite-state automata, described fully in terms of standard finite-state
operations.

It is true for all the contemporary morphosyntax implementations that they more
or less draw from item-and-arrangement style combinatorics over sets of
morphs or morphemes. In general form such a system can be described as a
disjunction of morphs repeated infinitely over each other, combined with
morphosyntactic filter describing precisely the right combinations of morphemes.
Both morphemes and the filters can be trivially described in finite-state form.

As an example, starting from a morpheme, that is a path or a language, e.g.  a
morpheme \emph{dog} would be automaton encoding a path \texttt{d o g}, and
\emph{cat} path \texttt{c a t}, similarly a plural marker \emph{-s} of English
would be a path \texttt{s} whereas singular marker \emph{$\emptyset$}
\texttt{$\varepsilon$} (marking empty morph). For morphosyntax of this language
\emph{dog} and \emph{cat} would form a set called e.g. \emph{Nouns} and
\emph{$\emptyset$} and \emph{s} a set of called \emph{Number}. A valid
morphosyntax here would be Start $\rightarrow$ Nouns $\rightarrow$ Number
$\rightarrow$ End. Now, that morphosyntax can be compiled as finite-state
automaton such that we replace $\rightarrow$ in the former description with the
morphs. Then we decorate the actual morphs with markers for e.g. starts of the
sets named \emph{Nouns} and \emph{Number}. The final language is now simply
composition of that morphosyntax over the kleene plus closure of morpheme sets.


\subsubsection{For Lexicographers}
% Tommi

\subsubsection{For Grammarians}
% Tommi & Miikka

\subsection{Performance}
% Erik

In Table~\ref{tab:compilation_times} we show compilation times for different 
morphologies using different HFST back-ends. The morphologies are OMorFi 
\cite{pirinen/2008} for Finnish, Morphisto \cite{zielinski/2009} for German,
Morph-it \cite{Zanchetta_2005-1} for Italian, Swelex 
\footnote{\url{https://kitwiki.csc.fi/twiki/bin/view/KitWiki/HfstSwelex}} 
for Swedish and TrMorph \cite{Coltekin/2010} for Turkish. 
OMorFi, Morphisto and Morph-it have several rules for
inflection and compounding, Morph-it and Swelex are basically word-lists.
We use HFST version 3.3.4, with SFST, OpenFst and foma as
back-ends. The times are average values counted from 10 compilations.


\begin{table} [h!]
\centering
  \caption{Compilation times for different morphologies with
    different HFST back-ends. The times are given in minutes and seconds
    and averaged over 10 compilations. HFST version is 3.3.4.}
  \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    Back-End & Finnish & German & Italian & Swedish & Turkish \\ \hline\hline
    SFST & 2:48 & 2:12 & 0:30 & 0:13 & 0:12 \\ \hline
    OpenFst & 7:52 & 7:45 & 2:24 & 0:49 & 0:40 \\ \hline
    foma & 1:52 & 1:33 & 0:31 & 0:13 & 0:05 \\ \hline
    \end{tabular}
  \label{tab:compilation_times}
\end{table}


In Table~\ref{tab:compilation_times_versions} we show both the current compilation 
times and the ones that we got in our earlier benchmarking \cite{hfst/2011} for 
Finnish and German. We also show the back-end versions used.

\begin{table} [h!]
\centering
  \caption{Compilation times for different morphologies with
    different HFST back-ends and their versions. 
    The times are given in minutes and seconds.}
  \begin{tabular}{| c | c | c | c |}
  \hline
  Back-End                 & version  & Finnish  & German \\ \hline\hline
  \multirow{2}{*}{SFST}    & 1.4.2    & 5:02     & 6:39 \\
                           & 1.4.6    & 2:48     & 2:12 \\ \hline
  \multirow{2}{*}{OpenFst} & 1.2.7    & 6:51     & 6:28 \\
                           & 1.2.10   & 7:52     & 7:45 \\ \hline
  \multirow{2}{*}{foma}    & 0.1.14   & 1:49     & 1:29 \\
                           & 0.1.16   & 1:52     & 1:33 \\
\hline
  \end{tabular}
  \label{tab:compilation_times_versions}
\end{table}

It can be clearly seen that the performance of HFST with SFST as back-end 
has improved: the compilation time of the Finnish morphology has almost
halved and the compilation time of the German morphology is only one third of the
time at previous benchmarking. 
This improvement comes from the newer version of SFST that has more optimized
composition and Hopcroft minimization functions. The improved functions 
were developed jointly with the HFST team based on our previous benchmarking.

The compilation times of HFST with OpenFst as back-end have become slower.
We are still looking into this fact trying to find out whether
the problem lies in the HFST code or if we are not using the features of 
OpenFst's newer version right.
The performance of HFST with foma has stayed almost the same, only a small
growth of a couple of seconds can be seen.
 

\section{Building Taggers}

\subsection{Introduction}
% Miikka

\begin{itemize}
\item Possible to construct HMM type taggers using different
  combinations of tags and wordforms~\cite{Silfverberg/2010/IceTal}
  and \cite{Silfverberg/2011}.
\item Accuracy comparable with TnT~\cite{Brants:2000}.
\item There are some new optimizations to the lexical model and
  sequence model. Especially we implement a version of the Viterbi
  algorithm for weighted treansducers.
\end{itemize}


\subsection{Including Morphologies without Harmonizing Tagsets}
% Miikka

\begin{itemize}
\item Tnt-type guesser work poorly for agglutinative languages.
\item Paradigm-based guessers for agglutinative languages using
  morphological analyzers work better.
\item Differing tag sets are a problem. It is sometimes difficult to
  find good translations between tag sets because the underlying
  linguistic description can differ a lot.
\item HFST provides a way to use a morphological analyzer to construct
  a paradigm based guesser, without harmonizing tag sets.
\item Words are transformed to their corresponding analysis sets and
  the analysis sets are associated with tags and corresponding
  conditional probabilities.
\item When a previously unseen word is encountered while tagging, it
  is transformed into its set of analyses and the tag probabilities
  for that set are used.
\item For words unknown to the morphological analyzer or for words
  whose analysis set does not figure in the training data, a
  suffix-based guesser is used.
\end{itemize}

\subsection{Optimization}
% Miikka
\begin{itemize}
\item Viterbi-style optimization.
\item Beam-search for Finnish, where the tag set is huge.
\end{itemize}

\section{Applying Transducers}

\subsection{Introduction}
% Tommi

\subsection{Spellers}

The task of finite-state spell-checking is well researched and documented. It
consists mainly of two phases, identifying incorrect word forms and creating
suggestions for corrections. For incorrect word-forms there are two types of
mistakes, non-word spelling errors, such as writing \emph{cta} where \emph{cat}
is meant, and real-word spelling errors, such as writing \emph{there} where
\emph{their} is intended. The method for finding former in finite-state system
is simply applying the dictionary to the text word by word, any unrecognised
string not belonging to the language of the dictionary automaton is a non-word
spelling error. For real-word errors a statistical n-gram model or syntactic
parser is typically required\cite{}. To correct a spelling error in finite-state
system, a two-tape automaton modeling the typing errors should be applied to
the misspelt string to get set of potential corrections\cite{pirinen/lrec/2010}.
For practical purposes the error model can also be implemented as
fuzzy finite-state traversal algorithm or similar methods\cite{,}. The result
of the correction step is a set of word-forms that are correct in the
language of the spell-checking dictionary. Another related task is to rank this
set to provide the most likely corrections first for the end-user of spelling
correction system. A trivial way to perform such ranking would be to use
unigram \cite{pirinen/lrec/2010} or n-gram probabilities of the words 
\cite{mays/1991} or word-form analyses \cite{pirinen/cicling/2012}.

\subsubsection{Creating Spellers}
% Tommi

The creation of finite-state speller requires compiling of (at least) two
automata: a dictionary that contains the correctly spelt word-forms and the
error model that can rewrite misspelt word-forms into correctly spelled ones.
The former automaton can be as simple as a reference corpus, containing larger
quantities of correctly spelled words and (possibly) smaller quantities of
misspelt ones\cite{norvig/2010}. Also more elaborate dictionaries, such as
morphological analysers described in sections \ref{,,} can be used, and also
trained for spell-checking purposes with reference corpora without any big
modifications to underlying implementation\cite{pirinen/lrec/2010}.

For the error-model we can trivially construct an automaton corresponding to
Levenshtein edit distance algorithm\cite{,}. For more elaborate error models
it is possible to use hunspell algorithms as automata \cite{pirinen/il/2010} or
construct further extensions by hand \cite{pirinen/lrec/2010}. Given a possibly
aligned error corpus it is also possible to construct a weighted error model
automaton automatically \cite{}.

\subsubsection{Checking Strings and Generating Suggestions}
% Sam
String checking is a straightforward process of matching against the lexical
automaton, or calculating the output language of $I \circ L$, where $I$ is the
input and $L$ is the morpholexical automaton. If the result is empty, the
string is absent from the lexicon and the correction set must be calculated.

A corrected string is a string that can be generated by transforming the
input string with the error source and is present in the lexicon.
These strings are thus the output language of the composition
$I \circ \ E \circ L$, where $E$ is the error source.

The desired behaviour, or result set $R$, of a speller given input $I$, a
lexicon $L$ and an error source $E$ is thus

\begin{equation*}R = \begin{cases}
\pi_2(I \circ L), & \mbox{if } \pi_2(I \circ L) \neq \emptyset \\
\pi_2(I \circ E \circ L) & \mbox{otherwise}\\
\end{cases}
\end{equation*}

where $\pi_2$ is the output projection.

It is undesirable to compute either of the intermediate compositions
$I \circ E$ and $E \circ L$; the former will require futile work (producing
strings that aren't in the lexicon) and the latter, though possible to
precompute, will be very large. For most typical case of an edit-distance
error model with $a$ symbols and distance $d$ acting on a lexical entry with
length $l$, there are on the order of $(a \times l)^d$
copies of the lexical transducer along the path (for each transition,
an edit may or may not be made, leading to as many different continuations
as edits can be made).

To circumvent these problems, a three-way online composition of $I$, $E$ and
$L$ was implemented (distributed as \verb!hfst-ospell!
under the GPL and Apache licenses).

We write $I = {Q^I, 0^I, T^I, E^I}$ and so on, where $Q$
is the set of states, $0$ is the starting state, $T$ is the set of terminal
states and $E$ is the set of transitions (triples of (state, symbol pair
(input and output), state)).

Eliding for the time being weights and flag diacritics, states in the
composition transducer are triples of states of the component transducers.
Analogously with two-way composition, the starting state is $(0^I, 0^E, 0^L)$
and the edge $e = (q_1, (\sigma_1, \sigma_2), q_2)$ exists if edges
$(q^I_1), (\sigma_1, \sigma'), q^I_2)$,
$(q^E_1), (\sigma', \sigma''), q^E_2)$ and
$(q^L_1), (\sigma'', \sigma_2), q^L_2)$ exist in the respective transducers
for some
$\sigma' \in \Sigma^I \cap \Sigma^E, \sigma'' \in \Sigma^E \cup \Sigma^L$,
where $q_n = (q^I_n, q^E_n, q^L_n)$ and $\sigma_1 \neq \epsilon \neq \sigma_2$.

If in a given state any of the component transducers has an edge involving
epsilon and the other symbols match as above, the resulting composed edges
go to states such that any state to the left of an input epsilon or to the
right of an output epsilon is unchanged, eg. if we have
$(q^I_1, (\sigma, \sigma'), q^{I'})$, $(q^E_1, (\epsilon, \sigma''), q^{E'})$ and
$(q^L_1, (\sigma'', \sigma'''), q^{L'})$, a successor state of $q_1$ will be
$(q^I_1, q^E_2, q^L_2)$.

The final states are $T = T^I \times T^E \times T^L$.

It can trivially be verified that this is equivalent to taking the two
compositions $(I \circ E) \circ L$.

This three-way composition is computed in a breadth-first manner with a
double-ended queue of states. The queue is initialised with the starting state,
and the target of every edge is computed and pushed onto the
queue. The starting state is then discarded, and the process is repeated
with a new state popped from the queue until the queue is empty.

For this process to be guaranteed to terminate, it is sufficient that none of
the component transducers have input-epsilon loops and that the input
transducer accepts strings of only finite length. This is because every newly
generated state will either have a shorter sequence of edges to traverse in
$I$ ("increment $q^I$") or be closer to requiring an edge in $I$ to be traversed
(due to a finite sequence of epsilon edges becoming shorter), establishing a
loop variant.

Weights representing the probability of a particular correction being the
correct one are a natural extension, and in \verb!hfst-ospell! are implemented
as multiplication in the tropical semiring
($x \oplus y = \min(\{x, y\}), x \otimes y = x + y$) of each edge traversed.
Each state in the queue is recorded with an accumulated weight, and its
successor states have this weight incremented by the sum of the weights of
the edges tranversed in the component transducers.

The alphabet of $I$ cannot be determined in advance, and in practice is taken
to be the set of Unicode code points. To allow the error model to correct
unexpected symbols in this large space, \verb!hfst-ospell! uses a special
symbol, \verb!@_UNKNOWN_SYMBOL_@! which is taken to be equal to any symbol
that is otherwise absent.

\subsubsection{Error model tool and optimizations}
For the most common case of generating Levenshtein and
Damerau-Levenshtein (in which transposition of adjacent symbols constitutes
one operation) distance error models, a tool (\verb!editdist.py!) and
definition format was developed.

The definition format serves the purposes of minimizing the number of symbols
used in the error source (the number of transitions is $\mathrm{O}(|\Sigma|^2)$)
and introducing weights for edits. Typical
morphologies have a number of unusual characters (punctuation, special symbols)
or internally used symbols (eg. flag diacritics) that should be filtered for
more efficient correction. This is accomplished by providing a facility for
reading the alphabet from a transducer, ignoring any symbols of more than
one Unicode code point, and reading further ignorable symbols from a
configuration file.

The configuration file allows specifying weights to be added to any edit
involving a particular character or a particular edit operation (for example,
assigning a low weight to the edit o $\rightarrow$ ö for an OCR application).

Certain characteristics of the spelling correction task permit
efficiency-oriented improvements to error models. A naive Levenshtein error
model with edit distance 2 in \verb!ospell! would, when given the word
word \verb!arrivew! where the French word \verb!arriver! was intended would
do the following useless work:

\begin{verbatim}
a:0 0:a r r i v e    [failure]
0:a a:0 r r i v e    [failure]
a r:0 0:r r i v e    [failure]
a 0:r r:0 r i v e    [failure]
...
a r r i v e w:0      [success]
a r r i v e w:z      [success]
a r r i v e w:r      [success]
...
\end{verbatim}

When the correctable error is near the end, almost every symbol is deleted and
inserted with no effect. This may be circumvented by adding, for each deletion
and insertion, a special successor state from which its inverse is absent.

\subsubsection{Ranking Suggestions}
% Tommi & Miikka

Applying the error model and the language model to the spelling error strings,
a result is typically an ordered set of corrected strings with some probability
associated to each correction. After applying the contextless error correction
described earlier it is possible to use context words and their potential
analyses to re-rank the corrections \cite{pirinen/cicling/2012}.

\subsection{Synonym and Translation Dictionaries}
% Krister

\section{Extending Transducers for Pattern Matching}
% Sam
Advanced, fast and flexible pattern matching is a major requirement for a
variety of tasks in information extraction, such as named entity recognition.
An approach to this task was presented by Lauri Karttunen in
\cite{karttunen2011}, and an outline of implementing it in HFST is given here.

\subsection{The Task}

Some desiderata for pattern matching are:

\begin{itemize}
\item Several patterns, including nested matches,
should be able to operate in parallel
\item It should be possible to impose contextual requirements (rules) on
the patterns to be matched
\item Matching should be efficient in space and time - in particular, it should
be possible to avoid long-range
dependencies which are awkward for FST transformations
\item A facility for referring to common subpatterns, such as dates, by name
\end{itemize}

\subsection{The pmatch Approach}

For a detailed overview the reader is directed to \cite{karttunen2011}.
Here we focus on the aspects of \verb!pmatch! that necessitate extensions
to a FST formalism from the point of view of the implementation.

\verb!pmatch! is presented as a tool for general-purpose pattern matching,
tokenizing and parsing. It is given a series of definitions and a
specialised regular expression, and it then operates on strings, passing
through unmodified any parts of them that fail to match the expression,
and applying transformations to any matched parts. If several matches beginning
from a common point in the input can be made, matches of less than maximal
length are considered invalid.

The expressions may refer to other expressions (possibly combined with each
other by operations on regular languages), contextual conditions
(left or right side, with negation, \verb!OR! and \verb!AND!) and certain
built-in transformation rules. The most interesting of these transformation
rules is \verb!EndTag()!, which triggers a wrap-around XML tag to be inserted
on either side of the match.

Referencing other regexes (including self-reference) is unrestricted, so
the complete system has the power of a recursive transition network (RTN),
and matching is therefore context-free.

The crucial extension-demanding features, as we see them, for a HFST utility
with similar applications are:

\begin{itemize}
\item A distinction between an augmented universal transducer (the top level
which echoes all input in the absence of matches) and sub-transducers
\item Ignoring non-maximal matching, ie. a left-to-right longest-distance
matcher
\item An unrestricted system for referencing other transducers by name
\item Special handling of \verb!EndTag()! and instructions for context
checking
\item Reserved symbols for implementing transducer insertion/reference,
\verb!EndTag()! and contexts
\end{itemize}

The referencing system is apparently the only one of these that would be
impossible to implement in a strict FST framework; the others suggest
compilations to larger, less efficient transducers.

\subsection{Implementation in a FST library}

Whereas \verb!pmatch! offers a unified interface with various built-in
transformations, sets and a Xerox-inherited syntax and environment,
the present effort is focused on an API that would support the development
of more specific applications.

The call stack is a convenient metaphor for the referential system. When a
transducer is loaded for matching in the initial position, a referential space
of transducers is populated by recursively traversing the alphabet, looking
for references to other transducers. This process is repeated until all
possible references have been located.

At every symbol in the input, the top level attempts to transform the longest
possible continuation (or, if the same maximal continuation can be transformed
in multiple ways, returns one of them, typically with a weighting system).
Failures to do this are interpreted as successful identity matches.

The encountering of an insertion symbol (eg. \verb!@I.FinnishAdjective@!)
triggers the calling of a subnetwork transducer with the transducer in
question. These transducers share the full input buffer (for checking
an arbitrary amount of context) but have independent spaces for flag
diacritics, possible alphabet-dependant special symbols (such as \verb!IDENTITY!
or \verb!UNKNOWN!).

\section{Discussion}

\section{Conclusion}

\subsubsection*{Acknowledgments}

\bibliographystyle{splncs03}

\bibliography{hfst2012,cla2012hfst}

\end{document}
% vim: set spell:
