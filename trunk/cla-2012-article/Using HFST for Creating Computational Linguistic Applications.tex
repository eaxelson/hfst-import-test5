\documentclass[draft]{llncs}
\usepackage{llncsdoc}
\usepackage[utf8x]{inputenc}
\usepackage{multirow}
\usepackage{url}
\usepackage{amsmath}

\begin{document}
%
\title{Using~HFST~for~Creating Computational~Linguistic~Applications}
%
\author{Krister Lind\'{e}n \and Erik Axelson \and Senka Drobac \and\\
  Sam Hardwick \and Miikka Silfverberg \and Tommi A Pirinen }

\institute{University of Helsinki\\
  Department of Modern Languages\\
  Unioninkatu 40 A\\
  FI-00014 Helsingin yliopisto, Finland\\
  \email{\{krister.linden, erik.axelson, senka.drobac, sam.hardwick,\\
    miikka.silfverberg,tommi.pirinen\}@helsinki.fi}}

\maketitle

\begin{abstract}
  \sloppy HFST – Helsinki Finite-State Technology (\url{hfst.sf.net})
  is a framework for compiling and applying linguistic descriptions
  with finite-state methods. HFST currently collects some of the most 
  important finite-state tools for creating morphologies and spellcheckers into 
  one open-source platform and supports extending and improving 
  the descriptions with weights to accommodate the modeling of 
  statistical information. HFST offers a path from language descriptions 
  to efficient language applications. In this article, we focus 
  on aspects of HFST that are new to the end user, i.e. new tools, new features in
  existing tools, or new language applications, in addition to some revised algorithms 
  that increase performance.

  \keywords{finite-state applications, morphology, tagging, HFST}
\end{abstract}

\section{Introduction}
%Krister

HFST – Helsinki Finite-State Technology (\url{hfst.sf.net}) is designed 
for creating and compiling morphologies, which has been documented in, e.g.,  
\cite{linden/2009/sfcm,linden/2011/sfcm}. In this article we focus on the applications 
created with HFST and some of their theoretical motivations. HFST
contains open-source replicas of xfst, lexc and twolc which are
well-known and well-researched tools for morphology building, see \cite{beesley/2003}. The tools
support both parallel and cascaded application of transducers. 
This is outlined in Section~\ref{LexiconTools} along with some new and 
previously undocumented extensions to the Xerox tools in HFST.  

There are a number of tools for describing morphologies. Many of them 
start with the item-and-arrangement approach in which an arrangement of 
sublexicons contain lists of items that may continue in other sublexicons. A formula
for compiling such lexical descriptions was documented in \cite{linden/2009/sfcm}.
In Section~\ref{MorphTools}, we demonstrate a simplified procedure 
for how such morphotactic descriptions 
can be compiled into finite-state lexicons using finite-state operations. 
To realize the morphological processes, rules may be applied to the finite-state lexicon. 
In addition, HFST now also offers the capability to train and apply part-of-speech
taggers on top of the morphologies using parallel weighted finite-state transducers on 
text corpora, which is outlined and evaluated in Section~\ref{PosTools}.

Using compiled morphologies, a number of applications
have been created, e.g. spellcheckers for close to 100 languages and
hyphenators for approximately 40 different languages. The spellcheckers were
derived from open-source dictionaries and integrated with OpenOffice and
LibreOffice, e.g. a full-fledged Greenlandic spellchecker, which is a
polyagglutinative language, is currently available for OpenOffice via HFST. 
By adding the tagger capability, we have also created an
improved spelling suggestion mechanism for words in context.
The spellchecker applications and some of their theoretical underpinnings are described 
in Section~\ref{Applications}.

Finally  in Section~\ref{Discussion}, we discuss some additional applications such as 
synonym and translation dictionaries as well as a framework for recognizing 
multi-word expressions for information extraction and how this can be done 
using finite-state technologies.

\section{Building Morphologies}\label{LexiconTools}


%Miikka

One of the earliest and most important goals of the HFST project has
been to provide open-source utilities for compiling full-fledged
morphological analyzers, which may be used for constructing spellcheckers,
taggers and other language technological utilities of good
quality. The Xerox toolkit \cite{beesley/2003} is among the most
widely used frameworks for constructing morphological utilities. The
toolkit is used to compile linguistic description into morphological
analyzers. More specifically, Xerox tools include the finite-state
lexicon compiler lexc, the two-level rule compiler \verb|twolc| and the
cascading replace rule compiler xfst. HFST includes the tools
\textbf{hfst-lexc}, \textbf{hfst-twolc} and \textbf{hfst-xfst}, which provide full backward
compatibility with Xerox tools and augment their functionality.

Lexicon files for the lexicon compiler \textbf{hfst-lexc} describe the morphotactics of a
language using sub-lexicons containing lists of morphs. A rule
component can be used for realizing phonological changes occurring 
within the morphs or at their boundaries. In this article, we do not introduce new features 
of \textbf{hfst-lexc}. Adding weights to lexicons using \textbf{hfst-lexc} is described in \cite{linden/2009/fsmnlp}.

The rule compilers \textbf{hfst-xfst} and \textbf{hfst-twolc} provide almost the same
functionality. Both are used to realize morphophonological variations
on a lexicon compiled with \textbf{hfst-lexc}. The difference between the tools is that
\textbf{hfst-xfst} rules are applied in succession gradually altering the
lexicon whereas \textbf{hfst-twolc} rules are applied as parallel constraints limiting
the realizations of morphophonemes used in the \textbf{hfst-lexc} lexicon.

\subsection{Parallel Rules with Negative Contexts}

The rule compiler \textbf{hfst-twolc} provides backward compatibility with the
Xerox tool twolc, but it augments twolc functionality by providing a new type of rules. 
In \textbf{hfst-twolc}, rules can have negative contexts.
\begin{figure}
{\footnotesize
\begin{verbatim}
! Change x to y after the string "a b" unless "d" or 
! "c d" follows.
"Rule with negative context"
x:y <=> a b _     ;
        except
            _ ( c ) d ;

"Rule without negative context"
x:y <=> a b _ [ ? - [ c | d ] | c [ ? - d ] | .#. ] ;
\end{verbatim}
}
\caption{Negative context rule and corresponding traditional two-level
  rule}\label{negative-context-rule}
\end{figure}

In traditional two-level rules, it can be very difficult to express
that a certain alternation should not occur in a given context. Such
restrictions are required because of rule conflicts, i.e. clashes of
two or more contradicting rules. When a rule conflict occurs, the
contexts of some of the contradicting rules need to be restricted in
order to resolve the
conflict.\footnote{\url{http://www.cis.upenn.edu/~cis639/docs/twolc.html}}

Sometimes an automated mechanism called conflict resolution of
two-level rules~\cite{silfverberg/2009/2} can be used to resolve rule
conflicts, but conflict resolution works only if the conflicting rules
can be ordered into chains of subcases. Often this can be difficult
to accomplish, especially for grammar writers who are not especially
well acquainted with writing regular expressions.

In twolc syntax, prohibition rules such as {\tt x:y /<= C1 \_ C2 ;}
can be used to forbid a pair {\tt x:y} in a context where the left
context matches the regular expression {\tt C1} and the right context
matches {\tt C2}. Unfortunately prohibition rules do not solve the
problem of conflicting rules, because they do not participate in conflict
resolution and simply adding new rules to a two-level grammar does not
remove rule conflicts.\footnote{Note that, if prohibition rules would
  participate in conflict resolution, it would still be challenging to
  write them in such a manner that conflict resolution could apply.}

Using an extension to the Xerox two-level rule syntax in
\verb|hfst-twolc|, it is possible to formulate rules with negative
contexts, i.e. contexts that prohibit the triggering of the
rule. Figure~\ref{negative-context-rule} shows a schematic example of
a negative context rule and a traditional Xerox style rule, which has
the same effect as the rule with the negative context.

Rules with negative contexts are used in the Kyrgyz morphology
developed in the Apertium project\footnote{\url{http://www.apertium.org/}}.
They significantly shorten the two-level
grammar\footnote{Personal communication with Francis Tyers.}.


\subsection{Cascaded Rules: Explanations and Examples}
% Senka

HFST replace rules provide backward compatibility with XFST replace rules, 
described in \cite{Kempe96parallelreplacement,beesley/2003}. 
Although they mostly share the same notation, the HFST replace rules differ in that
they were compiled with the \verb!.r-glc.! operator \cite{Gerdemann/1999} and preference
relations described in \cite{ylijyra/2008b}. This approach makes it possible to more freely
define contexts in parallel rules and to easily add new functionalities in future.

We present a general account of using replace rules with the command-line tool {\tt hfst-regexp2fst}. Since these rules mostly follow the behaviour of the XFST rules,
for a detailed description of each rule, see The Finite State Morphology book \cite{beesley/2003}.

% Simple rules

\textbf{Simple rules.} A simple right arrow replace rule,
\begin{equation}
  A \rightarrow\ B
\end{equation}

where A and B are regular expressions, 
expresses that A in the upper language maps to B in the lower language.

Replace rules can be compiled into a transducer using
\verb!hfst-regexp2fst!. This tool takes a regular expresion as input 
and gives a corresponding transducer written in a binary file as output. 
To convert transducers from binary format to text (ATT) format, 
there is an HFST tool \verb!hfst-fst2txt!. Therefore, the upper rule 
could be compiled as in Figure~\ref{fig:simple_replace}.

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo "A -> B ;" | hfst-regexp2fst | hfst-fst2txt
\end{verbatim}
}
\caption{Compiling simple replace rule}
\label{fig:simple_replace}
\end{figure}


The regular expression can be read from a file, or saved to a file (Figure~\ref{fig:read_from_file}). 

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo "A -> B ;" > regex
$ hfst-regexp2fst -i regex -o transducer
$ hfst-fst2txt transducer
\end{verbatim}
}
\caption{Reading from, and writing to a file}
\label{fig:read_from_file}
\end{figure}


Rules can be applied to any language by using composition.
The result of composing a word \verb!ABCD! with the aforementioned rule
\verb!A -> B!, the
result is the transducer in Figure~\ref{fig:replace_compose}.  It has
5 states (0 -- 4), and only state 4 is final. The transitions go from a state in the
first column to a state in the second column. The input label of a transition is
displayed in the third column and the output label in the fourth. The default
output of \verb!hfst-regexp2fst! is weighted. Since weights are
not used in replace rules, the weights get the value 0.0 when the rule is
compiled. The weights are displayed in the final column.

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo "A B C D .o. A -> B ;" | hfst-regexp2fst | hfst-fst2txt
0       1       A       B       0.000000
1       2       B       B       0.000000
2       3       C       C       0.000000
3       4       D       D       0.000000
4       0.000000
\end{verbatim}
}
\caption{Composing a rule with a word}
\label{fig:replace_compose}
\end{figure}


%\begin{figure} [h!]
%{\footnotesize
%\begin{verbatim}
%$ echo "c a t .o. c a t -> c a t s  ;" |
%> hfst-regexp2fst -f sfst | hfst-fst2txt
%0       1       c       c
%1       2       a       a
%2       3       t       t
%3       4       @0@     s
%4
%% \end{verbatim}
%% }
%% \caption{Using SFST transducer format}
%% \label{fig:sfst_format}
%% \end{figure}


% Context

\textbf{Context.} Every rule can have a context in which the replacement is made. 
Here, A will be mapped to B if and only if it is between regular expressions
L and R.
\begin{equation}
  A \rightarrow\ B\ ||\ L \_\  R
\end{equation}

Also, multiple context pairs are supported, when separated by comma.
\begin{equation}
  A \rightarrow\ B\ ||\ L_1 \_\  R_1 ,\ \ldots\ ,\ L_i \_\  R_i
\end{equation}

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
$ echo " m a n a a b .o. a -> b || m _ n , _ b;" |
hfst-regexp2fst | hfst-fst2txt
0       1       m       m       0.000000
1       2       a       b       0.000000
2       3       n       n       0.000000
3       4       a       a       0.000000
4       5       a       b       0.000000
5       6       b       b       0.000000
6       0.000000
\end{verbatim}
}
\caption{Replace rule between two contexts}
\label{fig:multiple_contexts}
\end{figure}

In Figure~\ref{fig:multiple_contexts}, the rule expression says that \verb!a! in the 
upper language is mapped to \verb!b! in lower language 
when between \verb!m! and \verb!n!, or in front of \verb!b!. 
In the word \verb!manaab!, the first and last \verb!a! are mapped to \verb!b!, 
because they occur between corresponding contexts, but the second \verb!a! is kept unchanged.


\begin{table} [h!]
  \centering
  \caption{Different context directions in Replace Rules}
  \begin{tabular}{ c c }
    \hline
    Operator & Operator description \\ \hline
    \ \verb!||!\ & \ both contexts are taken from the upper language \\ 
    \ \verb!//!\ & \ the left context is taken from the lower language, the right from the upper \\ 
    \ \verb!\\!\ & \ the left context is taken from the upper language, the right from the lower \\ 
    \ \verb!\/!\ & \ both contexts are taken from the lower language. \\ \hline
  \end{tabular}
  \label{tab:context_directions}
\end{table}

In a replace rule where A is the upper and B the lower language, 
there are four contextual directions that can be used with replace rules.
For example, when the \ \verb!//!\ sign is used as context orientation operator, 
the left context will be taken from the lower language. 
It is thus possible for the replace function to write its own context. 
This is shown in Figure~\ref{fig:context_orientation}. 

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
  $ echo " b a a .o. a -> b // b _ ;" | hfst-regexp2fst | hfst-fst2txt
  0       1       b       b       0.000000
  1       2       a       b       0.000000
  2       3       a       b       0.000000
  3       0.000000
\end{verbatim}
}
\caption{Replace rule writes its own context}
\label{fig:context_orientation}
\end{figure}

% Other replace functions

\textbf{Other replace functions.} We have hitherto only used the right arrow replace operator,
but there are many other operators that can be used. 
All the operators listed in Table~\ref{tab:replace_operators} have their left arrow version, 
which is the inversion of the right operator. Furthermore, all the rules can be used 
with epenthesis \verb![. .]! and markup \verb![...]! operators. 
The epenthesis operator should be used with empty strings to avoid replacing infinitely 
many epsilons, while the markup operator is used to insert markers around a word.

\begin{table} [h!]
  \centering
  \caption{List of right replace operators that can be used in HFST}
  \begin{tabular}{c  c} 
    \hline
    Right replace operators & Replace function \\ \hline
    \verb!->!   & Replace \\ 
    \verb!(->)! & Replace optional \\
    \verb!@->!  & Longest match from left to right \\ 
    \verb!->@!  & Longest match from right to left \\ 
    \verb!@>!   & Shortest match from left to right \\
    \verb!>@!   & Shortest match from right to left \\  \hline
  \end{tabular}
  \label{tab:replace_operators}
\end{table}



\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
  $ echo " a a a .o. [.a*.] @-> b ;" | hfst-regexp2fst > a.fst
  $ echo " b a a b .o. a @-> %[ ... %] ;" | hfst-regexp2fst > b.fst
\end{verbatim}
}
\caption{Epenthesis and markup operators}
\label{fig:epenthesis_markup}
\end{figure}


% Parallel rules

\textbf{Parallel rules.} Parallel rules are used when there are 
multiple replacements at the same time. The general parallel replace rule expression 
consists of individual replace rules separated by two commas.
\begin{equation}
  A_1 \rightarrow\ B_1\ ||\ L_1 \_\  R_1\ ,,\ \ldots\ ,,\ A_i \rightarrow\ B_i\ ||\ L_i \_\ R_i
\end{equation}

In XFST, all rules in a parallel rule expression have to have the same format, i.e. 
they need to have the same arrow and the same context layout. 
In HFST, the constraint that all the rules should have the same arrow is kept, 
but the context layout can differ freely. 
Therefore, the rule expression in Figure~\ref{fig:parallel_rules} 
would not be allowed in XFST, but is valid in HFST.
\begin{figure}
{\footnotesize
\begin{verbatim}
  $  echo " a -> b || m _ n ,, c -> d ;" | hfst-regexp2fst | hfst-fst2txt
\end{verbatim}
}
\caption{Parallel rules with different context layouts}
\label{fig:parallel_rules}
\end{figure}

\section{Morphological Descriptions}\label{MorphTools}

% Krister

Popular formalisms for describing the morphotactics of a language tend to be some variation 
of the item and arrangement scheme. We build on this to generalize from \verb!lexc! into other
item-and-arrangement notations such as Hunmorph and Apertium. This gives us the option of describing
morphology in a notation that is compiled into finite-state transducers which we can continue to process with other
HFST tools. We demonstrate how compilation could proceed when reducing a \verb!lexc! lexicon into a sequence of
finite-state operations. A morphological programming language like \verb!SFST-PL! forgoes a standard lexicon interface
like \verb!lexc! and only offers the end user an option to construct the lexicon using finite-state operations. 

\subsection{Morphotax and Morphological Formul\ae}
%Tommi & Krister

Morphotax is the component dealing with morphological combinations in language
description. This concerns word-formation processes, such as affixation and
compounding.  There are numerous formalisms for describing morphotactics in natural
language processing applications, such as
hunspell\footnote{\url{http://hunspell.sf.net}} (and its older *spell
relatives), the Apertium lttoolbox\footnote{\url{http://apertium.sf.net}} or
the Xerox lexc \cite{beesley/2003}. The HFST toolset contains parsers and
compilers for reading descriptions of morphologies written in these formalisms,
which can be used for compiling a finite-state automaton for the other HFST
tools to process \cite{pirinen/2010/il,linden/2009/sfcm}.

Consider a trivial lexicon consisting of the English nouns \emph{cat} and
\emph{ax} with the empty string as the singular marker and \emph{s} and
\emph{es} as plural markers, respectively,  forming a lexicon as outlined in
Figure~\ref{fig:morph0} using lexc notation.

\begin{figure} [h]
{\footnotesize
\begin{verbatim}
  LEXICON Root
  Nouns ;

  LEXICON Nouns
  cat NumberS ;
  ax NumberES ;

  LEXICON NumberS
  # ;
  s # ;

  LEXICON NumberES
  # ;
  es # ;
\end{verbatim}
}
\caption{Lexicon in lexc notation}
\label{fig:morph0}
\end{figure}

Many contemporary notations for describing morphotactics tend to use the same item-and-arrangement paradigm for
combining sets of morphs or morphemes, i.e. they use sublexicons for lists of morphs that may 
have continuations in other sublexicons. 
In its most general form, such a paradigm can be defined using a combination of two components expressing local restrictions:
(1) a disjunction of morphs in local lexical context repeated infinitely and 
(2) a morphotactic filter describing the permitted sequence of sublexicons.
Both the disjunction of morphs and the morphotactic filter can be described in finite-state form.

In Figure~\ref{fig:morphology1}, we demonstrate how the lexicon in
Figure~\ref{fig:morph0} is compiled into a loop of disjunctions of any of the
morphs in the lexicon. Note that we bracket special symbols with at-signs `@'. In
this case, we use special joiner symbols to calculate the morphotactics, and the
at-signs give the user a hint that they are not supposed to end up in the final
result. The sublexicon symbols like \texttt{@NumberES@} are used for lining up 
the sublexicons, and the symbol \texttt{@LEX@} is used as a boundary symbol, which simplifies the filter
algorithm\footnote{In \cite{linden/2009/sfcm}, the algorithm without a separate
boundary symbol requires a term complement of the disjunctive closure of the sublexicon
symbols.}.

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
echo "@Root@ @Nouns@ @LEX@" > strings
echo "@Nouns@ c a t @NumberS@ @LEX@" >> strings
echo "@Nouns@ a x @NumberES@ @LEX@" >> strings
echo "@NumberS@ @END@ @LEX@" >> strings
echo "@NumberS@ s @END@ @LEX@" >> strings
echo "@NumberES@ @END@ @LEX@" >> strings
echo "@NumberES@ e s @END@ @LEX@" >> strings
hfst-strings2fst --has-spaces --disjunct-strings < strings  | 
    hfst-repeat -f 1 > bag_of_morphs

\end{verbatim}
}
\caption{Compiling a disjunction of all the morphs in the lexicon}
\label{fig:morphology1}
\end{figure}

In Figure~\ref{fig:morphology2}, we show the command-line simulation for
creating the morphotactic filter and how this filter is composed with the
disjunction of morphs to create the final lexicon\footnote{Whole process is
also available from our SVN in
\url{https://hfst.svn.sf.net/svnroot/hfst/trunk/cla-2012-article/morphtest.bash
}}. In effect, we create a filter
that ensures that each morph, e.g. \emph{es}, only follows the morph that
was asking for it on the right, i.e. we require that the same sublexicon symbol, e.g. \texttt{@NumberES@},
occurs on both sides of the boundary symbol \texttt{@LEX@}. For a full algorithm, see e.g.~\cite{linden/2009/sfcm}.

\begin{figure} [h!]
{\footnotesize
\begin{verbatim}
echo "%@Root%@ [? - %@LEX%@]*"  |  hfst-regexp2fst > start
echo "%@END%@ %@LEX%@"  |  hfst-regexp2fst > end

echo "%@NumberS%@ %@LEX%@ %@NumberS%@ [? - %@LEX%@]*"  |
    hfst-regexp2fst > cont1
echo "%@NumberES%@ %@LEX%@ %@NumberES%@  [? - %@LEX%@]*"  |
      hfst-regexp2fst > cont2
echo "%@Nouns%@ %@LEX%@ %@Nouns%@  [? - %@LEX%@]*"  |
      hfst-regexp2fst > cont3
hfst-disjunct cont1 cont2 | hfst-disjunct - cont3 |
    hfst-repeat -f 1 > conts
hfst-concatenate start conts |
    hfst-concatenate - end > morphotactics
hfst-compose bag_of_morphs  morphotactics > lexicon
\end{verbatim}
}
\caption{Composing the morphotactics with the disjunction of morphs into a lexicon}
\label{fig:morphology2}
\end{figure}

Since many morphotactic formalisms tend to be some variation of the item and arrangement scheme,
we can build on this to generalize into other notations, since the morphotactics itself
does not depend on the description language of the morphotactics. 
This also makes additions like compound-based weighting of the language-model 
\cite{linden/2009/fsmnlp} generally applicable.

\subsubsection{For Lexicographers} this approach gives the option to
choose their favorite notation, e.g. hfst-lexc, Hunspell or Apertium and then continue with other
HFST tools. In theory, it would also be possible to use this morphotactic formula
to write implementations for any item-and-arrangement morphology, but in practice
it's easier to simply use some high-level scripting language to convert a lexical database into 
e.g. lexc notation.

\subsection{Performance}
% Erik

The HFST toolkit has been implemented using three back-end libraries:
SFST, OpenFst and foma. The libraries are linked with HFST. Usually one library is chosen
and used throughout the compilation of a given morphology.
We can thus compare how different back-end libraries perform
in the same task.

In Table~\ref{tab:compilation_times}, we show compilation times for different 
morphologies using different HFST back-ends. The morphologies are OMorFi 
\cite{pirinen/2008} for Finnish, Morphisto \cite{zielinski/2009} for German,
Morph-it \cite{Zanchetta/2005} for Italian, Swelex 
\footnote{\url{https://kitwiki.csc.fi/twiki/bin/view/KitWiki/HFSTSwelex}} 
for Swedish and TrMorph \cite{Coltekin/2010} for Turkish. 
OMorFi, Morphisto and TrMorph have several rules for
inflection and compounding, Morph-it and Swelex are basically word lists.
We use HFST version 3.3.4, with SFST, OpenFst and foma as
back-ends. The times are averages from runs of 10 compilations.

\begin{table} [h!]
  \centering
  \caption{Compilation times for different morphologies with
    different HFST back-ends. The times are given in minutes and seconds
    and averaged over 10 compilations. HFST version is 3.3.4.}
  \begin{tabular}{c c c c c c }
    \hline
    Back-End & Finnish & German & Italian & Swedish & Turkish \\ \hline
    SFST & 2:48 & 2:12 & 0:30 & 0:13 & 0:12 \\
    OpenFst & 7:52 & 7:45 & 2:24 & 0:49 & 0:40 \\
    foma & 1:52 & 1:33 & 0:31 & 0:13 & 0:05 \\ \hline
  \end{tabular}
  \label{tab:compilation_times}
\end{table}


In Table~\ref{tab:compilation_times_versions}, we show both the current compilation 
times and the ones that we achieved in an earlier benchmarking \cite{linden/2011/sfcm} for 
Finnish and German. We also show the back-end versions used.

\begin{table} [h!]
  \centering
  \caption{Compilation times for different morphologies with
    different HFST back-ends and their versions. 
    The times are given in minutes and seconds.}
  \begin{tabular}{ c c c c }
    \hline
    Back-End                 & version  & Finnish  & German \\ \hline
    \multirow{2}{*}{SFST}    & 1.4.2    & 5:02     & 6:39 \\
    & 1.4.6    & 2:48     & 2:12 \\
    \multirow{2}{*}{OpenFst} & 1.2.7    & 6:51     & 6:28 \\
    & 1.2.10   & 7:52     & 7:45 \\
    \multirow{2}{*}{foma}    & 0.1.14   & 1:49     & 1:29 \\
    & 0.1.16   & 1:52     & 1:33 \\
    \hline
  \end{tabular}
  \label{tab:compilation_times_versions}
\end{table}

It can clearly be seen that the performance of HFST with SFST as a back-end 
has improved: the compilation time of the Finnish morphology has almost
halved and the compilation time of the German morphology is only one third of the
time at the previous benchmarking. 
This improvement comes from the newer version of SFST that features more optimized
composition and Hopcroft minimization functions. The improved functions 
were developed by Helmut Schmid in cooperation with the HFST team.

% The compilation times of HFST with OpenFst as back-end have become slightly slower.
% We are still looking into this fact trying to find out whether
% the problem lies in the HFST code or if we are not using the features of 
% OpenFst's newer version right.
% The performance of HFST with foma has stayed almost the same, only a small
% growth of a couple of seconds can be seen.


\section{Building Taggers}\label{PosTools}


% Miikka

The HFST library includes tools for constructing statistical
part-of-speech (POS) taggers which resemble Hidden Markov Models (HMM) from
tagged training data. HFST taggers differ from other HMM taggers such
as Tnt~\cite{Brants:2000} and Hunpos~\cite{Halascy:2007} in that HFST
allows combining different estimates for tag probabilities during
tagging. It is possible to use e.g. the surrounding word forms in
estimating the probability of a given tag. This is more thoroughly
explained in \cite{silfverberg/2010/icetal,silfverberg/2011/nodalida} The
accuracy for basic HMM taggers implemented using HFST tools is
comparable to the accuracy of Tnt and Hunpos as demonstrated below.

HMM-type taggers include a lexical model and a tag sequence model. The
lexical model is needed for determining probabilities for the
co-occurrence of tags and word forms disregarding context, and the tag
sequence model is used for determining the probabilities for the
co-occurrence of tags of neighboring words. The lexical models in
HFST taggers include suffix guessers which can be modified to suit
the needs of particular languages. Additionally HFST supports using
morphological analyzers in tagging.

\subsection{The structure of HFST Taggers}

HMM taggers are statistical models which determine the most likely
POS tag from some tag set for each word in a sentence. For
determining the most likely tags, the taggers use
lexical probabilities ${\rm p}(w|t)$ for each word $w$ and each tag
$t$ together with \emph{transition probabilities} for the tag of a
word at a given position given the tags of the preceding words ${\rm
  p}(t_i | t_{i - 1},\ ...\, t_{i-n})$~\cite{silfverberg/2011/nodalida}. The integer $n$ determines how
many preceding tags are considered when estimating the probability of
tags. It is called the \emph{order} of the HMM tagger. Second order HMM
taggers are the most common in POS tagging.

HFST taggers extend traditional HMM taggers by allowing modifications
to the traditional estimate ${\rm p}(t_i | t_{i - 1},\ ...\,
t_{i-n})$. In addition to preceding tags also succeeding tags and word
forms can be used when deciding the probability of a tag at a given
position. These estimates are combined using finite-state calculus.

In HFST taggers, the lexical probabilities ${\rm p}(w|t)$ are given by
the lexical component of the tagger and the transition probabilities
${\rm p}(t_i | t_{i - 1},\ ...\, t_{i-n})$ are given by the sequence
component of the tagger. Both models are trained using tagged
training data.

Both components can be modified to suit the needs of a particular
language. In~\cite{silfverberg/2011/nodalida} it is explained how the sequence
model can be modified to include preceding and succeeding words in
the estimates of the probabilities of tags, and how suffix guessers
can be modified to better suit agglutinative languages. In the present paper
we show how a morphological analyzer can be integrated with an HFST
tagger to improve the accuracy of the tagger when there are a lot of
out-of-vocabulary words.


\subsection{The Lexical Model of HFST Taggers}
% Miikka

The accuracy of traditional POS taggers suffers greatly because of out
of vocabulary (OOV) words, i.e. word forms which were not observed
during training of the tagger. OOV words effect the accuracy, since
POS taggers generally rely heavily on lexical probabilities
$P(word|tag)$ computed for the words occurring in a training corpus.

For languages like English with few productive morphological
phenomena, OOV words are not a big problem when there is a lot of
training data and the genres of the training data and the data that is
tagged are sufficiently similar. When the genres differ considerably
there are more OOV words and consequently accuracy is reduced. When
building taggers for morphologically rich languages such as Turkish,
Finnish or Hungarian, OOV words become a major problem even when no
change of genre is involved and even when there is a lot of training
data. In agglutinative languages OOV words arise from productive
morphological phenomena such as inflection, derivation and
compounding.

Usually, e.g. in~\cite{Brants:2000}, OOV words are handled using
suffix guessers, which combine estimates of all suffixes of the OOV
word whose length does not exceed a given threshold. E.g. the POS tag
for {\it ameliorate} can be guessed based on the distribution of tags
for the suffixes {\it -e}, {\it -te}, ..., {\it -liorate} if the
threshold for the length of suffixes is 7.

HFST taggers offer two improvements for handling of OOV words: (1) the
tagger builder can adjust the way in which the probability
distribution for different length suffixes are combined to form
probability estimates for the OOV word, and (2) the tagger builder has
the option to combine taggers with morphological analyzers, whose tag
set does not need to equal the tag set of the training data of
the tagger.

Adjusting the way suffix estimates are combined is useful e.g. in
Finnish, where the suffix guesser proposed by \cite{Brants:2000} gives
poor results. For Finnish, the accuracy of the guesser improves when
the only suffix considered is the longest suffix of the word which was
seen during training. This is probably a result of the high number of
compound words where the final part of the compound is known, but the
compound itself is an OOV word. The final part of the compound is
generally a very good predictor of the word-class of the compound, so
suffixes of the OOV word containing the final part are very
informative.

The idea of incorporating a morphological analyzer in a tagger is not
new. E.g.  \cite{Tzoukerman:1996,Oravecz:2002} use morphological
analyzers as part of statistical taggers. The novel aspect of HFST
taggers is that they can incorporate morphological analyzers whose
morphological description differs from the morphological description
used in the training data of the statistical tagger. This is useful,
because it allows utilizing ready-made linguistic utilities in tagging
without the need to address problems such as differences in the
coarseness of the morphological descriptions between the utilities and
the training data of the tagger.

In approaches such as \cite{Tzoukerman:1996,Oravecz:2002}, the tag set
of the morphological analyzer and the statistical tagger have to be
the same. When an OOV word is encountered, the morphological analyzer
is used to look up the possible analyses for the OOV word. These
analyses are used directly by the tagger. In contrast, HFST taggers
keep track of {\it ambiguity classes}, which are the sets of
morphological analyses emitted by the morphological analyzer for words
in the training data. The taggers use the tag distributions of all
training data words in a given ambiguity class to determine the tag
distribution of an OOV word in the same ambiguity class.

Given an English morphological analyzer, which emits two analyses {\it
  dog\-+N\-+SG} and {\it dog\-+V\-+INF} for the word form {\it dog},
the ambiguity class of {\it dog} is the set of its analyses {\it
  \{+N\-+SG, +V\-+INF\}}. In the training data, {\it dog} might
receive Penn Treebank tags like {\tt NN} and {\tt VB}. When the tagger
encounters an OOV word in the same ambiguity class as {\it dog}, such
as {\it man}, it first maps {\it man} onto its ambiguity class {\it
  \{+N\-+SG, +V\-+INF\}} using the morphological analyzer. It then
uses the tag distribution of all words in the ambiguity class {\it
  \{+N\-+SG, +V\-+INF\}} to estimate the probabilities $P(man|tag)$
for {\it man} given each tag (e.g. {\tt NN} and {\tt VB}). Note that
this does not require that the tag sets of the training data and the
morphological analyzer are the same. It also does not require that
their morphological descripitions are equally fine-grained.


\subsection{Experiments with HFST taggers}

We demonstrate HFST taggers by constructing POS taggers for Finnish
and English. For English we construct a regular second order HMM
tagger. For Finnish we construct two taggers: one regular second
order HMM tagger and another second order HMM tagger which utilizes
the OMorFi morphological analyzer for Finnish~\cite{pirinen/2008}.

The English training data and test data come from the Penn
Treebank. Sections 1 to 18 were used for training and sections 22 to
24 for testing. The Finnish training and test data come from Finnish
newspaper text which has been automatically tagged using the
Textmorfo parser\footnote{http://www.csc.fi/kielipankki/}. The data
is described in Table~\ref{data-taggers}.

\begin{table}
  \caption{Data used for training and testing the English and Finnish taggers.}\label{data-taggers}
  \begin{center}
    \begin{tabular}{lrrr}
      \hline
      Language       & ~Training data size (tokens)~& Test data size (tokens)~& Distinct tags\\
      \hline
      English        &   912,344~~~~~~~~~~~~~~    & 129,654~~~~~~~~~~ &  45~~~~~~~ \\
      Finnish        & 1,027,511~~~~~~~~~~~~~~    & 156,572~~~~~~~~~~ & 764~~~~~~~ \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

As Table~\ref{eng-tagging-acc} shows, HFST taggers achieve comparable
results to the well known second order HMM tagger TNT. The accuracy
for unknown words is slightly worse using the HFST tagger, but for
known words the accuracies are nearly identical.

\begin{table}
  \caption{Results for second order HMM taggers of English. The accuracy
    figures for TNT can be found in~\cite{Halascy:2007}.}\label{eng-tagging-acc}
  \begin{center}
    \begin{tabular}{lccc}
      \hline
      English Model       & ~~~~Seen & ~~~~Unseen & ~~~~All \\
      \hline
      TNT         & 96.77\%  &    85.19\% & 96.46\% \\
      Basic HFST  & 96.68\%  &    80.71\% & 96.23\% \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

For Finnish, the accuracy for the basic second order HMM tagger is
poorer than for English as seen in Table~\ref{fin-tagging-acc}. This
is mostly caused by words in the test data that are
missing from the training data, i.e. OOV words. In the Finnish test
data 11.51\% of the words are OOV words. For comparison, only 2.81\%
of the words in the English test data are OOV words.

To reduce the number of OOV words in the Finnish test data, a
morphological analyzer was used as explained above. Using the
morphological analyzer, only 2.73\% of the words in the test data
were OOV words. Consequently a significant increase in total tagging
accuracy is seen. See Table~\ref{fin-tagging-acc}. The increase is
negligible for known words, but significant for unknown words.

\begin{table}
  \caption{Results for Finnish taggers. The model Basic HFST is a
    regular second order HMM tagger. The model With Morph HFST is
    augmented with a morphological analyzer.}\label{fin-tagging-acc}
  \begin{center}
    \begin{tabular}{lccc}
      \hline 
      Finnish Model            & ~~~~Seen & ~~~~Unseen & ~~~~All \\
      \hline 
      Basic HFST       &  97.51\% &    77.51\% & 95.23\% \\
      With Morph HFST  &  97.53\% &    83.65\% & 95.90\% \\
      \hline
    \end{tabular}
  \end{center}
\end{table}


\section{Transducer applications}\label{Applications}


% Tommi

Automata technology is a general framework for describing language models
and phenomena in a wide range of linguistic fields from phonology to
morphology, as well as certain areas of syntax and semantics (in POS tagging and
machine translation).  The practical applications cover spell-checking, 
as demonstrated in Voikko\footnote{\url{voikko.sf.net}} with
bindings to LibreOffice, Gnome desktop, Mozilla and Mac OS X Spell Service, to
morphological and syntactic analysis as demonstrated in native HFST tools
from HFST downloads\footnote{\url{hfst.sf.net}}, and to machine translation as demonstrated in
several released language pairs in the machine translation system
Apertium\footnote{\url{www.apertium.org}}. This demonstrates a very important feature of HFST,
alluded to in previous chapters, i.e. language models can be
described with one tool in one theoretical and practical framework. For example,
existing morphological analyzers for the Sámi languages found on the
Internet\footnote{\url{divvun.no}}, written with the Xerox formalism were converted into a spell-checker 
in one evening and evaluated in \cite{pirinen/2010/lrec} with additional training from likewise freely
available Sámi Wikipedia. Similar results of using machine translation
dictionaries from the free/libre open source project
Apertium to create not only
dictionaries, but morphological analyzers and spell-checkers, are also
demonstrated on our web page.\footnote{The examples are available at
  \url{http:/www.ling.helsinki.fi/cgi-bin/omor/omordemo.bash}}

Regarding the general applicability of finite-state
language models in practical applications it may be noted that we can now generate
e.g. spell-checkers for a language that has a machine
readable dictionary (such as Manx) or a morphological analyzer (such as
Greenlandic, which is not easily implemented in other formalisms). 
The transformation of a dictionary or analyzer in transducer format 
to a baseline spell-checker (with a homogeneous 
Damerau-Levenshtein edit distance as an error model) can be performed using finite-state tools 
without  feedback from linguists or native speakers.

\subsection{Spellcheckers}

The task of finite-state spell-checking is well researched and documented. It
consists mainly of two phases, identifying incorrect word forms and creating
suggestions for corrections. For incorrect word forms there are two types of
mistakes, non-word spelling errors, such as writing \emph{cta} where \emph{cat}
is meant, and real-word spelling errors, such as writing \emph{there} where
\emph{their} is intended. The method for finding the former in finite-state systems
is simply to apply the dictionary to the text word by word. Any unrecognized
string not belonging to the language of the dictionary automaton is a non-word
spelling error. For real-word errors a statistical n-gram model or syntactic
parser is typically required. To correct a spelling error in a finite-state
system, a two-tape automaton modeling the typing errors should be applied to
the misspelt string to get set of potential corrections \cite{pirinen/2010/lrec}.
For practical purposes the error model can also be implemented as a
fuzzy finite-state traversal algorithm or similar methods \cite{oflazer/1996}. The result
of the correction step is a set of word-forms that are correct in the
language of the spell-checking dictionary. Another related task is to rank this
set to provide the most likely corrections first.
A trivial way to perform such ranking would be to use
unigram \cite{pirinen/2010/lrec} or n-gram probabilities of the words 
\cite{mays/1991} or word-form analyses~\cite{pirinen/2012/cicling}.

\subsubsection{Creating Spellcheckers.}
% Tommi

The creation of a finite-state spellchecker involves compiling (at least) two
automata: a dictionary that contains the correctly spelled word-forms and the
error model that can rewrite misspelt word-forms into correctly spelled ones.
The former automaton can be as simple as a reference corpus, containing larger
quantities of correctly spelled words and (possibly) smaller quantities of
misspelt ones \cite{norvig/2010}. Also more elaborate dictionaries, such as
morphological analyzers described in Section~\ref{MorphTools} can be used, and also
trained for spell-checking purposes with reference corpora without any big
modifications to the underlying implementation \cite{pirinen/2010/lrec}.

For the error-model we can trivially construct an automaton corresponding to
the Levenshtein edit distance algorithm \cite{oflazer/1996,agata/2002}. For more elaborate error models
it is possible to use hunspell algorithms as automata \cite{pirinen/2010/il} or
construct further extensions by hand \cite{pirinen/2010/lrec}. Given an
aligned error corpus, it is also possible to construct a weighted error model
automaton automatically \cite{brill/2000}.

\subsubsection{Checking Strings and Generating Suggestions.}
% Sam

String checking is straight\-forward. It consists of composing the
input $I$ with the lexical automaton $L$ and checking whether the
output language of the result is empty. If the result is empty, the
correction set must be calculated.

%String checking is straight\-forward. The input is matched against the
%lexical automaton, i.e. the output language of $I \circ L$ is
%calculated, where $I$ is the input and $L$ is the lexical automaton. If
%the output language is empty, the string is absent from the lexicon and the
%correction set must be calculated.

A corrected string is a string that can be generated by transforming the
input string with the error model and is present in the lexicon.
These strings are thus the output language of the composition
$I \circ \ E \circ L$, where $E$ is the error model.

The desired behavior, or result set $R$, of a spellchecker given input $I$, a
lexicon $L$ and an error model $E$ is thus given by equation \ref{result_set_eqn},
where $\pi_2$ is the output projection.

\begin{equation}
  \label{result_set_eqn}
  R = \begin{cases}
    \pi_2(I \circ L), & \mbox{if } \pi_2(I \circ L) \neq \emptyset \\
    \pi_2(I \circ E \circ L) & \mbox{otherwise}\\
  \end{cases}
\end{equation}

It is undesirable to compute either of the intermediate compositions
$I \circ E$ and $E \circ L$; the former will require futile work (producing
strings that are not in the lexicon) and the latter, though possible to
precompute, will be very large (see table \ref{composed_error_table}).

\begin{table}
  \centering
  \caption{Size of two lexical transducers composed with
    Damerau-Levenshtein edit distance transducers, all results minimized}
  \label{composed_error_table}
  \begin{tabular}{ c c c c }
    \hline
    Transducer               & states   & transitions & SFST file size \\ \hline
    Morphalou (French)       & 77.0K    & 190.7K   & 1.7MB \\
    With edit distance 1     & 9.6K     & 733.6K   & 5.7MB \\
    With edit distance 2     & 18.2K    & 344.7K  & 28MB \\ \hline
    OMorFi (Finnish)         & 203.8K   & 437.9K   & 4.0M \\
    With edit distance 1     & 17.7K    & 16186.6K & 120MB \\
    With edit distance 2     & 30.5K    & 53738.2K & 410MB \\ \hline
  \end{tabular}

\end{table}

To circumvent these problems, a three-way on-line composition of $I$, $E$ and
$L$ was implemented (distributed as \verb!hfst-ospell!
under the GPL and Apache licenses). It is along the lines of a more general
n-way composition described by Allauzen and Mohri in \cite{allauzen/2009}.
The present algorithm is, however, considerably simpler, due to certain
implementation details. Firstly, we use an efficient and compact indexing
transducer format (\verb!hfst-optimized-lookup!, documented
in~\cite{silfverberg/2009/ol}, obviating the
need to process the transducers involved into hash tables. Secondly,
in the present application it may be guaranteed that no special handling
of epsilons is necessary.

We write $I = (Q^I, 0^I, T^I, \Delta^I)$, where $Q$
is the set of states, $0$ is the starting state, $T$ is the set of terminal
states and $\Delta$ is the set of transitions (triples of (state, symbol pair
(input and output), state)), and similarity for $E$ and $L$.

Eliding for the time being weights and flag diacritics, states in the
composition transducer are triples of states of the component transducers.
Analogously with two-way composition, the starting state is $(0^I, 0^E, 0^L)$
and the edge $\delta = (q_1, (\sigma_1, \sigma_2), q_2)$ exists if edges
$(q^I_1, (\sigma_1, \sigma'), q^I_2)$,
$(q^E_1, (\sigma', \sigma''), q^E_2)$ and
$(q^L_1, (\sigma'', \sigma_2), q^L_2)$ exist in the respective transducers
for some
$\sigma' \in \Sigma^I \cap \Sigma^E, \sigma'' \in \Sigma^E \cap \Sigma^L$,
where $q_n = (q^I_n, q^E_n, q^L_n)$ and $\sigma_1 \neq \epsilon \neq \sigma_2$.

The set of the states of the composition, $Q$, are the reachable subset
(in the sense of having a path from $0$ to the state) of
$Q^I \times Q^E \times Q^L$.

If in a given state any of the component transducers has an edge involving
epsilon and the other symbols match as above, the resulting composed edges
go to states such that any state in a transducer to the left of an input
epsilon or to the right of an output epsilon is unchanged, eg. if we have
$(q^I_1, (\sigma, \sigma'), q^{I'})$, $(q^E_1, (\epsilon, \sigma''), q^E_2)$ and
$(q^L_1, (\sigma'', \sigma'''), q^L_2)$, a successor state of $q_1$ will be
$(q^I_1, q^E_2, q^L_2)$.

There is no special handling of cases where an epsilon output and an epsilon
input occur simultaneously in consecutively acting transducers. The algorithm
is however guaranteed to terminate in the present application as long as
epsilon cycles do not occur on the input side of the transducers and $I$
is acyclic. That being the case, the state reached in $I$ is a loop variant
that increases (towards termination) whenever $E$ consumes input. For this
not to happen for an indefinitely large number of iterations, either $E$
itself would have to have an input epsilon cycle, or indefinitely many states
in the composition would have to be created such that the state of $E$ is
unchanged. This would require $L$ to have an input epsilon cycle.

The final states are the reachable subset of $T = T^I \times T^E \times T^L$.

It can trivially be verified that this is equivalent to taking the two
compositions $(I \circ E) \circ L$.

This three-way composition is computed in a breadth-first manner with a
double-ended queue of states. The queue is initialized with the starting state,
and the target of every edge is computed and pushed onto the
queue. The starting state is then discarded, and the process is repeated
with a new state popped from the queue until the queue is empty.

Conceptually, the state space of $E \circ L$, which contains all the
misspellings (in the sense of $E$) of all the entries in the lexicon, is
explored in such a way that only the states visited when looking up $I$ are
generated.

For this process to be guaranteed to terminate, it is sufficient that none of
the component transducers have input-epsilon loops and that the input
transducer accepts strings of only finite length. This is because every newly
generated state will either have a shorter sequence of edges to traverse in
$I$ ("increment $q^I$") or be closer to requiring an edge in $I$ to be traversed
(due to a finite sequence of epsilon edges becoming shorter), establishing a
loop variant.

Weights representing the probability of a particular correction being the
correct one are a natural extension, and in \verb!hfst-ospell! correspond to
multiplication in the tropical semiring (for details see \cite{openfst/2007})
of the weight each edge
traversed. Multiplication in this semiring is the standard addition operation
of positive real numbers, which we approximate by addition of \verb!float!s.
Each state in the queue is recorded with an accumulated weight, and its
successor states have this weight incremented by the sum of the weights of
the edges traversed in the component transducers.

The alphabet of $I$ cannot be determined in advance, and in practice is taken
to be the set of Unicode code points. To allow the error model to correct
unexpected symbols in this large space, \verb!hfst-ospell! uses a special
symbol, \verb!@_UNKNOWN_SYMBOL_@! which is taken to be equal to any symbol
that is otherwise absent.

\subsubsection{Error model tool and optimizations.}
For the most common case of generating Levenshtein and
Damerau-Levenshtein (in which transposition of adjacent symbols constitutes
one operation) distance error models, a tool (\verb!editdist.py!) and
definition format was developed.

The definition format serves the purposes of minimizing the number of symbols
used in the error model (the number of transitions is $\mathrm{O}(|\Sigma|^2)$)
and introducing weights for edits. Typical
morphologies have a number of unusual characters (punctuation, special symbols)
or internally used symbols (eg. flag diacritics) that should be filtered for
more efficient correction. This is accomplished by providing a facility for
reading the alphabet from a transducer, ignoring any symbols of more than
one Unicode code point, and reading further ignorable symbols from a
configuration file.

The configuration file allows specifying weights to be added to any edit
involving a particular character or a particular edit operation (for example,
assigning a low weight to the edit o $\rightarrow$ ö for an OCR application).

Certain characteristics of the correction task permit
efficiency-oriented improvements to error models. A naive Levenshtein
error model with edit distance 2 in \verb!ospell! would, when given
the word word \verb!arrivew! where the French word \verb!arriver! was
intended, do the following useless work, where e.g. a:0 means output
epsilon for input a:

{\footnotesize
\begin{verbatim}
  a:0 0:a r r i v e    [failure]
  0:a a:0 r r i v e    [failure]
  a r:0 0:r r i v e    [failure]
  a 0:r r:0 r i v e    [failure]
  ...
  a r r i v e w:0      [success]
  a r r i v e w:z      [success]
  a r r i v e w:r      [success]
  ...
\end{verbatim}
}

When the correctable error is near the end, almost every symbol is deleted and
inserted with no effect. This may be circumvented by adding, for each deletion
and insertion, a special successor state from which its inverse is absent.

\subsubsection{Ranking Suggestions.}
% Tommi & Miikka

When applying the error model and the language model to input with spelling errors,
the result is typically an ordered set of corrected strings with some probability
associated with each correction. After applying the contextless error correction
described earlier, it is possible to use context words and their potential
analyses to re-rank the corrections \cite{pirinen/2012/cicling}.


\section{Discussion and Future Work}\label{Discussion}

\subsection{Synonym and Translation Dictionaries}
% Krister

Other finite-state applications created with HFST include inflecting thesauri and translation dictionaries.
These applications have been created from the bilingual parallel Princeton WordNet and FinnWordNet. 
The creation of FinnWordNet is documented in \cite{linden/2010}. FinnWordNet contains roughly 150,000 
word meanings in Finnish with their English translations. The synonym dictionaries for Finnish and English
and the Finnish-English and English-Finnish translation dictionaries as well as their demos can be found on \url{hfst.sf.net}.

The inflecting synonym dictionaries were created as a composition of three transducers: (1) a morphological analyzer,
(2) a transducer that replaces one word with another while copying the inflection tags and (3) a morphological
generator as an inversion of the morphological analyzer. The translation dictionaries only have components (1) and (2).
In the future, we intend to take advantage of the weighted transducers to introduce contexts so as to be able to
suggest the most likely synonym or the most likely translation in context.

\subsection{Extending Transducers for Pattern Matching}
% Sam

Advanced, fast and flexible pattern matching is a major requirement for a
variety of tasks in information extraction, such as named entity recognition.
An approach to this task was presented by Lauri Karttunen in
\cite{karttunen/2011}, and an outline for implementing it in HFST is given here.

\subsubsection{Some desiderata for pattern matching.} Several patterns,
including nested matches, should be able to operate in parallel, and
it should be possible to impose contextual requirements (rules) on
the patterns to be matched. Matching should be efficient in space and
time --- in particular, it should be possible to avoid long-range
dependencies which are awkward for FST transformations.
A powerful system should also have a facility for referring
to common subpatterns by name.

\subsubsection{The pmatch Approach.}

For a more detailed overview the reader is directed to \cite{karttunen/2011}.
Here we focus on the aspects of \verb!pmatch! that necessitate extensions
to a FST formalism from the point of view of the implementation.

\verb!pmatch! is presented as a tool for general-purpose pattern matching,
tokenizing and parsing. It is given a series of definitions and a
specialized regular expression, and it then operates on strings, passing
through unmodified any parts of them that fail to match the expression,
and applying transformations to any matched parts. If several matches beginning
from a common point in the input can be made, matches of less than maximal
length are considered invalid.

The expressions may refer to other expressions (possibly combined with each
other by operations on regular languages), contextual conditions
(left or right side, with negation, \verb!OR! and \verb!AND!) and certain
built-in transformation rules. The most interesting of these transformation
rules is \verb!EndTag()!, which triggers a wrap-around XML tag to be inserted
on either side of the match.

Referencing other regexes (including self-reference) is unrestricted, so
the complete system has the power of a recursive transition network (RTN),
and matching is therefore context-free.

The crucial extension-demanding features for an HFST utility
with similar applications are:

\begin{itemize}
\item A distinction between an augmented universal transducer (the top level
  which echoes all input in the absence of matches) and sub-transducers
\item Ignoring non-maximal matching, ie. a left-to-right longest-distance
  matcher
\item An unrestricted system for referencing other transducers by name
\item Special handling of \verb!EndTag()! and instructions for context
  checking
\item Reserved symbols for implementing transducer insertion/reference,
  \verb!EndTag()! and contexts
\end{itemize}

The referencing system is apparently the only one of these that would be
impossible to implement in a strict FST framework; the other extensions suggest
compilations to larger, possibly less efficient transducers.

% \subsection{Implementation in a FST library}

% Whereas \verb!pmatch! offers a unified interface with various built-in
% transformations, sets and a Xerox-inherited syntax and environment,
% our present effort is focused on an API that supports the development
% of more specific applications.

% The call stack is a convenient metaphor for the referential system. When a
% transducer is loaded for matching in the initial position, a referential space
% of transducers is populated by recursively traversing the alphabet, looking
% for references to other transducers. This process is repeated until all
% possible references have been located.

% At every symbol in the input, the top level attempts to transform the longest
% possible continuation (or, if the same maximal continuation can be transformed
% in multiple ways, returns one of them, typically with a weighting system).
% Failures to do this are interpreted as successful identity matches.

% The encountering of an insertion symbol (e.g. \verb!@I.FinnishAdjective@!)
% triggers the calling of a subnetwork transducer with the transducer in
% question. These transducers share the full input buffer (for checking
% an arbitrary amount of context) but have independent spaces for flag
% diacritics, possible alphabet-dependent special symbols (such as \verb!IDENTITY!
% or \verb!UNKNOWN!).

\section{Conclusion}
HFST---Helsinki Finite-State Technology (\url{hfst.sf.net})
is a framework for compiling and applying linguistic descriptions
with finite-state methods. We have demonstrated how HFST uses finite-state techniques 
for creating runtime morphologies, taggers, spellcheckers, inflecting synonym dictionaries as well as 
translation dictionaries using one open-source platform which supports extending  
the descriptions with statistical information to allow the applications 
to take advantage of context. HFST offers a path from language descriptions 
to efficient language applications.


\subsection*{Acknowledgments}
We wish to acknowledge the FIN-CLARIN and META-NORD projects for their financial support 
as well as HFST users for their many constructive suggestions. Addititonally, the research leading to some of these results has received funding from the European Commission’s 7th Framework Program under grant agreement n° 238405 (CLARA).


\bibliographystyle{splncs03}

\bibliography{hfst2012}

\end{document}
% vim: set spell:
