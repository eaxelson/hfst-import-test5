%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Automated Lossless Hyper-Minimization for Morphological Analyzers}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Third Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}


\date{}

\begin{document}
\maketitle
\begin{abstract}
  This paper presents a fully automated lossless hyper-minimization
  method for finite-state morphological analyzers in Xerox {\tt lexc}
  formalism. The method utilizes flag diacritics to preserve the
  structure of the original {\tt lexc} description in the finite-state
  analyzer, which results in reduced size of the analyzer. We compare
  our method against an earlier solution by Drobac et al. \shortcite{drobac2014} which
  requires manual selection of flag diacritics and results in slow
  lookup. We show that our method gives similar size reductions while
  maintaining fast lookup without requiring any manual input.

%  In this paper, we present a fully automated method for inducing flag
%  diacritics into lexical descriptions in the Lexc formalism in order
%  to achieve lossless hyper-minimization. A previous partly automated
%  solution presented in \cite{drobac2014} reduced the size of
%  analyzers. However, it required manual selection of flag diacritics
%  and also slowed down lookup speed. While resulting in size reduction
%  of similar size, our method also maintains fast lookup speed of the
%  morphological analyzer.
\end{abstract}

\section{Introduction}

Morphological analyzers are commonly implemented as finite state
machines (FSM) because finite-state technology enables both fast
processing of large amounts of input and manipulation of the analyzer
using finite-state algebra. Sometimes finite-state analyzers may,
however, become quite large. This may be a problem e.g. when analyzers are
used on mobile devices where a moderate memory footprint is required.
 
%Finite-state transducers are an established way of encoding
%morphological analysers for natural languages. Nevertheless,
%full-scale morphological analysers can often grow to be too large for
%use cases like spell checkers, speech processing and shallow parsing,
%which should have a moderate memory footprint.

The usual way to reduce the size of FSMs is to use a minimization
algorithm which combines suffix-equivalent states
\cite{Aho1986}. Minimization can have a substantial effect on the size
of the FSM but, as it is only able to combine suffix-equivalent
states, there may still be residual redundancy in the state space of the
machine.

%The size of large finite-state machines, both automata and
%transducers, can be reduced by applying a minimization algorithm which
%combines states that are
%suffix-equivalent~\cite{Aho1986}. Minimization can have a substantial
%impact on the size of a finite-state machine, but as it is only able to combine
%suffix-equivalent states, there may still be residual redundancy in
%state space of the machine.

Further size reduction can be accomplished by introducing a limited
form of context-free structure into the finite-state graph using
special symbols called flag
diacritics~\cite{beesley1998constraining}. Using flag diacritics, it
is possible to combine sub-graphs which are equivalent, i.e. accept
the same strings, but which are not necessarily
suffix-equivalent. Flag diacritics are used to couple entrance points
of the sub-graphs with appropriate exit points. During lookup, paths
whose flag diacritics do not match are filtered out. Thus, the
original language of the machine is preserved.

Traditionally, the lexicon writer manually inserts flag diacritics
into the lexicon of the morphological analyzer. There are two major
problems with this approach: (1) In practice, manually inserted flag
diacritics often do not result in great size reduction because many
lexicon writers have poor understanding of the structure
of the finite-state networks built from lexicographical-morphological
descriptions; (2) The addition of flag diacritics to these
descriptions makes them unreadable and unmanageable since the amount
of non-linguistic data in the linguistic description increases.

%flag diacritics have been required a linguist to
%provide the lexicon compiler with their positions. There are two major
%problems with this kind of approach: Firstly, linguists often do not
%have a very good understanding of the structure of the finite-state
%networks built from lexicographical-morphological descriptions;
%Secondly, the addition of flag diacritics to these descriptions makes
%them unreadable and unmanageable since the amount of non-linguistic
%data in the linguistic description increases.

This paper introduces an automated method for inducing flag diacritics
into finite-state morphological analyzers based on the Xerox {\tt
  lexc} formalism.  We refine an earlier approach by Drobac et
al. \shortcite{drobac2014} which required manual selection of flag
diacritics and resulted in slow lookup speeds. We show that our
approach achieves similar reductions in size, but with a fully
automated process. Moreover, the approach presented in this paper is
conceptually simpler and faster because, unlike Drobac et
al. \shortcite{drobac2014}, we do not need additional processing after
applying phonological rules. Additionally, our approach results in
substantially improved lookup speed compared to Drobac et al. \shortcite{drobac2014}
due to an operation which we call path condensation.

We apply our approach to morphological analyzers for three
morphologically complex languages Greenlandic, North Saami, and
Finnish. Compared to Drobac et al. \shortcite{drobac2014}, our approach
results in near equal size reduction for these languages without
requiring any manual intervention. Furthermore, because of the new
\texttt{lexc} approach, compilation time is reduced for all languages
and due to path condensation, lookup time is reduced for all three
languages.

\section{Background}
Finite state morphology~\cite{beesley2003finite} is the
state-of-the-art in writing morphological analysers for natural
languages of the whole range of typologically varying morphological
features. The finite-state approach is built around two practical
concepts: constructing lexicographical descriptions of a language
by using a grammar formalism called \texttt{lexc} and expressing morphophonological
variations as regular expression rules. In this paper, we study lossless
hyper-minimization of finite-state machines derived from lexicographic
descriptions in the \texttt{lexc} formalism.

Hyper-minimization of minimal deterministic finite-state machines
refers to procedures which produce an even smaller finite-state
machine which preserves some of the qualities of the original
deterministic minimal machine. Hyper-minimization comes in two flavors:
lossy and lossless.

Lossy hyper-minimization algorithms~\cite{maletti2011} introduce a
limited amount of changes to the language accepted by the original
machine. This makes the machine susceptible to further size reduction
using conventional minimization algorithms. Lossy hyper-minimization
results in a deterministic machine which allows fast lookup.

In contrast to lossy hyper-minimization, the approach presented in
this paper is lossless. We introduce a limited amount of
non-determinism into the finite-state machine using flag
diacritics. Thus we achieve size reduction, while at the same time
preserving the original language accepted by the finite-state
machine. The non-determinism introduced by the algorithm results in
some reduction in lookup speed, which is not prohibitive in practice.

Finding the smallest non-deterministic finite-state machine equivalent
to a given machine is PSPACE complete \cite{jiang1993} for general
finite-state machines and therefore intractable. Nevertheless, using a
linguistic description rather than a compiled finite-state machine
as the starting point, it is possible to achieve substantial reduction in
the size of the machine without penalties in compilation time.

\section{Methods}\label{sec:methods}

In this section, we describe our lossless hyper-minimization algorithm
of morphological lexicons. The algorithm is applicable on finite-state
lexicons formulated as regular grammars. An example of this kind of
formalism is the Xerox {\tt lexc} formalism \cite{beesley2003finite}.

\begin{figure}
\begin{small}
\begin{verbatim}
LEXICON Root
0 N ;
0 Adj ;

LEXICON N
cat+A:cat Num ;

LEXICON Adj
small+A:small Deg ;

LEXCION Num
+Sg:0 # ;
+Pl:s # ;

LEXICON Deg
+Pos:0 # ;
+Comp:er # ;
+Sup:est # ; 
\end{verbatim}
\end{small}
\caption{A {\tt lexc} lexicon}\label{fig:lexc}
\end{figure}

In the lexc formalism, lexicons are formulated as right branching
regular grammars of morpheme continuation classes as demonstrated in
Figure \ref{fig:lexc}. Each path through the grammar defines one word form
and its analysis. 

{\tt lexc} lexicons are compiled into finite-state transducers, which
accept exactly the set of correspondences between word forms
(e.g. {\tt cats}) and analyses ({\tt cat+N+Pl}) defined by the regular
grammar using well known methods \cite{hopcroft2006}.

Our method is not based on transforming the grammar into a finite-state
machine directly. Instead we utilize so called {\it lexicon joiners}
introduced by \cite{linden2009}. Joiners are specialized labels
(e.g. {\tt \$N\$} and {\tt \$Deg\$}) that are appended to each
entry in the lexicon. They identify the sub-lexicon of the entry and
its continuation class. Together with a set of finite-state
constraints, this completely encodes the structure of the original
grammar.

All regular expressions in this paper use Xerox {\tt xfst} formalism.

\subsection{Compiling lexicons using joiners}

Compiling a lexicon using joiners starts with appending appropriate joiners to each lexicon entry. E.g. the entry {\tt cat+N:cat} with continuation class {\tt Num} in sub-lexicon {\tt N} becomes {\tt \$N\$cat+N\$Num\$:\$N\$cat\$Num\$}. We then compile all the modified lexicon entries into one trie {\tt T} and form its Kleene closure {\tt T*}. 

The closure {\tt T*} accepts arbitrary combinations of morphs from the original lexicon. In order to restrict it to valid combinations produced by the original {\tt lexc} lexicon, we 

\begin{itemize}
\item append root and end joiners to {\tt T*} and get a language {\tt \$Root\$ T* \$\#\$}, and
\item apply one finite-state constraint {\tt CJ} for each joiner {\tt \$J\$} type, which requires that each {\tt \$J\$} has to occur next to another identical joiner {\tt \$J\$}. E.g.
\begin{verbatim}
CJ = NOJ* [$J$ $J$ NOJ*]*
\end{verbatim}
where {\tt NOJ = [?* - \$J\$]}.

\end{itemize}

These constraints encode the structure of the original lexicon. After
composing the language {\tt \$Root\$ T* \$\#\$} and the joiner
constraints, the resulting lexicon transducer {\tt L} accepts exactly
the word forms and analyses defined by the original regular grammar,
though interspersed with joiner symbols. In a final processing state,
the joiners are removed and the lexicon is determinized and minimized.

\subsection{Hyperminimization using Flag Diacritics}

When compiling lexicons using joiners, it is often the case that the
lexicon is small before the joiners are removed and the lexicon is
determinized and minimized but grows in size after this final
processing stage. This may seem surprising, as the transducers
essentially encode the same strings notwithstanding joiner
symbols. However, joiner symbols seem to add useful structure into
the lexicon which helps to maintain a smaller size.

By transforming each joiner {\tt \$J\$} into a flag diacritic {\tt
  @P.J.ON@}, we can maintain the useful structure while at the same
time allowing for lookup of word forms and further processing such as application of
rules. Note, that there is no actual flag diacritic functionality involved
apart from the fact that the symbols are treated as labeled epsilon
symbols.

\subsection{Path Condensation}

Although using joiners results in a smaller lexicon, it also slows down transducer lookup. Sometimes the slowdown can be rather drastic, even around 70\% \cite{drobac2014}.  Our initial experiments showed that this happens mainly because of empty lexicon entries, which can result in long sequences of joiner symbols when chained together. During regular compilation, the chains of joiners vanish when all joiners are removed. In our case, however, the chains are unfortunately preserved.

In order to speed up lookup, we apply a final optimization stage before converting joiners into flag diacritics. In this stage we apply a parallel rewrite rule which transform all sequences of joiner symbols into a single joiner symbol, i.e. ``condenses'' consecutive joiners into one joiner. Let {\tt JOINER} be the set of of joiner symbols {\tt [\$J1\$| ... | \$JN\$]}, then the rule is
\begin{verbatim} 
    JOINER -> 0 || _ JOINER ;
\end{verbatim}
The rule and its inverse are composed with the output and input side of lexicon, respectively. Finally, the lexicon is determinized and minimized.

It is easy to see that path condensation preserves the original language encoded by the {\tt lexc} lexicon disregarding joiner symbols.

\section{Experiments}\label{sec:experiments}
We performed experiments using three full scale open-source
morphological analyzers distributed by the Giellatekno
project\footnote{\url{http://giellatekno.uit.no/}}. We used the analyzers
for Finnish (fin), Greenlandic (kal) and Northern
Sami (sme) available from the Giellatakno repository\footnote{{\tt svn
    co -r 109628 https://victorio.uit.no/langtech/trunk main}}. For
compilation we use the Helsinki Finite State Transducer (HFST) library
and tools\footnote{\url{hfst.sf.net}} \cite{linden2011}.

We compile the morphological analyzers in three different ways

\begin{itemize}
\item Basic compilation without joiner symbols.
\item Compilation using hyper-minimization.
\item Compilation using hyper-minimization and path condensation.
\end{itemize}

For each compilation method, we report results on 

\begin{itemize}
\item Compilation time.
\item Size of the final morphological analyzer.
\item Lookup speed of the final morphological analyzer.
\end{itemize}

Lookup speed is tested using continuous text spanning tens of
thousands of words for each language. All experiments were performed
on an Intel Core i5-4300U laptop with a dual core 1.90 GHz processor
and 16 GB of RAM.

\section{Results}\label{sec:results}

The results of experiments are shown in Table
\ref{tab:res}. 

Compilation time using hyper-minimization and path condensation does
not seem to be prohibitive for any of the analyzers. In fact
compilation time for both Greenlandic and Norther Sami is reduced by
over 60\% compared to compilation without hyper-minimization. The
compilation time for Finnish, however, increases compared with compilation without
hyper-minimization.

Hyper-minimization together with path condensation decreases the size
of the Greenlandic lexicon by over 90\% from 140 MB to 13 MB. The size
of the Northern Sami transducer also decreases by approximately 7\%
but hyper-minimization does not seem to have an appreciable effect on the
Finnish analyzer. These results are almost as good as the results
obtained by Drobac et al. \shortcite{drobac2014} who report size
reduction of 90\% for Greenlandic, 17\% for Northern Sami and 6\% for
Finnish.

Lookup speeds for hyper-minimized transducers with path condensation
are greatly improved to at least 77 \% of the original lookup speed
from around 30 \% of the original speed using only hyper-minimization
without path condensation. These are much better than the lookup
speeds reported by Drobac et al. \shortcite{drobac2014}, where the
lookup speeds for hyper-minimized Finnish and Nortern Sami dropped to
below 40 \% of the original lookup speeds.

\begin{center}
\begin{table}[!htb]
\begin{tabular}{lccc}
\multicolumn{4}{c}{{\sc Finnish}}\\
\hline
               & None & H-M  & H-M + PC \\
\hline
Compile (min)  &    1 &    6 &    5        \\
Size (MB)      &   18 &   17 &   18        \\
Lookup (kw/s)  &  103 &   31 &   79        \\
Speed of orig. &  100\% & 30\% & 77\% \\
\hline & & & \\
\multicolumn{4}{c}{{\sc Greenlandic}}\\
\hline
               & None & H-M  & H-M + PC \\
\hline
Compile (min)  &    6 &    2 &    2        \\
Size (MB)      &  140 &   12 &   13        \\
Lookup (kw/s)  &    2 &    2 &    2        \\
Speed of orig. &  100\% & 100\% & 100\% \\
\hline & & & \\
\multicolumn{4}{c}{{\sc Northern Sami}}\\
\hline
               & None & H-M  & H-M + PC \\
\hline
Compile (min)  &    7 &    1 &    1        \\
Size (MB)      &   14 &   13 &   13        \\
Lookup (kw/s)  &   39 &   13 &   31        \\
Speed of orig. &  100\% & 33\% & 79\% \\
\hline
\end{tabular}
\caption{Results of experiments. The columns denote (1) compilation without hyper-minimization (None), (2) with hyper-minimization (H-M) and (3) hyper-minimization together with path condensation (H-M + PC). The rows denote (1) compilation time, (2) fst binary size, (3) fst lookup speed (as thousands of words per second) and (4) lookup speed compared with the original compiled transducer.}\label{tab:res}
\end{table}
\end{center}

\section{Discussion and Conclusions}\label{sec:discussion-and-conclusions}

Although comparison between our method and \cite{drobac2014} is not
entirely fair, because our method is fully automatic, and their method
allows manual selection of joiner flags, it is clear that our results
are similar with regard to size of the analyzers. Lookup speed, however,
is greatly improved because of path condensation.

Removal of some manually selected flag diacritics following Drobac et
al. \shortcite{drobac2014} would probably give us an even smaller binary
size while maintaining high lookup speed.

\section{Acknowledgements}
...\\
...\\
...
\bibliographystyle{acl}
\bibliography{fsmnlp}

\end{document}