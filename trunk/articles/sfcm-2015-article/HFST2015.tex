\documentclass{llncs}

\usepackage{llncsdoc}

%% PDFLaTeX
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{textcomp}      % for Â° symbol
\usepackage{multirow}
\usepackage{caption}
\usepackage{url}
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{framed}

%% XeLaTeX
% \usepackage{fontspec}
% \usepackage{xunicode}
% \usepackage{xltxtra}

\usepackage{expex}

%
\begin{document}
%
\title{Semantic tagging using HFST}
%
\author{N.N. and N.N.}

 \institute{xx\\
 yy\\
 zz\\
 ww\\
 \email{\{n.n., n.n.\}@xxx.yy}}

\maketitle

% Removed the manual bibstyle in favor of splncs03.bst,
% if it's required fetch it from svn history

\begin{abstract}
%Krister
\end{abstract}

\section*{Introduction}
% Krister (1 p.)

\section{Tokenization using {\tt hfst-pmatch}}\label{sec:tokenization}
% Sam (4 p.)
Tokenization is a necessary first step in most text-based NLP tasks. For some
languages (eg. English) it is often considered to be a mechanical
preprocessing task without linguistic importance, and for others (eg. Chinese)
it is a subtle task given a different name (``segmentation'').

However, even in languages that generally insert spaces between words, there
are issues that influence the quality or feasibility of tools down the
pipeline. We may, for example, want to be able to identify multiword units,
identify compound words and mark their internal boundaries, control various
dimensions of normalisation, or produce possible part-of-speech tags or
deeper morphological analyses.

We describe a general approach to these issues based on morphological
transducers, regular expressions and the pattern matching operation
\verb+pmatch+~\cite{pmatchcite}.

\subsection{Tokenizing with a Dictionary}

\subsubsection{Preserving the Parts of a Multiword Unit}

\subsection{Tokenization Rules as an OOV Fallback}

\subsection{Analysis Cohorts}

\subsection{Chunking}

\subsection{Incorporating other Linguistic Resources}

\section{Morphological Tagging using {\tt hfst-finnpos}}\label{sec:morph-tagging}
% Miikka (4 p.)

FinnPos \cite{silfverberg2015} is a morphological tagger toolkit based
on the Conditional Random Field framework. It is especially geared
toward morphologically rich languages with large label sets which
cause data sparsity and slow down estimation.



Besides optimizations and sub-label dependencies
\cite{silfverberg2014}, Finnpos also provides a flexible way of
integrating a morphological analyzer in the tagging
process. Morphological analyses can be use both as features during
estimation and inference and for constraining the set of possible
morphological labels for word forms. Additionally, the analyzer is
used in lemmatization.

For words not recognized by the morphological analyzer, Finnpos
includes a data driven lemmatizer which is based on the averaged
perceptron classifier.

\subsection{Data Format and Feature Extraction}

\begin{figure}
\begin{verbatim}
The    WORD=The LC_WORD=the       the  DT    _
dog    WORD=dog LC_WORD=dog       dog  NN    _
barks  WORD=barks LC_WORD=barks   bark VBZ   _
.      WORD=. LC_WORD=.           .    .     _

The    WORD=The LC_WORD=the       the  DT    _
cat    WORD=cat LC_WORD=cat       cat  NN    _
meows  WORD=meows LC_WORD=meows   meow VBZ   _
.      WORD=. LC_WORD=.           .    .     _
\end{verbatim}
\caption{Small example of data format.}
\end{figure}

The utilities finnpos-train and finnpos-label read and write input sentences in a five column tab-separated format where each row corresponds to one input word and the columns denote

\begin{enumerate}
  \item  Word form (e.g. ``Dogs'').
  \item  Features separated by spaces (e.g. \verb|WORD=dogs PREV_WORD=the|).
  \item  lemma (e.g. ``dog'').
  \item  Label (e.g. \verb|NNS|).
  \item  Annotations (arbitrary text not containing tabulators).
\end{enumerate}

When using a file as training or development file for finnpos-train, the lemma and label fields have to contain exactly one value each.

If the label field is non-empty in the input for finnpos-label, the tagger will disambiguate between the candidates provided.

The default feature extraction script {\tt finnpos-ratna-feats.py}
extracts the following features for frequent words:

\begin{enumerate}
\item Word form.
\item Previous word form.
\item Next word form.
\item Previous two words.
\item Next two words.
\end{enumerate}

For rare words, the script also extracts orthographic features

\begin{enumerate}
\item Suffixes and prefixes up to length 10.
\item Capitalization.
\item Whether the word includes digits or dashes.
\end{enumerate}

Users may add their own features or write their own feature extraction script.

\subsection{Training and Using a Model}

Users can train their own models using the utility {\tt
  finnpos-train}. The parameters of the training process are set using
a configuation file.

\begin{figure}
\begin{verbatim}
# Config file for FinnTreeBank tagger.

guess_mass=0.999
beam_mass=0.999
max_train_passes=3
max_lemmatizer_passes=7
\end{verbatim}
\caption{Example configuration file.}
\end{figure}

\begin{figure}
\begin{tabular}{ll}
{\tt suffix\_length} & The maximal suffix length used in feature extraction during lemmatization.\\
{\tt degree} & The degree of structured features used by the tagger. \\
{\tt max\_train\_passes} & The maximal number of training passes during estimation of tagger parameters.\\
{\tt max\_lemmatizer\_passes} & The maximal number of training passes during estimation of lemmatizer parameters.\\
{\tt max\_useless\_passes} & Determined the maximal number of passes over the training data that do not improve\\
&  the tagging accuracy for the development data.\\
{\tt guess\_mass} & A generative label guesser is used to prune the label candidates considered for each word \\
                  & during training. {\tt guess\_mass} is a float in range (0, 1.0) which detemines the mass \\
                  &  preserved by the guesser for each word.\\
{\tt beam\_mass} & FinnPos uses an adaptive beam to prune search histories during beam search. \\
                 &  This parameter determines the probability mass of the beam.
\end{tabular}
\caption{Description of configuration file fields.}
\end{figure}

\subsection{Using a Morphological Analyzer}

\section{Semantic Tagging using {\tt hfst-pmatch}}\label{sec:sem-tagging}
% Sam (4 p.)

\section{Distortion Filtering with Weighted Regular Expressions}
% Sam & Miikka (3 p.)

\section{Background}\label{sec:background}
% Erik (2 p.)
During the past two years, there have been many improvements to the HFST 
interface. We now have a native lexc compiler and the possibility to 
hyperminimize the lexc lexicons. There have also been many bugfixes and 
improvements to the native XFST compiler. Flag diacritics are fully supported 
in all HFST command line tools. Weighted regular expressions, especially replace rules.

We have a download page (http://hfst.sourceforge.net/downloads/). For Linux 
users, we recommend a Debian installation which is offered via Apertium. There
are instructions and links to Apertium web pages. For Mac and Windows users,
we offer binaries for eight tools: hfst-xfst, hfst-lexc, hfst-pmatch,
hfst-pmatch2fst, hfst-twolc, hfst-proc, hfst-lookup and hfst-optimized-lookup.
There is also a separate MacPorts installation available.

We have tentatively added Xerox's xfsm library as one implementation to HFST.
In Table~\ref{tab:operation_times} we whow preliminary benchmarking results for
minimization and composition implementations for an unweighted OMorFi intermediate
transducer with ca 250 000 states and 600 000 transitions (in composition the transducer was composed
with itself).

\begin{table} [h!]
  \centering
  \caption{Times for two transducer operations with different back-end implementations.
    Times are given in seconds and averaged over 10 runs.}
  \begin{tabular}{c c c }
    \hline
    Back-End & Minimization & Composition \\ \hline
    xfsm & 0,74 & 0,49 \\
    foma & 0,53 & 2,00 \\
    SFST & 1,32 & 1,78 \\
    OpenFst & 5,09 & 2,34 \\ \hline
  \end{tabular}
  \label{tab:operation_times}
\end{table}

(fstminimize, old and new version, are there differences)

The slowness of openfst is possibly due to the old version (our and the newest 
version number) that we have as our
back-end and maybe because openfst handles weighted transducers. In xfsm
composition, unknown and identity symbols are handled as a part of the 
composition itself, but for other back-ends some post- and preprocessing is
needed. This might explain why they are slower in composition.

\section{Discussion and Conclusion}\label{sec:discussion}
% Krister (1 p.)

\bibliographystyle{splncs03}
\bibliography{sfcm-2015}

\end{document}
% vim: set spell:
