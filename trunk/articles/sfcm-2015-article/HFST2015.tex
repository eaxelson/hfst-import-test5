\documentclass{llncs}

\usepackage{llncsdoc}

%% PDFLaTeX
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{textcomp}      % for Â° symbol
\usepackage{multirow}
\usepackage{caption}
\usepackage{url}
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{automata,positioning}
\usepackage{framed}
\usepackage{gb4e}

%% XeLaTeX
% \usepackage{fontspec}
% \usepackage{xunicode}
% \usepackage{xltxtra}

\usepackage{expex}

%
\begin{document}
%
\title{Semantic tagging using HFST}
%
\author{N.N. and N.N.}

 \institute{xx\\
 yy\\
 zz\\
 ww\\
 \email{\{n.n., n.n.\}@xxx.yy}}

\maketitle

% Removed the manual bibstyle in favor of splncs03.bst,
% if it's required fetch it from svn history

\begin{abstract}
%Krister
\end{abstract}

\section*{Introduction}
% Krister (1 p.)

\section{Tokenization using {\tt hfst-pmatch}}\label{sec:tokenization}
% Sam (4 p.)
Tokenization is a necessary first step in most text-based NLP tasks. For some
languages (eg. English) it is often considered to be a mechanical
preprocessing task without linguistic importance, and for others (eg. Chinese)
it is a subtle task given a different name (``segmentation'').

However, even in languages that generally insert spaces between words, there
are issues that influence the quality or feasibility of tools down the
pipeline. We may, for example, want to be able to identify multiword units,
identify compound words and mark their internal boundaries, control various
dimensions of normalisation, or produce possible part-of-speech tags or
deeper morphological analyses.

We describe a general approach to these issues based on morphological
transducers, regular expressions and the pattern matching operation
\verb+pmatch+~\cite{pmatchcite}.

\subsection{A Short Introduction to pmatch}



\subsection{Tokenizing with a Dictionary}

A tokenizer consisting of the input side of a morphological dictionary
with good coverage in vocabulary and derivation can satisfactorily solve
many tokenization headaches on its own. For example, consider the compound
plural possessive in

\begin{exe}
  \item The Attorney-Generals' biographies are over there.
\end{exe}

To get tokenization exactly right, a tokenization rule needs to understand that
the hyphen is joining parts of a compound word (unlike in eg.
``Borg-McEnroe'') and that the apostrophe is indicating the possessive form,
not the end of a quotation.

A dictionary can also be augmented to recover from issues stemming from
formatting or digitalisation issues. For example, a text may split words
at line boundaries with hyphens, as in

\begin{exe}
\item He seemed suddenly to have been endowed with super-

  human strength
\end{exe}

Here the only correct tokenization is ``superhuman'' rather than ``super''
and ``human'', but a dictionary would miss this possibility. However,
we can use a finite-state operation to allow the string \verb+-\n+ (hyphen
followed by a newline) to appear anywhere inside the words in the dictionary.
In regular expressions this operation is sometimes called ``ignoring'' and in
\verb+pmatch+ is invoked with a forwards slash, like so:

\begin{center}
  \begin{framed}
\begin{verbatim}
    define dict_with_linebreaks [dict]/[{-}"\n"]
\end{verbatim}
  \end{framed}
\end{center}

The \verb+\n+ is in double quotes in order to invoke parsing it as a newline
rather than as a literal ``\textbackslash n''.

\subsubsection{Preserving the Parts of a Multiword Unit}

Dictionaries are often equipped with a collection of short idioms (eg.\@
``in view of'') and other tokens which include whitespace (eg.\@
``New York''). While these are useful, it may be too early at this stage
to fix the tokenization as the longest possible match. A discriminative
tagger won't be able to make the correct choice in

\begin{exe}
  \item The ball was in view of the referee.
\end{exe}

if it only sees a tokenization where \emph{in view of} is a single token.

\subsection{Tokenization Rules as an OOV Fallback}

When no match from the dictionary is possible, \verb+pmatch+ by default
writes input to output as-is. Ideally, we'd like to assign every character
in input either to some intentional token, or discard it (this concerns mainly
whitespace between tokens). To achieve this, we must write a rule representing
``some other word'' and disjunct it with the dictionary at the top level of
our tokenizer. To discard unwanted input, we must either adopt the convention
(and use an appropriate operating mode) that parts of input that haven't been
given any tag are discarded, or explicitly add a rule that rewrites single
unmatched characters to empty strings.

\begin{framed}
define wordchars 
\end{framed}

\subsection{Analysis Cohorts}

In addition to bare tokens, many downstream tools use analysis cohorts, ie.\@
the full set of possible baseforms and morphological tags for the token in
question.

\subsection{Chunking}

Abbreviations ending in a period

\subsection{Incorporating other Linguistic Units}

\section{Morphological Tagging using {\tt hfst-finnpos}}\label{sec:morph-tagging}
% Miikka (4 p.)

FinnPos \cite{silfverberg2015} is a morphological tagger toolkit based
on the Conditional Random Field framework. It is especially geared
toward morphologically rich languages with large label sets which
cause data sparsity and slow down estimation.



Besides optimizations and sub-label dependencies
\cite{silfverberg2014}, Finnpos also provides a flexible way of
integrating a morphological analyzer in the tagging
process. Morphological analyses can be use both as features during
estimation and inference and for constraining the set of possible
morphological labels for word forms. Additionally, the analyzer is
used in lemmatization.

For words not recognized by the morphological analyzer, Finnpos
includes a data driven lemmatizer which is based on the averaged
perceptron classifier.

\subsection{Data Format and Feature Extraction}

\begin{figure}
\begin{verbatim}
The    WORD=The LC_WORD=the       the  DT    _
dog    WORD=dog LC_WORD=dog       dog  NN    _
barks  WORD=barks LC_WORD=barks   bark VBZ   _
.      WORD=. LC_WORD=.           .    .     _

The    WORD=The LC_WORD=the       the  DT    _
cat    WORD=cat LC_WORD=cat       cat  NN    _
meows  WORD=meows LC_WORD=meows   meow VBZ   _
.      WORD=. LC_WORD=.           .    .     _
\end{verbatim}
\caption{Small example of data format.}
\end{figure}

The utilities finnpos-train and finnpos-label read and write input sentences in a five column tab-separated format where each row corresponds to one input word and the columns denote

\begin{enumerate}
  \item  Word form (e.g. ``Dogs'').
  \item  Features separated by spaces (e.g. \verb|WORD=dogs PREV_WORD=the|).
  \item  lemma (e.g. ``dog'').
  \item  Label (e.g. \verb|NNS|).
  \item  Annotations (arbitrary text not containing tabulators).
\end{enumerate}

When using a file as training or development file for finnpos-train, the lemma and label fields have to contain exactly one value each.

If the label field is non-empty in the input for finnpos-label, the tagger will disambiguate between the candidates provided.

The default feature extraction script {\tt finnpos-ratna-feats.py}
extracts the following features for frequent words:

\begin{enumerate}
\item Word form.
\item Previous word form.
\item Next word form.
\item Previous two words.
\item Next two words.
\end{enumerate}

For rare words, the script also extracts orthographic features

\begin{enumerate}
\item Suffixes and prefixes up to length 10.
\item Capitalization.
\item Whether the word includes digits or dashes.
\end{enumerate}

Users may add their own features or write their own feature extraction script.

\subsection{Training and Using a Model}

Users can train their own models using the utility {\tt
  finnpos-train}. The parameters of the training process are set using
a configuation file.

\begin{figure}
\begin{verbatim}
# Config file for FinnTreeBank tagger.

guess_mass=0.999
beam_mass=0.999
max_train_passes=3
max_lemmatizer_passes=7
\end{verbatim}
\caption{Example configuration file.}
\end{figure}

\begin{figure}
\begin{tabular}{ll}
{\tt suffix\_length} & The maximal suffix length used in feature extraction during lemmatization.\\
{\tt degree} & The degree of structured features used by the tagger. \\
{\tt max\_train\_passes} & The maximal number of training passes during estimation of tagger parameters.\\
{\tt max\_lemmatizer\_passes} & The maximal number of training passes during estimation of lemmatizer parameters.\\
{\tt max\_useless\_passes} & Determined the maximal number of passes over the training data that do not improve\\
&  the tagging accuracy for the development data.\\
{\tt guess\_mass} & A generative label guesser is used to prune the label candidates considered for each word \\
                  & during training. {\tt guess\_mass} is a float in range (0, 1.0) which detemines the mass \\
                  &  preserved by the guesser for each word.\\
{\tt beam\_mass} & FinnPos uses an adaptive beam to prune search histories during beam search. \\
                 &  This parameter determines the probability mass of the beam.
\end{tabular}
\caption{Description of configuration file fields.}
\end{figure}

\subsection{Using a Morphological Analyzer}

\section{Semantic Tagging using {\tt hfst-pmatch}}\label{sec:sem-tagging}
% Sam (4 p.)

\section{Weighted Regular Expressions}
% Sam & Miikka (3 p.)

\subsection{A Weighted Edit Distance Model}

Parallel weighted replace rules offer a systematic representation of various
regular transformations. Consider an edit distance transducer that accepts
any string and outputs all the strings that result from making up to $n$ edits
to it. An abbreviated example:

\begin{framed}
\begin{verbatim}
"a" -> "a" | ["" | "b" | "c" | ...]::1.0 ,,
"b" -> "b" | ["" | "a" | "c" | ...]::1.0 ,,
...
"" -> "" | ["a" | "b" | ...]::1.0 ;
\end{verbatim}
\end{framed}

the \verb+,,+ indicates that the replacements are to be compiled to operate
in parallel. Completed, this example results in a transducer that rewrites
its input with any number of edits, and gives it a cumulative weight of
$1.0$ per edit (assuming that weights are added, ie.\@ that the weight semiring
has addition as its multiplication operation, as in the log or tropical
semirings).

Suppose we have a different notion of edit, and further that we want to weight
different edits differently. We can incorporate swaps of consecutive
characters as follows:

\begin{framed}
\begin{verbatim}
{ab} -> {ab} | {ba}::1.5 ,,
{ac} -> {ac} | {ca}::1.1 ,,
... 
\end{verbatim}
\end{framed}

The replacements are quite arbitrary. They can also include context conditions,
with a number of different replacements operating in different contexts with
different weights all compiled together in parallel into one rule.

The edits can be constrained to occur a given number of times with an edit
filter:

\begin{framed}
\begin{verbatim}
define edit_distance_with_markers
  "a" -> "a" | [ ["b"::1.1 | "c"::1.2 ...] "<EDIT>"] ,,
  "b" -> "b" | [ ["a"::0.9 | "c"::1.0 ...] "<EDIT>"] ,,
  ...
define filter [ [? - "<EDIT>"]* ["<EDIT>":""] [? - "<EDIT>"*] ]^{1,3}
define 1_to_3_edits edit_distance_with_markers .o. filter;
\end{verbatim}
\end{framed}

Here each rule making an edit appends a special marker, which is removed
by composing with \verb+filter+, which allows (in this case) between 1 and 3
edits to pass through, replacing them with the empty string.

\section{Background}\label{sec:background}
% Erik (2 p.)
During the past two years, there have been many improvements to the HFST 
interface. We now have a native lexc compiler and the possibility to 
hyperminimize the lexc lexicons. There have also been many bugfixes and 
improvements to the native XFST compiler. Flag diacritics are fully supported 
in all HFST command line tools. Weights can be defined for all regular expressions,
including replace rules.

We have a download page (http://hfst.sourceforge.net/downloads/) that tells what 
we offer and where it can be found. For Linux users, we recommend a Debian installation
which is offered via Apertium. For Mac and Windows users,
we offer binaries for eight tools: hfst-xfst, hfst-lexc, hfst-pmatch,
hfst-pmatch2fst, hfst-twolc, hfst-proc, hfst-lookup and hfst-optimized-lookup.
There is also a separate MacPorts installation available.

We currently have a Python interface for HFST for Linux and Mac, generated with SWIG directly
from our C++ API. We are working to create an alternative, more Python-like interface
and implement it also for Windows.

We have tentatively added Xerox's xfsm library as one implementation to HFST.
In Table~\ref{operationtimes} we whow preliminary benchmarking results for different back-end
implementations for two opearations, minimization and composition. As a test transducer we use an 
unweighted OMorFi intermediate transducer with ca 250 000 states and 600 000 transitions (in composition
the transducer is composed with itself).

\begin{table}[h!]
  \centering
  \caption{Times for two transducer operations with different back-end implementations.
    Times are given in seconds and averaged over 10 runs.}
  \begin{tabular}{c c c }
    \hline
    Back-End & Minimization & Composition \\ \hline
    xfsm & 0,74 & 0,49 \\
    foma & 0,53 & 2,00 \\
    SFST & 1,32 & 1,78 \\
    OpenFst & 5,09 & 2,34 \\ \hline
  \end{tabular}
  \label{operationtimes}
\end{table}

In minimization, the openfst implementation is clearly the slowest. We tested minimizing
the transducer with native openfst command line tools, but the result was still 5,36 seconds.
The few extra milliseconds come from reading and writing files because not all openfst tools support reading
and writing in a pipeline. As we have an old version (1.3.1) of openfst as our back-end, we also
used the command line tools of the newest version (1.4.1), but minimization was not any faster.
The slowness is probably due to the fact that openfst must be ready to handle weights.

In composition, the xfsm implementation is the fastest. In xfsm composition, unknown and identity
symbols are handled as a part of the composition itself, but for other back-ends some post- and preprocessing is
needed. This probably explains why they are all slower.

\section{Discussion and Conclusion}\label{sec:discussion}
% Krister (1 p.)

\bibliographystyle{splncs03}
\bibliography{sfcm-2015}

\end{document}
% vim: set spell:
