\documentclass{llncs}

\usepackage{llncsdoc}
\usepackage{multirow}
\usepackage{caption}
\usepackage{url}

\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{expex}

%
\begin{document}
%
\title{HFST---a System for Creating NLP Tools}
%
\author{Krister Lind\'{e}n \and Erik Axelson \and Senka Drobac \and Sam Hardwick \and\\
Tommi A Pirinen \and Miikka Silfverberg \and ...}

\institute{University of Helsinki\\
Department of Modern Languages\\
Unioninkatu 40 A\\
FI-00014 Helsingin yliopisto, Finland\\
\email{\{krister.linden, erik.axelson, senka.drobac, sam.hardwick,\\
tommi.pirinen, miikka silfverberg, ...\}@helsinki.fi}}

\maketitle

% Removed the manual bibstyle in favor of splncs03.bst,
% if it's required fetch it from svn history

\begin{abstract}
%Krister
\keywords{keywords here}
\end{abstract}


\section*{Introduction}
% Krister

\section{Applications and Tests}\label{hfst:structural-layout}
% Miikka

\subsection{Language identification}
Language identification is the task of recognizing the language of a
text or text fragment. It is highly useful in applications, that need
to process documents written in various languages where the language
might not be overtly marked in the document. For example a translation
application might need to identify the language of a document in order
apply the correct translation model. Another example is a speller for
Finnish might need to identify paragraphs, that are written in
English, in order to not perform spell checking on the paragraphs.

In this section we outline how to use Hfst tagger tools and language
identification tools for creating language identifiers. We also
present an experiment on language identification for documents written
in Dutch, English, Estonian, Finnish, German or Swedish. The
experiment shows that Hfst language identifiers are highly accurate
(99.5\% of the input sentences were correctly classified).

There are several methods for performing languages
identification. Highly accurate language identification can be
accomplished by treating documents as letter sequences and training
Markov chain from training documents whose language is
known~\cite{cavnar/1994}. One Markov chain is trained for each
language that the system recognizes. Language identification consists
of applying each Markov chain on input and choosing the language whose
model gives the highest likelihood for the text. 

Hfst language identifiers adopt a Markov chain framework, which can be
implemented using weighted finite-state technology. Using Hfst tagger
tools~\cite{silfverberg/2011}, we train Markov models for all
languages. A separate program, {\tt hfst-guess-language}, reads the
models and input text and labels each sentence with the language,
whose model gave the highest likelihood for the sentence.

We present an experiment on applying Hfst language identifiers for
guessing the language of sentences written in six languages. For all
languages except Swedish, we used training data from corpora
containing newspaper text. For Swedish, we used more general text.

For Dutch we used the Alpino treebank~\cite{bouma/2000}, for English
we used the Penn Treebank~\cite{marcus/1993}, for Estonian we used the
Estonian National
Corpus~\footnote{http://www.cl.ut.ee/korpused/segakorpus/}, for
Finnish we used text from the largest Finnish newspaper Helsingin
Sanomat year 1995~\footnote{http://www.csc.fi/kielipankki/}, for
German we used the TIGER Corpus~\cite{brants/2002} and for Swedish we
used Talbanken~\cite{einarsson/1976}.

\begin{table}
\begin{center}
\begin{tabular}{l|rr}
Language & train data~~~~~~~~~ & test data~~~~~~~~~\\
\hline
Dutch    & 245,000 chars  & 24,000 chars\\
English  & 265,000 chars  & 26,000 chars\\
Estonian & 238,000 chars  & 23,000 chars\\
Finnish  & 155,000 chars  & 14,000 chars\\
German   & 280,000 chars  & 28,000 chars\\
Swedish  & 164,000 chars  & 16,000 chars\\
\end{tabular}
\caption{For each language, we used 2000 sentences for training and
  200 sentence for testing. We give the sizes of the data sets in
  utf-8 characters.}\label{tab:lang-id-data}
\end{center}
\end{table}

For each language, we chose 2200 sentences for training and
testing. Of the sentences, every eleventh sentence was used for
testing and the rest for training. This totals 2000 sentences for
training and 200 sentences for testing for each language. The sizes of
the data sets in utf-8 characters are described in
Table~\ref{tab:lang-id-data}. The average length of a sentence in
characters was shorter for Finnish and Swedish than for the other
languages.

\begin{table}
\begin{center}
\begin{tabular}{l|l}
Language & Accuracy\\
\hline
Dutch    & ~~~99.0\%\\
English  & ~~~99.5\%\\
Estonian & ~~~99.5\%\\
Finnish  & ~~~99.5\%\\
German   & ~~100.0\%\\
Swedish  & ~~~99.5\%\\
\hline
ALL      & ~~~99.5\%
\end{tabular}
\caption{We give accuracy of the language guesser per language and for
  all languages.}\label{tab:lang-id-acc}
\end{center}
\end{table}

We ran the language identifier for test sentences from all six
languages (totally 1200 sentences) and computed the accuracy of the
language identification system as the $corr / all$, where $corr$ is
the number of sentences whose language was correctly guessed and $all$
is the number of all sentences. In Table~\ref{tab:lang-id-acc}, we
show results for each individual language and all languages
combined. 

Of all sentences, 99.5\% were correctly classified, which demonstrates
that the language identifiers are highly accurate. This is encouraging
because Finnish and Estonian have similar orthographies. This applies
to German, Swedish and Dutch as well.

Currently identification is limited to identifying the closest
language corresponding to a sentence. There is no option to label a
sentence as belonging to an unknown language. It could be possible to
apply some threshold likelihood $l(n)$ which would state that a model
has to give a sentence of $n$ characters at least likelihood $l(n)$,
in order for the sentence to be labeled as belonging to the language
of the model. 

In practice it has been very difficult to establish $l(n)$ in a
reliable way. It is likely to be dependent on the genre of the
document, which makes it less useful. Identifying unknown language
using Hfst language identifiers thus remains future work.

\subsection{Morphologies and Guessers}
\label{sec: morph-guessers}
% Juha, Miikka
\begin{itemize}
\item Morphological guessers are needed to compensate for insufficient
  coverage of morphological analyzers.
\item Suffix based guessing has proved sufficient in practice for
  agglutinating languages like Finnish~\cite{linden/2009/nodalida}.
\item Using hfst tools it is possible to transform an existing morphological analyzer into a guesser. 
\item If the analyzer marks declensions of words, the guesser will
  guess the declension in addition to the inflectional information.
\item In addition the guesser constructs a stem for the word.
\item If the morphological analyzer is weighted, the weights are used
  to arrange guesses in order of likelihood. The guesser also prefers
  longer matches with words recognized by the analyzer to shorter
  ones.
\item Using information about declension classes also helps in
  guessing. Some declensions might only contain a couple of words,
  implying that an unknown word is unlikely to belong to that
  declension. In addition it might be known that new words are likely
  to be added to a small set of declension classes.
\item The guesser is a convenient tool when extending the lexicon.
\item Some results for Finnish.
\end{itemize}

\subsection{Spell-checking}
% Tommi

Weighted finite-state methods in performing spell-checking and correction is a
relatively recent branch of study in research of spell-checking. The concept is
simple: finite-state morphological analysers and such can be trivially ported
into spell-checking dictionaries providing a language model for the correctly
spelled words in the spell-checking system. A baseline finite-state model for
correcting spelling errors can be inferred from the language model by creating
a Levenshtein-Damerau automaton based on the alphabetic characters present in
the language. The language model can be simply trained to prefer more common
words when the Levenshtein-Damerau distance between to suggestions is the same.
This is done by basic unigram language model training that simply maximises
the frequency of the suggested word. To our experience even relatively moderate
training material will gain improvement in quality as the statistical training
improves the discriminative power of the model, and the likelihood of random
typing error is more likely in frequent words.

The practical process of creating a finite-state spell-checker and corrector
is really simple: given an analysator capable of recognising correctly spelled
word-forms of a language, take a projection to the surface forms to create a
single-tape automaton. The automaton is trained with corpus word-form list, 
where end-weight of each word-form is e.g. $-\log\frac{c(wf)}{CS}$, where 
$c(wf)$ is the count of word-forms, and $CS$ is the corpus size. Words not
found in the corpus are given some weight $w_{max} > -\log\frac{1}{CS}$ to
push them in the bottom of the suggestion list; this weighting can be done
in finite-state algebra by composition of weighted $\Sigma^{\star}$ language,
or by manually fixing the data structure.

The error model can be improved from the baseline Levenshtein-Damerau distance
metric as well. For this purpose we need an error corpus, that is, set of
errors with their frequencies. This can be semi-automatically extracted from
weakly annotated sources, such as Wikipedia. From wikipedia we get, among tons
of other things, word-to-word corrections. It is possible to use the specific
word-to-word corrections to create simple extension of common confusables to
error model. Another way is to re-align the corrections using the
Damerau-Levenshtein algorithm, and train the original distance measure with
frequencies of the corrections in same manner as we did for word-forms above.

As an example of simplicity of this process, we have obtained an open source
German morphological analyser
morphisto~\footnote{\url{http://code.google.com/p/morphisto/}} to generate a
spelling checker, trained it with word-forms extracted from German
Wikipedia~\footnote{\url{http://de.wikipedia.org}} and applied it to Wikipedia
data to find spelling errors and correct them with n~\% of precision. The whole
script to do this is in our version control~\footnote{\url{}}, and it took us
no more than one work day by one researcher to implement this application.

\subsection{Named Entity Recognition}
% Jyrki, Juha (Sam?)

\subsection{Language Generation for Unknown words}
\begin{itemize}
\item In language generation the goal is to generate automatic
  messages about a computer system in natural language.
\item The system includes some concepts like weather phenomena or
  airline connections and it generates messages about the state of the
  concepts for a user.
\item For agglutinating languages this presents n challenge because
  the base forms of the concepts to be inflected depending on syntactic
  context.
\item A generation system can utilize a morphological analyzer, but
  all of the concepts might not be recognized by the analyzer.
\item Therefore another mechanism is needed for generating word forms
  from the lemma of the base form of the concept.
\item Morphological guessers presented in~\ref{sec: morph-guessers}
  can be used for generation of word forms in addition to analysis.
\item When the word class and base form of the word are known, a
  paradigm of word forms can be generated.
\item This can also be used when expanding a morphological
  analyzer. It can be used to generate a list of suggestion forms,
  which can be examined quickly to determine whether a word could
  belong to a given declension class.
\item Example from Finnish.
\end{itemize}

\section{Examples for User Environments}

\subsection{An Interface in Python}
% Erik, Tommi

In addition to API library and command line tools, HFST library can also be used
through SWIG-generated python bindings. Currently the bindings are offered for
python programming language versions 2 and 3. All HFST functionalities are available
via both versions, but the python interpreters themselves have some differences.
Python 2 allows HFST exceptions to be caught directly, but python 3 requires the use
of a special wrapper function written as a part of the bindings. On the other hand, 
python 3 has better support for unicode characters, so it is probably a better choice
for linguistic applications.

The python bindings in particular make it smooth to use language models
developed for HFST in rapid prototyping of advanced tools. For an example, in
my current project I developed a chunker for Finnish language by simply
bracketing adjacent agreeing cases and few other similar expressions with
basically few lines of code on top of existing morphological analysers. E.g.
given Finnish sentence ``miljoona kärpästä voi olla väärässä paikassa'' we get
bracketing all three phrases bracketed by really simple python based bracketing
that are illustrated in following figure gloss:

\ex
\begingl
\gla Miljoona$_1$ kärpästä$_1$ voi$_2$ olla$_2$ väärässä$_3$ paikassa$_3$//
\glb Million-{\sc Num} fly-{\sc Par} can-{\sc AuxV} be-{\sc Inf} wrong-{\sc Ine} place-{\sc Ine}//
\glft `Million flies can be in a wrong place' //
\endgl
\xe


\subsubsection{A Chatroom Morphology tool}
% Sam

\subsection{HFST on Unix, Mac and Windows}
% Erik, Tommi

Portability has been one of the design goals of HFST system. The current 
versions are available or compilable on all POSIX-supporting systems, including
Linuxes, Mac OS X and Cygwin under Windows system. Compilation is also possible 
on MinGW under Windows. Another layer of portability
is attained by HFST's programming language bindings via SWIG, it is possible to
use full HFST library through it's SWIG interfaces in all supported systems with
python versions 2 and 3.


\subsection{Other user-oriented items}
% Erik

\section{Under the Hood}

\subsection{An independent XFST module}
% Erik (Erik)

HFST command line tools include an XFST parser tool that can be used in interactive
mode or to compile scriptfiles. The tool implements the same functionalities as the
original XFST (Xerox Finite-State Tool) which is a general-purpose utility for computing
with finite-state networks.


\subsection{XFST regexp compilation}
% Senka, Miikka

\subsection{Pmatch with applications for NER}
% Sam

\subsection{Other HFST development tools}
% Erik

\section{Discussion}\label{hfst:discussion}

\subsection{HMM vs. CRF for English and Finnish taggers}
% Miikka

\subsection{Adding rules to statistical systems (taggers and parser)}
% Miikka

\section{Conclusion}\label{hfst:conclusion}

\subsubsection*{Acknowledgments}

\bibliographystyle{splncs03}
\bibliography{sfcm-2013}

\end{document}
% vim: set spell:
