\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
%\usepackage{xltxtra}
%\usepackage{hyperref}
%\usepackage{ifpdf}

\begin{document}
\title{Improving~Finite-State~Spell-Checker~Suggestions with 
Part~of~Speech~N-Grams}

\author{Tommi A Pirinen and Miikka Silfverberg and Krister Lind\'{e}n}

\institute{University of Helsinki\\
Department of Modern Languages\\
University of Helsinki, 00014\\
tommi.pirinen@helsinki.fi, miikka.silfverberg@helsinki.fi, krister.linden@helsinki.fi}

\date{\today}

\maketitle

\begin{abstract}
  In this paper we demonstrate a finite-state implementation of context-aware
  spell checking utilizing an n-gram based part of speech (POS) tagger to rerank the
  suggestions from a simple edit-distance based spell-checker. We
  demonstrate the benefits of context-aware spell-checking for English and Finnish and
  introduce modifications that are necessary to make traditional n-gram
  models work for morphologically more complex languages, such as Finnish.
\end{abstract}

\section{Introduction}

Spell-checking by computer is perhaps one of the oldest and most researched
applications in the field of language technology starting from the mid 20th
century~\cite{damerau/1964}. One of the crucial parts of spell-checking---both
from an interactive user-interface point of view and for unsupervised correction
of errors---is the production of spelling suggestions.  In this article we test
various finite-state methods for using context and shallow morphological
analysis to improve the suggestions generated by traditional edit distance
measures or unigram frequencies such as \cite{pirinen/2010/lrec}.

The spell-checking task can be split into two parts, i.e. \emph{detection} and
actual \emph{correction} of the spelling errors. The spelling errors can be
detected in text as word forms that are unlikely to belong to the
natural language in question, such as writing `cta' instead of `cat'. This form
of spelling errors is commonly called \emph{non-word (spelling) errors}.
Another form of spelling errors is word forms that do not belong to the given
context under certain syntactic or semantic requirements, such as writing
`their' instead of `there'. This form is correspondingly called \emph{real-word
(spelling) errors}. The non-word type of spelling errors can easily be detected
using a dictionary, whereas the detection of the latter type of errors typically
requires syntactic analysis or probabilistic methods~\cite{mitton/2009}. For
the purpose of this article we do not distinguish between them, as the same
correction methods can be applied to both.

The correction of spelling errors usually means generating a list of word forms
belonging to the language for a user to chose from. The mechanism for generating 
correction suggestions for the erroneous word-forms
is an \emph{error-model}. The purpose of an error-model is to act as a
filter to revert the mistakes the user typing the erroneous word-form has made.
The simplest and most traditional model for making such corrections is the
Levenshtein-Damerau edit distance algorithm, attributed initially to
\cite{levenshtein/1966} and especially in the context of spell-checking to
\cite{damerau/1964}. The Levenshtein-Damerau edit distance assumes that spelling
errors are one of insertion, deletion or changing of a single character to
another, or swapping two adjacent characters, which models well the spelling
errors caused by an accidental slip of finger on a keyboard. It was originally
discovered that for most languages and spelling errors, this simple method
already covers 80~\% of all spelling errors~\cite{damerau/1964}. This model is
also language-independent, ignoring the differences in character repertoires of
a given language. Various other error models have also been developed, ranging
from confusion sets to phonemic folding~\cite{kukich/1992}.

In this paper, we evaluate the use of context for further fine-tuning of the
correction suggestions. The context is still not commonly used in
spell-checkers. According to \cite{kukich/1992} it was lacking in the majority
of spell-checkers and while the situation may have improved slightly for some
commercial office suite products, the main spell-checkers for open source
environments are still primarily context-ignorant, such as
hunspell\footnote{\url{http://hunspell.sf.net}} which is widely used in the open source
world.  For English, the surface word-form trigrams model has been demonstrated
to be reasonably efficient \cite[for non-word cases]{church/1991} and
\cite[for real-word cases]{mays/1991}. As an additional way to improve the 
set of suggestions, we propose to use morphosyntactically
relevant analyses in context. In this
article, we evaluate a hybrid model combining a statistical
morphological tagger~\cite{silfverberg/2011} with surface word-form
n-grams.

Furthermore, we test the context-based spelling methods using both English and
Finnish language materials to ensure the applicability of the method for
morphologically different languages. The reason for doing this is two-fold; firstly
the fact that English has rather low morphological productivity may make
it behave statistically differently from other languages. On the other hand,
English has the largest amount of freely available text corpora. For other
languages, the availability of free corpora, especially annotated material, is
often seen as a problem.

The article is laid out as follows: in Section~\ref{sec:material}, we introduce
the corpora and dictionaries used for spell-checking and training material as
well as the corpora used for obtaining the spelling errors with context. In
Section~\ref{sec:methods}, we outline the implementation of a finite-state
context-aware spell-checker and describe the statistical methods used. In
Section~\ref{sec:evaluation}, we show how the created spelling correctors
improve the results and explain the errors left. In
Section~\ref{sec:future-work}, we compare our work to other current systems and
enumerate possible improvements for both.

\section{Material}
\label{sec:material}

To train the spell-checker lexicons, word-form probabilities can be acquired
from arbitrary running text. By using unigram frequencies, we can assign
all word-forms some initial probabilities in isolation, i.e. with no spell-checking context.
The unigram-trained models we used were acquired from existing 
spell-checker systems~\cite{norvig/2010,pirinen/2010/lrec}, but we briefly
describe the used corpora here as well.

To train the various n-gram models, corpora are required. For the surface-form
training material, it is sufficient to capture running n-grams in the text.
For training the statistical tagger with annotations, we also require 
disambiguated readings. Ideally of course this means hand-annotated
tree banks or similar gold standard corpora. 

For evaluation, two forms of error corpora are needed. One error corpus is based
on actual spelling errors from real sources with corrections
checked by linguists. Another, larger error corpus was generated from Wikipedia
using an automatic error model to generate Damerau-Levenshtein type errors at
roughly the probability of $\frac{1}{33}$ per character of running text. Each
error type has exactly a quarter of this probability\footnote{with an exception for
swapping two adjacent characters at the word-final character}. 

The corpora used are summarized in Table~\ref{table:corpora}. The sizes are
provided to make it possible to reconstruct the systems. In practice, they are the newest
available versions of the respective corpora at the time of testing. In the
table, the first row is the training material used for the finite-state
lexicon, i.e. the extracted surface word-forms without the analyses for unigram
training. The second row is for the analyzed and disambiguated material for the
n-gram based taggers for suggestion improvement. The third line is the corpora
of spelling errors used only for the evaluation of the systems.  As we can see
from the figures of English compared with Finnish, there is a significant
difference in freely available corpora such as Wikipedia. When going further to
lesser resourced languages, the number will drop enough to make such statistical
approaches less useful, e.g. Northern S\'{a}mi in \cite{pirinen/2010/lrec}.


\begin{table}
    \caption{Sizes of training corpora.
    \label{table:corpora}}
  \begin{center}
      \begin{tabular}{lccc}
        \hline
         & Sentences & Tokens & Word-forms \\
        % $\downarrow$Language    &   &        &  \\
        \hline
        \multicolumn{4}{c}{\textbf{English}} \\
        \hline
        Unigrams &  & 2,110,728,338 & 128,457  \\
        % (Wikipedia & & & \\
        % etc.) & & & \\
        N-grams & ? & & \\
        % (WSJ) & & & \\
        Errors (real) & 3,229 & 20,237 & 2,397 \\
        % (real) & & & \\
        \,\,\,\,\,\,\,\,\,\,\,\,\,\,\, (generated) & 9,409 & 1,047,089 & 8656 \\
        \hline
        \multicolumn{4}{c}{\textbf{Finnish}} \\
        \hline
        Unigrams &  & 17,479,297 & 968,996 \\
        % (Wikipedia) & & & \\
        N-grams & ? & & \\
        % (europarl) & & & \\
        Errors (real) & 333 & 4,177 & 2,762 \\
        % (real) & & & \\
        \,\,\,\,\,\,\,\,\,\,\,\,\,\,\, (generated) & 9,640 & 723,526 & 14,983 \\
        \hline
      \end{tabular}
  \end{center}
\end{table}

\subsection{English corpora}

The English dictionary is based on a frequency weighted word-form list of
the English language as proposed in \cite{norvig/2010}. The word-forms were
collected from the English Wiktionary\footnote{\url{http://en.wiktionary.org}},
the English EBooks from the project
Gutenberg\footnote{\url{http://www.gutenberg.org/browse/languages/en}} and the
British National
Corpus\footnote{\url{http://www.kilgarriff.co.uk/bnc-readme.html}}. This
frequency weighted word-list is in effect used as a unigram lexicon for spell-checking.

To train an English morphosyntactic tagger, we use the WSJ corpus. In
this corpus each word is annotated by a single tag that encodes some
morphosyntactic information, such as part-of-speech and inflectional form. The
total number of tags in this corpus is XYZ.

The real-world spelling errors of English were acquired by extracting the ones
with context from the Birkbeck error
corpus\footnote{\url{http://ota.oucs.ox.ac.uk/headers/0643.xml}}. In this
corpus, the errors are from a variety of sources, including errors made by
children and language-learners. For the purpose of this experiment we picked
the subset of errors which had context and also removed the cases of
word joining and splitting to simplify the implementation of parsing and
suggestion.

\subsection{Finnish Corpora}

As the Finnish dictionary, we selected the freely available open source
finite-state implementation of a Finnish morphological
analyser\footnote{\url{http://home.gna.org/omorfi}}. The analyser had the
frequency-weighted word-form list based on Finnish
Wikipedia\footnote{\url{http://download.wikipedia.org/fiwiki/}} making it in
practice an extended unigram lexicon for Finnish. The Finnish morphological
analyser, however, is capable of infinite compounding and derivation, which
makes it a notably different approach to spell checking than the English finite
word-form list. 

The Finnish morphosyntactic n-gram model was trained using the europarl corpus\footnote{xyz}
analysed with fdg\footnote{xyz}. In this format, the annotation is based on a sequence of
tags, encoding part of speech and inflectional form. The total number of
different tag sequences for this annotation is ABCDEF! So  the
statistics for these is likely to be different than for English.

For Finnish spelling errors, we ran the Finnish unigram spell-checker through
Wikipedia, europarl and a corpus of Finnish EBooks from the project
Gutenberg\footnote{\url{http://www.gutenberg.org/browse/languages/fi}} to
acquire the non-word spelling errors, and picked at random the errors having
frequencies in range 1 to 8 instances; a majority of higher frequency non-words
were actually proper nouns or neologisms missing from the dictionary. Using all of
Wikipedia, europarl and Gutenberg provides a reasonable variety of both
contemporary and old texts in a wide range of styles.



\section{Methods}
\label{sec:methods}

In this article we use a finite-state formulation of spell-checking. We
assume the standard notation for finite-state algebra and define the language
model as a weighted finite-state automaton assigning a weight to each correctly
spelled word-form of a language, and an error model automaton mapping a
misspelled string to a set of corrected strings and their weights. The
probabilistic interpretation of the components is such that the weighted fsa as
a language model assigns weight $w(s)$ to word $s$ corresponding to the probability
$P(s)$ for the word to be a correct word in the language. The error model assigns
weight $w(s:r)$ to string pair $s, r$ corresponding to the probability $P(s|r)$ of a
user writing word r when intending to write the word $s$, and the context model
assigns weight $w(s_3 a_3)$ to word $s_3$ with associated POS tagging $a_3$
corresponding to probability $P(s_3 a_3|s_1 a_1, s_2 a_2, s_3 a_3, s_4 a_4, s_5
a_5)$ of the analysis being in a 3-gram context.

In a weighted finite-state system, the probabilistic data needs to be converted
to the algebra supported by the finite-state weight structure.
In this case we use the tropical semi-ring by transforming the
frequencies into penalty weights with the formula $-\log\frac{f}{CS}$, where $f$ is
the frequency and $CS$ the corpus size in number of tokens. If the language
model allows for words that are not in the dictionary, a maximal weight is assigned
to the unseen word forms that may be in the language model but not in the training
corpus, i.e.  any unseen word has a penalty weight of $-\log\frac{1}{CS}$.

The spelling corrections suggested by these unigram lexicon-based spell-checkers
are initially generated by composing an edit-distance automaton~\cite{agata/2002} 
with an  error weight corresponding to the probability of the error
estimated in a corpus, i.e. $-\log\frac{f_{F}}{CS+1}$, where $f_F$ is the frequency
of the misspelling in a corpus. This weight is attached to the edit distance type
error. In practice, this typically still means that the corrections are
initially ordered primarily by the edit distance of the correction, and
secondarily by the unigram frequency of the word-form in the reference corpus.
This order is implicitly encoded in the weighted paths of the resulting
automaton; to list the corrections we use the n-best paths
algorithm~\cite{mohri/2002}. This ordering is also used as our baseline.

For a context-based reordering of the corrections, we use the POS tagging
probabilities for the given suggestions. The implementation of the analysis n-gram
probability estimation is similar to the one described in \cite{silfverberg/2011} with
the following adaptations for the spelling correction. For the suggestion which gives
the highest ranking, the most likely analysis is selected.  The n-gram probability
is estimated separately with each spelling suggestion and then combined with
the baseline probability given by the unigram probability and the edit distance
weight. The ideal scaling for the weights of unigram probabilities, i.e.
edit distance probabilities with respect to n-gram probabilities, was acquired
by performing tests on an automatically generated spelling error corpus.

For example when correcting the misspelling of `an' as `anx' in the sentence ``this
is anx example sentence'', as shown in the figure~\ref{fig:example}, we have
the surface trigrams \{this, is, \_\}, \{is, \_, example\}, \{\_, example,
sentence\}, and corresponding analysis trigrams \{DET, VVBZ, \_\}, \{VVBZ, \_,
NN\}, \{\_, NN, NN\}. The suggestions for anx at edit distance one include
`ax', `an' (one deletion), `ant', `and', `any' (one change) and so on. To rank the
possible suggestions, we substitute $s_3$ for the suggestions, and estimate its
likely analysis. Then we use the ones with the highest combined likelihood of the n-gram
formula $P(s_1:s_5)$ etc.

\begin{table}
\caption{Example trigram combinations\label{fig:example}}
\begin{center}
\begin{tabular}{llcrr}
\hline
this$_{s_1}$ & is$_{s_2}$ & \_$_{s_3}$ & example$_{s_4}$ & sentence$_{s_5}$\\
DET$_{a_1}$ & VVBZ$_{a_2}$ & \_ $_{a_3}$& NN$_{a_4}$ & NN$_{a_5}$\\
\hline
\end{tabular}
\end{center}
\end{table}

The resulting finite-state system consists of three automata, i.e. the dictionary
for spell-checking, the error-model as described in
\cite{pirinen/2010/lrec}, and the new n-gram model automata. The automata
sizes are given in Table~\ref{table:sizes} for reference. The sizes also
give an estimate of the memory usage of the spell-checking system, although the
actual memory-usage during correction will rise depending on the actual
extent of the search space during the correction phase.

\begin{table}
\caption{Automata sizes\label{table:sizes}.}
\begin{center}
\begin{tabular}{lrrr}
    Automaton & States & Transitions & Bytes \\
    \hline
    \multicolumn{4}{c}{\textbf{English}} \\
    \hline
    Dictionary & 25,330 & 42,448 & 1.2 MiB \\
    Error model & 1,303 & 492,232 & 5.9 MiB \\
    N-gram lexicon & 363,053 & 1,253,315 & 42 MiB \\
    N-gram sequences & 46,517 & 200,168 & 4.2 MiB \\
    \hline
    \multicolumn{4}{c}{\textbf{Finnish}} \\
    \hline
    Dictionary & 179,035 & 395,032 & 16 MiB \\
    Error model & 1,863 & 983,227 & 12 MiB \\
    N-gram lexicon & 70,665 & 263,298 & 8.0 MiB \\
    N-gram sequences & 3,325 & 22,418 & 430 KiB \\
    \hline
\end{tabular}
\end{center}
\end{table}

\subsection{English-Specific Finite-State Weighting Methods}

The language model for English was created as described in
\cite{norvig/2010}\footnote{The finite-state formulation of this is informally
described in \url{}}.  It consists of the word-forms and their  probabilities in the
corpora. The edit distance is composed of the standard English alphabet with
an estimated error likelihood of 1 in 1000 words.  Similarly for the English n-gram
material, the initial analyses found in the WSJ corpus were used in the
finite-state tagger as such. The scaling factor between the dictionary probability model and
the edit distance model was acquired by estimating the optimal multipliers using the automatic
misspellings and corrections of a Project Gutenberg Ebook\footnote{\url{}}.

\subsection{Finnish-Specific Finite-State Weighting Methods}

The Finnish language model was based on a readily-available morphological
weighted analyser of Finnish language~\cite{pirinen/2011/nodalida}.  We
further modified the automaton to penalize suggestions with newly created
compounds and derivations by adding a weight greater than the maximum to such suggestions, 
i.e. $-A\log\frac{1}{CS+1}$ in the training material. This has nearly the same effect as using a separate
dictionary for suggestions that excludes the heavily weighted forms without
requiring the extra space.  Also for Finnish, a scaling factor was estimated by using automatic misspellings and corrections
of a Project Gutenberg Ebook\footnote{\url{}}.

In the initial Finnish tagger, there was a relatively large tagset, all of
which did not contain information necessary for the task of
spell-checking, such as discourse particles, which are relatively
context-agnostic~\cite{visk}, so we opted to simplify the tagging in these
cases. Furthermore, the tagger used for training produced heuristic readings for
unrecognized word-forms, which we also removed. Finally, we needed to add
some extra penalties to the word forms unknown to the dictionary in the n-gram model,
since this phenomenon was more frequent and diverse for Finnish than English.

\section{Tests and Evaluation}
\label{sec:evaluation}

The evaluation of the correction suggestion quality is described in
Tables~\ref{table:real-eval}~and~\ref{table:fake-eval}. The
Table~\ref{table:real-eval} contains the precision for the real spelling errors, 
and Table~\ref{table:fake-eval} for the automatically
introduced spelling errors. The precision is measured by ranked suggestions. In
the tables, we give the results separately for ranks 1---4, and then for the cases
that are beyond the used error or langauge model.  In the last column, we have
the cases where correctly written words could not be found with the tested 
suggestion algorithm. The  rows of the table represent different combinations of
the n-gram models. The first row is a baseline score achieved by the unigram
dictionary and the weighted edit distance model alone.

\begin{table*}
    \caption{Precision of suggestion algorithms with real spelling errors.
    \label{table:real-eval}}
  \begin{center}
      \begin{tabular}{lrrrrrrr}
        \hline
        Algorithm & 1 & 2 & 3 & 4 & None & Total \\
        \hline
        \multicolumn{7}{c}{\textbf{English}} \\
        \hline
        Unigrams         (baseline) 
&    ?   &       &      &      &       &     \\
& 28.2 \% & 5.9 \% & 29.4 \% &     \% &     \%  &       \% \\
        \hline
        N-grams 
&     ?  &       &      &      &       &     \\
& 41.2 \% & 50.6 \% & 2.3 \% &     \% &      \% &       \% \\
\hline
        \multicolumn{7}{c}{\textbf{Finnish}} \\
        \hline
        Unigrams          (baseline) 
& 221    & 21    & 13    & 15    & 24    & 322 \\
& 68.6 \% & 6.5 \% & 4.0 \% & 4.7 \% & 7.5 \% & 100.0 \% \\

        \hline
        N-grams
& 230    & 30    & 4    & 11    & 47    & 322 \\
& 71.4 \% & 9.3 \% & 1.2 \% & 3.4 \% & 14.6 \% & 100.0 \% \\
        \hline
      \end{tabular}
  \end{center}
\end{table*}

\begin{table*}
    \caption{Precision of suggestion algorithms with automated spelling errors.
    \label{table:fake-eval}}
  \begin{center}
      \begin{tabular}{lrrrrrr}
        \hline
        Algorithm & 1 & 2 & 3 & 4 & None & Total \\
        \hline
        \multicolumn{7}{c}{\textbf{English}} \\
        \hline
        Unigrams          (baseline)
 &     &       &      &      &        &     \\
 &  \% &    \% &   \% &   \% &    \%  & 100 \% \\
        \hline
        N-grams
&       &      &      &      &       &     \\
&    \% &   \% &   \% &   \% &    \% & 100 \% \\
        \hline
        \multicolumn{7}{c}{\textbf{Finnish}} \\
        \hline
        Unigrams        (baseline)
   &     &       &      &      &       &     \\
 &  \% &    \% &   \% &   \% &    \% & 100 \% \\
        \hline
        N-grams 
&       &      &      &      &       &     \\
 &    \% &   \% &   \% &   \% &    \% & 100 \% \\
        \hline
      \end{tabular}
  \end{center}
\end{table*}

We note that using the hybrid model, we get an improvement regardless
of the language used, whereas the traditional methods of only using either surface 
or POS n-grams result in deterioration for the morphologically complex Finnish material ?????. A reason
for this may be the relative sparseness of n-grams in the Finnish
material. We present a further analysis of both languages in the following 
subsections.

\subsection{English Error-Analysis}

In \cite{norvig/2010}, the authors identify errors that are not solved using simple
unigram weights, such as correcting \emph{rember} to \emph{remember} instead of
\emph{member}. Here, our scaled context-model which can bypass the edit distance
model weight will select the correct suggestion. However, when correcting e.g.
\emph{ment} to \emph{meant} in stead of \emph{went} or \emph{met} the POS
based context reranking gives no help as the POS stays the same.
 
\subsection{Finnish Error-Analysis}

In the Finnish material, the traditional method of using POS n-grams results
in approximately 50 \% good suggestions ???????????. Looking at the examples, this
most commonly happens in nominals where the expected correction in oblique
cases is shadowed by some other correction of the grammatical cases; since word-ordering
of verbal complements and adjuncts is relatively free the model expects that
statistically more common cases are statistically more common in all positions.

\subsection{Performance Evaluation}

We did not work on optimizing the trigram analysis and selection,
but we found that the speed of the system is reasonable---even in its current
form. Table~\ref{table:speed-eval} summarizes the average
speed of performing the experiments in Tables~\ref{table:real-eval}~and~\ref{table:fake-eval}.

\begin{table}
    \caption{The speed of ranking the errors.
    \label{table:speed-eval}}
  \begin{center}
      \begin{tabular}{lcccc}
        \hline
        Material  & English &  & Finnish &  \\
        Algorithm & real    &  automatic         & real    &  automatic         \\
        \hline
        Unigram (baseline)   &    10.0 s &      s & 51.8 s &     s  \\
         & 399.1 wps &    wps & 6.2 wps &   wps  \\
        \hline
        POS n-grams   &   377.4 s &     s & 1616.2 s &     s  \\
          & 10.6  wps &   wps & 0.14 wps &   wps  \\
        \hline
      \end{tabular}
  \end{center}
\end{table}


\section{Future Work and Discussion}
\label{sec:future-work}

We have shown that the combined n-gram models are suitable for improving the
spelling corrections for both morphologically more complex languages such as
Finnish and for further improving languages with simpler morphologies like English. To
further verify the suitability of the method, it still needs to be tested on
a typologically wider spectrum of languages.

In this article, we used readily available and hand-made error corpora to test
our error correction method. A similar method as the one we use for error correction 
should be possible in error detection as well, especially when detecting real-word
errors~\cite{mays/1991}. In future research, an obvious development is to
integrate the n-gram system as a part of a real spell-checker system for both
detection and correction of spelling errors, as is already done for the
unigram based spell checker demonstrated in~\cite{pirinen/2010/lrec}.

\section{Conclusion}

In this paper we have demonstrated the use of finite-state methods for trigram based
generation of spelling suggestions. We have shown that the basic trigram
methods suggested for languages like English are not
useful without modification for morphologically more complex languages like
Finnish.  Instead a more elaborate n-gram scheme combining surface forms and 
POS n-grams is successful for Finnish as well as English.

%\section*{Acknowledgements}

% We are grateful to Sam Hardwick for making the spell checking software avialable and the
% HFST research group for fruitful discussions.

\bibliographystyle{splncs}
\bibliography{cicling2011}


\end{document}
