\documentclass[a4paper,conference]{IEEEtran}

\IEEEoverridecommandlockouts
%\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[cmex10]{amsmath}
\usepackage{balance}
\usepackage{subfig}
\usepackage {graphicx}

\title{Improving Predictive Entry of Text Messages using IRC Logs}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockA{\ldots\\
\ldots\\
\ldots\\
\ldots}
\and
\IEEEauthorblockA{\ldots\\
\ldots\\
\ldots\\
\ldots}
\and
\IEEEauthorblockA{\ldots\\
\ldots\\
\ldots\\
\ldots}
}



\begin{document}



%\IEEEspecialpapernotice{(Authors' pre-print draft version)}
\maketitle


\begin{abstract}

\end{abstract}

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{M}{obile} phone text messages are a hugely popular way
of communication, but mobile phones are not especially well suited for
inputing text because of their small size and often limited
keyboard. There are several technological solutions for inputing text
on mobile phones and other limited keyboard devices. This paper is
concerned with so called predictive text entry, which utilizes
redundancy in natural language in order to enable efficient text entry
using limited keyboards (typically including 12 keys).

There has been a lot of research into improving predictive text entry
e.g. (...), but the research has mainly been concerned with improving
the statistical model, or other technical aspects of the text entry
algorithm. In this paper, we investigate the role of choice of training
data for the accuracy of the text entry system. 

There has been work on improving text entry systems by training them
on actual text messages, e.g. (...). Since text messages are difficult
to come by and there are legal restrictions for using them,
open-source text entry systems require alternative sources of training
data, which yield good accuracy. In this paper, we use Internet Relay
Chat (IRC) logs, to train a predictive text input system for Finnish,
and show that this gives significant improvement compared to a
baseline system, which is trained using data from the Finnish
Wikipedia. 

To the best of our knowledge, there have not been earlier inquiries
into using IRC logs for training predictive text entry systems. IRC
log material is nevertheless very well suited for the task, since
there is a lot of material available in different languages. Like text
messages, it resembles spoken language and consists of short messages.

We evaluate our system against the predictive text entry in a number
of widely available mobile phones (...) and show that we get
comparable (better?, only slightly worse?) results. This demonstrates
that it is possible to construct an accurate predictive text entry
system without resorting to actual text message data. Even
optimization of the accuracy of the system can be accomplished without
using actual text message data.

Because Finnish is a morphologically complex language, our system uses
a morphological analyzer for Finnish Omorfi (...). The word forms
found in Omorfi are given probabilities according to their frequency
in the Finnish Wikipedia. These probabilities are combined using
similar probabilitites computed from IRC logs and the final
probability given for a word form is a combination of the probabilities
given by Omorfi and the IRC log model.

Since Omorfi is implemented as a weighted finite-state transducer, we
implemented our predictive text entry system in the weighted
finite-state framework. We used a freely available open-source C++
interface for constructing and utilizing weighted finite-state transducers,
Hfst (...).

This paper is organized as follows: In section \ref{sec:related-work},
we present earlier work in improving the accuracy of predictive text
entry systems. In section \ref{sec:methods}, we explain how to augment
a morphological analyzer with word frequencies computed from IRC logs
and how such a system is used to disambiguate between words
corresponding to an ambiguous input sequence. After this we present
the morphological analyzer, Omorfi, and the Hfst interface in section
\ref{sec:tools} and present the IRC log data used for training out model
and the text message data used for evaluation in \ref{sec:data}. We
evaluate our system in section \ref{sec:evaluation} and present some
general and closing remarks in sections \ref{sec:discussion} and
\ref{sec:conclusions}.

% - Demonstrate the relevance of the research problem.
%   * Predictive text entry works poorly, because it doesn't adequately take 
%     into account the difference between general written language and text 
%     messages.
%   * We need more realistic training data.
%   * Genuine text messages are hard to come by.
% - Instead of text messages, we use IRC logs, which can easily be harvested
%   from the internet (from the public domain?) and which ressemble text 
%   messages.
% - We claim that using IRC Logs and a dictionary we can achieve significant 
%   improvement compared to using only a morphological dictionary
% - We demonstrate that IRC logs can be used to both train and weight a 
%   predictive text entry system for text messages withput resorting to 
%   text message data even for adjusting the weights of component models.
%   (something like that...) 
% - To the best of our knowledge there have been no previous published
%   using IRC logs to train predictive text entry systems.

\section{Related Work}
\label{sec:related-work}

\begin{itemize}
\item Using genre or domain-specific text to train a NLP system is not
  a new idea.The approach has been tested in e.g. automatic
  translation (...) and tagging (...).
\item \cite{Harbusch/2003} Investigate the usefulness of a
  domain-specific lexical model in predictive text entry. They
  established that it is difficult to assemble a good enough purely
  domain-specific lexicon. According to them, the best approach is
  therefore to use a combination of a high coverage general lexicon
  and a domain-specific model, which is exactly what we have done.
\item \cite{Harbusch/2003} are concerned with building systems for very
  specific domains such as school exercises ad scientific texts. They
  do not really address the question of what would constitute
  practical training material for a general text message system. We
  are expressly interested in improving text entry using widely
  available materials.
\end{itemize}

\section{Combining a Lexicon and IRC Logs}
\label{sec:methods}

The weighted language model in our finite-state language mode is based
on morphological dictionary of the language created in traditional
finite-state morphology framework\cite{beesley/2003}. The weights in
this system will be basic scaled unigram weights transformed in
finite-state form by $w = -\log \frac{f}{cs}$ where $w$ is the weight,
$f$ is the frequency of token and $cs$ the size of corpus in
tokens. For tokens that do not exist maximum weight of $w_{max} =
-\log \frac{1}{cs+1}$ is used.  The probabilities are gained by
extracting the word frequencies from a large scale corpus such as
wikipedia~\cite{pirinen/2010/lrec}. The basic unigram weighting system
can be fine-tuned, especially on part of unknown words, by composing
additional weights based on e.g. morphological complexity, as
suggested in \cite{karlsson/1992}, which can be easily modeled in
weighted finite-state form as suggested e.g. in\cite{schiller/2005}

The probability weightins systems need to be scaled in combination to match and
\ldots
% Tähän kai pitää vielä tarkentaa mitä on tehty omorfin + irkkilokipainojen
% skaalaukseen ja mitä se tarkotaa


\section{Tools and Resources}
\label{sec:tools}

To implement the full system we have used only freely available open source
tools and resources that anyone can download from the Internet to reproduce the
results. For the finite-state system we have selected the HFST
tools\footnote{\url{http://hfst.sf.net}}, which is relatively complete
reproduction of the classical tools of finite-state
morphology\cite{beesley/2003}. Similarly we have downloaded a freely available
Finnish finite-state morphological analyser
omorfi\footnote{\url{http://home.gna.org/omorfi/}} for our language model
data\cite{pirinen/2011/nodalida}. For further training of the language model we
have used the Finnish
Wikipedia\footnote{\url{http://dumps.wikimedia.org/fiwiki/latest/}} as corpus
to acquire the unigram probabilities\cite{pirinen/2010/lrec}.

\subsection{Tools}

In our setup we use unmodified version of HFST tools for creation, manipulation
and use of finite state transducers\cite{hfst/2011}. This means that we have
not needed to prepare any specialised algorithms for application of the t9
predictive input model. This basically means that it can be used on any of
current or future finite-state systems as long as they implement needed subset
of finite-state algebra as defined in \ref{sec:methods}.

For processing of corpora we have used standard GNU tools like coreutils as
well as bash and python scripting languages. The source code of the full system
used for building, applying and evaluation of the full system is available
under free
licence\footnote{\url{http://hfst.svn.sourceforge.net/viewvc/hfst/trunk/cla-2011-article/}}.

\subsection{Data}
\label{sec:data}

For language model we have selected to use ready-made free open source
finite-state implementation of Finnish language\cite{pirinen/2011/nodalida}.
The language model here is meant for parsing running text consisting mainly of
standard written Finnish language, which is relatively far from the language
sms text messages.  To train the language model we have used
the Finnish Wikipedia data to add unigram probabilities to word forms that are
recognised by the language model we are using. 

The text message data used for evaluation was collected from students of
university blah \ldots

\section{Evaluation}
\label{sec:evaluation}

\begin{figure*}[!t]
\centerline{\subfloat[The first guess is correct.]{\includegraphics[width=3.5in]
  {accuracy_first_guess.pdf}}
\hfil
\subfloat[Correct guess found among the three first guesses.]{\includegraphics[width=3.5in]
  {accuracy_first_to_third_guess.pdf}}}
\caption{The accuracy of our algorithm using different linear combinations of the weights given by the morphological analyzer and the IRC log transducer. On the x-axis we give the relative weight of the IRC log transducer and on the y-axis we give the accuracy. The dotted line gives the accuracy for the text message test material and the solid line gives the accuracy for the IRC log test material.}
\label{AccuraciesIRCMaterial}
\end{figure*}

\section{Discussion and Future Work}
\label{sec:discussion}

This work demonstrates that using simple methods we can improve a lot.\ldots

In this paper we used readily available language model geared towards parsing
running texts of literary written Finnish. It would be interesting to see if
modifying the language model towards the standard spoken Finnish dialect
might yield a significant increase in recall.

The demonstrated model only considers unigrams for probability
likelihood maximisation. It has been demonstrated that it is possible
to extend these models trivially to arbitrary n-gram and varigram
models \cite{Silfverberg/2011}, and this should be explored to see if
there's improvement for wider statistical models in task of predictive
text input for mobile text messages.

Finally we have only performed our experiment for Finnish, but we believe that
same methods will apply to at least most of the latin abjad based languages
with no modifications excepting the orthographic conventions---such as accent
dropping---on some texting cultures.

\section{Conclusion}
\label{sec:conclusions}

\ldots \balance
\section*{Acknowledgment}
Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.

% We the people thank the HFST research team and the staff of University of
% Helsinki for fruity discussions as well as the anonymous reviewers for
% good suggestions.
\
\bibliographystyle{IEEEtran}
\bibliography{cla2011}




% that's all folks
\end{document}


