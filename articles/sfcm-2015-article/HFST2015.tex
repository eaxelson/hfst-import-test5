\documentclass{llncs}

\usepackage{llncsdoc}

%% PDFLaTeX
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{textcomp}      % for ° symbol
\usepackage{multirow}
\usepackage{caption}
\usepackage{url}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{mathptmx}
\usepackage{latexsym}
\usetikzlibrary{automata,positioning}
\usepackage{framed}
\usepackage{gb4e}

%% XeLaTeX
% \usepackage{fontspec}
% \usepackage{xunicode}
% \usepackage{xltxtra}

\usepackage{expex}

%
\begin{document}
%
\title{Semantic tagging using HFST}
%
\author{N.N. and N.N.}

 \institute{xx\\
 yy\\
 zz\\
 ww\\
 \email{\{n.n., n.n.\}@xxx.yy}}

\maketitle

% Removed the manual bibstyle in favor of splncs03.bst,
% if it's required fetch it from svn history

\begin{abstract}
%Krister
\end{abstract}

\section*{Introduction}
% Krister (1 p.)

\section{Tokenization using {\tt hfst-pmatch}}\label{sec:tokenization}
% Sam (4 p.)
Tokenization is a necessary first step in most text-based NLP tasks. For some
languages (eg. English) it is often considered to be a mechanical
preprocessing task without linguistic importance, and for others (eg. Chinese)
it is a subtle task given a different name (``segmentation'').

However, even in languages that generally insert spaces between words, there
are issues that influence the quality or feasibility of tools down the
pipeline. We may, for example, want to be able to identify multiword units,
identify compound words and mark their internal boundaries, control various
dimensions of normalisation, or produce possible part-of-speech tags or
deeper morphological analyses.

We describe a general approach to these issues based on morphological
transducers, regular expressions and the pattern matching operation
\verb+pmatch+~\cite{karttunen/2011}.

\subsection{A Short Introduction to {\tt pmatch}}

\verb+pmatch+~\cite{hfst-pmatch} is a pattern-matching operation for text based
on regular expressions. It is a HFST development of the original ideas in Xerox
\verb+fst+. The regular expressions (or ``rules'') are named, and are
invoked ultimately by a root expression (the ``top level'', which by convention
has the name ``TOP''). Expressions may be refer to themselves or each other
circularly by way of special arcs which are interpreted at runtime, allowing
context-free grammars to be expressed.

Matching operates in a loop, accepting the largest
possible amount of input from the current position, possibly modifying it
according to the rules and tagging left and right boundaries of sub-rules, and
continuing on the the next position in the input.

When the rules successfully accept (and possibly transform) some length of
input, that is a ``match''.
When the match has triggered the operation of a tagging directive
(\verb+EndTag()+ or \verb+[].t()+), the enclosed length of input is ``tagged'' with
``tags''. For example, here is a very naïve tokenizer for English.

\begin{verbatim}
define TOP [[ ("'") Alpha+ ] | Sigma({,.;!?})]] EndTag(w);
! If you want apostrophe-joined elisions as single tokens, use
! [Alpha | "'"]+
\end{verbatim}

\verb+Sigma()+ is a function that extracts the alphabet of its argument, here
some punctuation given as a string (which is the effect of the curly braces).
When operated on the sentence
``If I am out of my mind, it's all right with me, thought Moses Herzog.'',
it produces output that looks like this:

\begin{verbatim}
<w>If</w> <w>I</w> <w>am</w> <w>out</w> <w>of</w> <w>my</w> <w>mind</w>
<w>,</w> <w>it</w> <w>'s</w> <w>all</w> <w>right</w> <w>with</w>
<w>me</w><w>,</w> <w>thought</w> <w>Moses</w> <w>Herzog</w><w>.</w>
\end{verbatim}

in normal, ``matching'' mode. The runtime operation of matching can be
controlled to only output the matched parts, or give positions and
lengths of tagged parts (``locate mode''). Matches may also be extracted
via an API call to control the flow of data more precisely. The above example
operating as a more conventional tokenizer (outputting one token per line and
omitting everything else) could be written as

\begin{verbatim}
define TOP [[Alpha | "'"]+ | Sigma({,.;!?})]] 0:"\n";
\end{verbatim}

and run in \verb+extract-matches+ mode.

\subsection{Tokenizing with a Dictionary}

A tokenizer consisting of the input side of a morphological dictionary
with good coverage in vocabulary and derivation can satisfactorily solve
many tokenization headaches on its own. For example, consider the compound
plural possessive in

\begin{exe}
  \item The Attorney-Generals' biographies are over there.
\end{exe}

To get tokenization exactly right, a tokenization rule needs to understand that
the hyphen is joining parts of a compound word (unlike in eg.
``Borg-McEnroe'') and that the apostrophe is indicating the possessive form,
not the end of a quotation.

A dictionary can also be augmented to recover from
formatting or digitalisation issues. For example, a text may split words
at line boundaries with hyphens, as in

\begin{exe}
\item He seemed suddenly to have been endowed with super-

  human strength
\end{exe}

Here the only correct tokenization is ``superhuman'' rather than ``super''
and ``human'', but a dictionary would miss this possibility. However,
we can use a finite-state operation to allow the string \verb+-\n+ (hyphen
followed by a newline) to appear anywhere inside the words in the dictionary.
In regular expressions this operation is sometimes called ``ignoring'' and in
\verb+pmatch+ is invoked with a forwards slash, like so:

\begin{center}
  \begin{framed}
\begin{verbatim}
    define dict_with_linebreaks [dict]/[{-}"\n"]
\end{verbatim}
  \end{framed}
\end{center}

The \verb+\n+ is in double quotes in order to invoke parsing it as a newline
rather than as a literal ``\textbackslash n''.

\subsubsection{Preserving the Parts of a Multiword Unit}

Dictionaries are often equipped with a collection of short idioms (eg.\@
``in view of'') and other tokens which include whitespace (eg.\@
``New York''). While these are useful, it may be too early at this stage
to fix the tokenization as the longest possible match. A discriminative
tagger won't be able to make the correct choice in

\begin{exe}
  \item The ball was in view of the referee.
\end{exe}

if it only sees a tokenization where \emph{in view of} is a single token.

We can extend the dictionary in a simple way to also contain the other
possible tokenizations (and, in the case of a morphological dictionary,
the analyses) as follows.

\begin{verb}
define combined_tokenizations [dict].u .o. [dict | [" " dict]*]  
\end{verb}

Here, \verb+dict+ is our dictionary and \verb+[dict].u+ is its input
projection. We compose it with arbitrarily many copies of itself,
interpolated with space characters. The result contains every multiword
expression both as itself, and as a combination of other words found
in the dictionary.

\subsection{Tokenization Rules as an OOV Fallback}

When no match from the dictionary (or a morphological guesser) is possible,
\verb+pmatch+ by default
writes input to output as-is. Ideally, we'd like to assign every character
in input either to some intentional token, or discard it (this concerns mainly
whitespace between tokens). To achieve this, we must write a rule representing
``some other word'' and disjunct it with the dictionary at the top level of
our tokenizer. To discard unwanted input, we must either adopt the convention
(and use an appropriate operating mode) that parts of input that haven't been
given any tag are discarded, or explicitly add a rule that rewrites single
unmatched characters to empty strings. This abbreviated example for English
employs some knowledge about intra-word punctuation.

\begin{framed}
\begin{verbatim}
define special_plurals [abbreviation | numeric_expression] {'s};
define possessive_s_ending
 OR(LC({s}) LC({se}) LC({z}) LC({ze}) LC({ce}) LC({x}) LC({xe}))
 ! at runtime check for these strings on the left side
 {'} ({s});
 ! then allow an apostrophe and possibly an s 
define possessive_suff {'s} | possessive_s_suff;
define OOVwordpart [Alpha+ (possessive_suff)] | special_plurals;
! assume contractions are all in the dictionary
define OOVword [OOVwordpart [{-} OOVwordpart]*]::1.0;
define TOP dict | OOVword;
\end{verbatim}
\end{framed}

Here we join possessive endings and hyphen-joined compound words at the top
level into single matches, but by either adding tags at the appropriate levels
or by changing what is included at the top level it is easy to control the
level of tokenization.

\verb+hfst-pmatch+ expressions are weighted, and the \verb+::1.0+ at the end
of the \verb+OOVword+ rule ensures that dictionary matches, which have
weight zero, will be preferred over \verb+OOVword+ matches.

\subsection{Analysis Cohorts}

In addition to bare tokens, many downstream tools use analysis cohorts, ie.\@
the full set of possible baseforms and morphological tags for the token in
question. \verb+hfst-pmatch+ exposes an API that allows retrieval of the
position, length, input, output, tag and weight of each longest match (when
there are many equally long ones). To go directly from running text to, say,
Constraint Grammar cohorts (possibly via some intermediate processing) is a
matter of string interpolation of the fields retrieved from \verb+pmatch+.

\subsection{Chunking}

Since in \verb+pmatch+ multiple rules operate on the same input, it is possible
to integrate higher-level tokenization, such as grouping tokens into sentences
and sentences into paragraphs in the same ruleset.

(example if deemed useful, otherwise remove this section)

\subsection{Incorporating other Linguistic Units}

time expressions? numeric quantities? money? (or remove section?)

\section{Morphological Tagging using {\tt hfst-finnpos}}\label{sec:morph-tagging}
% Miikka (4 p.)

FinnPos \cite{silfverberg/2015} is a data driven {\it morphological
  tagging} toolkit distributed with the HFST interface. The term
morphological tagging \cite{chrupala/2008} refers to assigning one full
morphological label (including for example part-of-speech, tense, case
and number) for each word in a text. It can be contrasted with POS
tagging where the task is to infer the only the correct part-of-speech
of each word.

The FinnPos toolkit is based on the Conditional Random Field framework
\cite{lafferty/2001}. Most work on CRF taggers and other discriminative
taggers has concentrated on POS tagging for English, which has a very
limited selection of productive morphological phenomena. In contrast,
FinnPos is especially geared toward morphologically rich languages
with large label sets, which cause data sparsity and slow down
estimation when using standard solutions. FinnPos gives
state-of-the-art results for the morphologically rich language Finnish
\cite{silfverberg/2015} both with regard to runtime and accuracy.

In addition to morphological tagging, FinnPos also performs data
driven lemmatization. Moreover, it can be combined with a
morphological analyzer to make a data-driven morphological
disambiguator.

In this section, we will describe the FinnPos tagger from a
practical point of view.

\subsection{FinnPos for Morphologically Rich Languages}

Large label sets commonly used in tagging of morphologically rich
languages are a source of data sparsity. Label set sizes of around
1,000 label types frequently occur. For a second order morphological
tagger, this means that an overwhelming majority of the one billion
possible (1,000$^3$) label trigrams are never seen in a training
corpus of realistic scope.

Although morpholgical label sets can be very large, they are usually
structured. A typical example of a structured morphological label is
the label {\tt Noun|Sg|Nom}, where the main word class is {\tt Noun},
the number is singular ({\tt Sg}) and the case is nominative ({\tt
  Nom}). FinnPos utilizes the internal structure of large label sets
by extracting features for sub units of labels as well as for the
entire labels \cite{silfverberg/2014}. This alleviates the data
sparsity problem because features relating to sub units of entire tags
can be used as fallback. Additionally, we can now model grammatical
phenomena such as congurence in case which can occur in a variety of
word classes.

In addition to data sparsity, training time is a problem because the
complexity of standard CRF training of an $n$th order model depends on
the $(n+1)$st power of the label set size. To speed up training,
FinnPos uses an adaptive beam search and a label guesser
\cite{silfverberg/2015} during inference and estimation. These reduce
run time to fraction in practice.

\subsection{FinnPos Tools}

The FinnPos toolkit includes three commandline tools

\begin{itemize}
\item {\tt finnpos-train} for training tagger models.
\item {\tt finnpos-label} for using models to label data.
\item {\tt finnpos-eval} for evaluation against a gold standard.
\end{itemize}

Additionally, FinnPos provides a Python script {\tt
  finnpos-ratna-feat.py} used for feature extraction \footnote{so
  named after Adwait Ratnaparkhi because FinnPos uses a modified version of
  the feature set introduced by Ratnaparkhi in
  \cite{ratnaparkhi/1996}}.

FinnPos can utilize HFST morphological analyzers and other
morphological analyzers. The integration of a morphological analyzer
is accomplished by piping the output of the analyzer to FinnPos. The
tags emitted by particular morphological analyzers may require post
editing because FinnPos uses the '\verb@|@' sign to separate sub units
of structured tags (for example in {\tt Noun\|Sg\|Nom}). For HFST
morphological analyzers, editing can be accomplished using the utility
{\tt hfst-xfst}.

\subsection{Data Format and Feature Extraction}

\begin{figure}
\begin{framed}
\begin{verbatim}
The    WORD=The LC_WORD=the       the   Det                _
dog    WORD=dog LC_WORD=dog       dog   Noun|Sg|Nom        _
barks  WORD=barks LC_WORD=barks   bark  Verb|Ind|Pres|3sg  _
.      WORD=. LC_WORD=.           .     .                  _

Their  WORD=Their LC_WORD=their   they  Pron|Pl|Gen        _
cat    WORD=cat LC_WORD=cat       cat   Noun|Sg|Nom        _
meows  WORD=meows LC_WORD=meows   meow  Verb|Ind|Pres|3sg  _
.      WORD=. LC_WORD=.           .     .                  _
\end{verbatim}
\end{framed}
\caption{Small example of data format.}
\end{figure}

The utilities {\tt finnpos-train} and {\tt finnpos-label} read and write input
sentences in a five column tab-separated format where each row
corresponds to one input word and the columns denote

\begin{enumerate}
  \item  Word form (e.g. ``Dogs'').
  \item  Features separated by spaces (e.g. \verb|WORD=dogs PREV_WORD=the|).
  \item  lemma (e.g. ``dog'').
  \item  Label (e.g. \verb|NNS|).
  \item  Annotations (arbitrary text not containing tabulators).
\end{enumerate}

When using a file as training or development file for {\tt finnpos-train}, the lemma and label fields have to contain exactly one value each.

If the label field is non-empty in the input for {\tt finnpos-label}, the tagger will disambiguate between the candidates provided.

The default feature extraction script {\tt finnpos-ratna-feats.py}
extracts the following features for frequent words:

\begin{enumerate}
\item Word form.
\item Previous word form.
\item Next word form.
\item Previous two words.
\item Next two words.
\end{enumerate}

For rare words, the script also extracts orthographic features

\begin{enumerate}
\item Suffixes and prefixes up to length 10.
\item Capitalization.
\item Whether the word includes digits or dashes.
\end{enumerate}

Users may add their own features (such as the output of a
morphological analyzer) before calling {\tt finnpos-ratna-feats.py} or
write their own feature extraction script.

The fifth field in the data format is reserved for annotations. These
are passed unchanged through the tool {\tt finnpos-label}. The
annotation field can be used to transport lemmatization information
through the tagger. It can also be used for e.g. named entity labels.

\subsection{Training and Using a Model}

FinnPos uses an early stopping averaged perceptron algorithm for
estimation of model parameters. The error-driven perceptron training
algorithm iterates through the training corpus one sentence at a time,
labels the sentence and adjusts model weights when erroneous labels
are detected. Usually the Viterbi algorithm \cite{collins/2002} is
used for labeling. This however, is too slow in practice when dealing
with large label sets.

Instead of the Viterbi algorithm, FinnPos uses a beam search with
adaptive beam width \cite{pal/2006}. Additionally FinnPos uses a generative
label guesser (modeled after the OOV word model used in
\cite{brants/2000}) to restrict label candidates during
training. Because of inexact inference during the training phase,
FinnPos additionally uses violation fixing \cite{huang/2012}.

Users can train their own models using the utility {\tt
  finnpos-train}. The parameters of the training process are set using
a configuation file (see Figure \ref{fig:config-file}). The
configuration file is used to set the hyper parameters of the beam
search and label guesser as well as the stopping conditions for the
perceptron algorithm. See figure \ref{fig:config-fields}.

\begin{figure}
\begin{framed}
\begin{verbatim}
# Config file for FinnTreeBank tagger.

guess_mass=0.999
beam_mass=0.999
max_train_passes=3
max_lemmatizer_passes=7
\end{verbatim}
\end{framed}
\caption{Example configuration file.}\label{fig:config-file}
\end{figure}

\begin{figure}
\begin{tabular}{ll}
{\tt suffix\_length} & The maximal suffix length used in feature extraction during\\
                     & lemmatization.\\
{\tt degree} & The degree of structured features used by the tagger. \\
{\tt max\_train\_passes} & The maximal number of training passes during estimation of tagger\\
                         & parameters.\\
{\tt max\_lemmatizer\_passes} & The maximal number of training passes during estimation of\\
                              & lemmatizer parameters.\\
{\tt max\_useless\_passes} & Determined the maximal number of passes over the training data\\
                           & that do not improve the tagging accuracy for the development data.\\
{\tt guess\_mass} & A generative label guesser is used to prune the label candidates\\
                  & considered for each word during training. {\tt guess\_mass} is a float\\
                  & in range (0, 1.0) which detemines the mass \\
                  &  preserved by the guesser for each word.\\
{\tt beam\_mass} & FinnPos uses an adaptive beam to prune search histories during\\
                 & beam search. This parameter determines the probability mass of the\\
                 & beam.
\end{tabular}
\caption{Description of configuration file fields.}\label{fig:config-fields}
\end{figure}

\subsection{FinnPos and Morphological Analyzers}

FinnPos can be used for morphological disambiguation together with a
morphological analyzer. The analyzer can be used in two ways: to
provide label candidate for words and as a generator of features.

For words not recognized by the analyzer, FinnPos will use a built-in
suffix-based guesser.

FinnPos does not depend on HFST for morphological analysis. Any
morphological analyzer may be used, because morphological
disambiguation is accomplished using a pipeline of tools.

In addition to the morpological tag, FinnPos also uses the
morphological analyzer for determining the lemma of a given word. For
words not recognized by the analyzer, a data-driven lemmatizer is used
instead.

\section{Semantic Tagging using {\tt hfst-pmatch}}\label{sec:sem-tagging}
% Sam (4 p.)

This section contains material intended for presentation in the
demonstration session of NODALIDA 2015.

\subsection{Introduction}

In this section we outline a scheme for extracting semantic frames from text
using hand-written rules. While the rules presented here have actual
implementations, this does not represent a full-fledged system
for extracting a large number of different frames -- rather, our motivation is
to give a description of \verb+hfst-pmatch+ as a NLP system in context.

A semantic frame \cite{semantic-frame} is a description of a \emph{type} of event, relation or entity
and related participants. For example, in FrameNet,
a database of semantic frames,
the description of an \verb+Entity+ in terms of physical space occupied by it is
an instance of the semantic frame \verb+Size+. The frame is evoked by
a lexical unit (LU), also known as a frame evoking element (FEE), which is a
word (in this case an adjective)
such as ``big'' or ``tiny'', descriptive of the size of the \verb+Entity+.
Apart from \verb+Entity+, which is a core or compulsory element, the
frame may identify a \verb+Degree+ to which the \verb+Entity+ deviates
from the norm (``a \textbf{really} big dog'') and a \verb+Standard+ to
which it is compared (``tall \textbf{for a jockey}'').

  \begin{center}
\begin{table}[h]
  \begin{tabular}{ | l | l |}
\hline
Lexical Unit (LU) & Adjective describing magnitude (large, tiny, ...) \\
\hline
Entity (E) & That which is being described (house, debt, ...) \\
\hline
Degree (D), optional & Intensity or extent of description
(really, quite, ...) \\
\hline
Standard (S), optional & A point of comparison (for a jockey, ...) \\
\hline
    \end{tabular}
    \caption{The semantic frame \emph{Size}.}
\end{table}
      \end{center}

For example:

\begin{table}[h]
\begin{center}
\begin{math}
\Big[_{\text{Size}}\Big[_{\text{E}}\text{He} \Big]
  \text{is} \Big[_{\text{D}} \text{quite} \Big] \Big[_{\text{LU}}\text{tall} \Big]
  \Big[_{\text{S}} \text{for a jockey} \Big] \Big]
\end{math}
\end{center}
\caption{A tagged example of \emph{Size}}
\end{table}

\subsection{A Rule}

A simple and common syntactic realisation of the \verb+Size+ frame is a single
noun phrase containing one of the LUs, such as
``the big brown dog that ran away''. Here we'd like to identify ``big'' as \verb+LU+,
``brown dog'' as \verb+Entity+ and the combination as \verb+Size+.
Our first rule for identifying this type of construction might be

\begin{table}[h]
  \small
  \begin{framed}
\begin{verbatim}
define LU {small} | {large} | {big} EndTag(LU);
define Size1 LU (Adjective) [Noun EndTag(Entity)].t(Entity);
define TOP Size1 EndTag(Size);  
\end{verbatim}
\end{framed}
\normalsize
\caption{A simplified first rule}
\end{table}

This ruleset has been simplified for brevity -- it has only a few of the
permitted LUs, and word boundary issues have not been addressed.

The \verb+[].t()+ syntax in the definition of \verb+Size1+ is a tag delimiter
controlling the are tagged as \verb+Entity+. The extra \verb+Adjective+ is
optional, which is conveyed by the surrounding parentheses.

We can verify that our rules extract instances of our desired pattern by compiling
them with \verb+hfst-pmatch2fst+ and running the compiled result with
\verb+hfst-pmatch --extract-tags+. In the following we have
inputted the text of the King James Bible from Project
Gutenberg (\url{gutenberg.org}) and added some extra characters on both
sides for
a concordance-like effect:

%\begin{table}[h]
\hfill \break
  \small
  \begin{framed}
\begin{verbatim}
...
there lay a <Size><LU>small</LU>
round <Entity>thing</Entity></Size>
...
there was a <Size><LU>great</LU>
<Entity>cry</Entity></Size> in Egypt
...
saw that <Size><LU>great</LU>
<Entity>work</Entity></Size> which
...
\end{verbatim}
\end{framed}
\normalsize
%\caption{Fragments of tagged running text}
%  \label{bibletext}
%\end{table}

A natural next step is to add optional non-core elements, such as an adverb
preceding the LU being tagged as \verb+Degree+ and a noun phrase beginning with
``for a'' following it as \verb+Standard+.

\begin{table}[h]
  \small
  \begin{framed}
\begin{verbatim}
define Size1 [Adverb].t(Degree) LU (Adjective)
  [Noun].t(Entity) [{for a} NP].t(Standard);
\end{verbatim}
\end{framed}
\normalsize
\caption{Extending the rule with optional elements}
\end{table}

Here are some examples this rule finds in the British National
Corpus~\cite{bnc}.

%\begin{table}[h]
  \small
  \begin{framed}
\begin{verbatim}
...
presence of an <Size>
  <Degree>arbitrarily</Degree>
  <LU>small</LU> <Entity>
  amount</Entity></Size> of dust
...
one <Size><LU>small</LU>
  <Entity>step</Entity>
  <Standard>for a man</Standard>
  </Size>
...
\end{verbatim}
  \end{framed}
  \normalsize
%  \caption{Tagged text with optional elements}
%  \label{bnctext}
%  \end{table}

We can see that in ``small amount of dust'' we might want to
tag not just the immediate noun as \verb+Entity+ but the entire noun phrase
(which could be implemented up to a context-free definition of a noun phrase),
and in ``one small step for a man'' a common indirect use of the \verb+Standard+
construction.

As well as correct matches, such as ``small round thing'' in the biblical
example, we have metaphorical meanings of \verb+Size+, such as ``great cry''.
This may or may not be desired -- perhaps we wish to do further processing to
identify the target domains of such metaphors, or perhaps we wish to be able
to annotate physical size and physical size only.

\subsection{Incorporating Semantic Information}

Size is a very metaphorical concept, and syntactic rules as above will produce a large amount of matches that relate to such uses, eg. ``a great cry'' or ``a big deal''. If we wish to refine our rules to detect such uses, there are a few avenues for refinement.

First of all, some LUs are much more metaphorical than others. A ``great man'' is almost certainly a metaphorical use, whereas a ``large man'' is almost certainly concrete. Accuracy may be improved by requiring ``great'' to be used together with a common concrete complement, like ``great crowd''.

There are also semantic classifications of words, such as WordNet~\cite{wordnet}. We may compile the set of hyponyms of \emph{physical entity} and require them to appear as the nouns in our rules.

\begin{table}[h]
\small
\begin{framed}
\begin{verbatim}
define phys_entity
  @txt"phys_entity.txt";
! a list of singular baseforms can be expanded to include
! eg. plurals by suitably composing it with a dictionary automaton
\end{verbatim}
\end{framed}
\normalsize
\caption{Reading an external linguistic resource}
\end{table}

\subsection{Incorporating Part-of-speech Information}

We have hitherto used named rules for matching word classes, like \verb+Noun+,
without specifying how they are written. Even our collection of LUs might need
some closer attention -- for example ``little'' could be an adverb.

Considering that in writing our
rules we are effectively doing shallow syntactic parsing, even a very simple
way to identify parts of speech may suffice: a morphological dictionary.
For example, a finite-state transducer representing English morphology may be
used to define the class of common nouns as in table \ref{dictrules}.

\begin{table}[h]
\small
  \begin{framed}
\begin{verbatim}
! The file we want to read
define English @bin"english.hfst";
! We compose it with a noun filter and extract the input side
define Noun English .o. [?+ "<NN1>" | "<NN2>"].u;
! (NN1 is singular, NN2 plural)
\end{verbatim}
\end{framed}
  \normalsize
  \caption{Using a dictionary to write POS rules}
  \label{dictrules}
  \end{table}

If we have the use of a part-of-speech tagger, we may write our rules to act
on its output, as in table \ref{posrules}.

\begin{table}[h]
  \small
  \begin{framed}
\begin{verbatim}
define Noun LC(W) Wordchar+ ["<NN1>"|"<NN2>"] RC(W);
\end{verbatim}
\end{framed}
  \normalsize
  \caption{Using tags in pre-tagged text}
  \label{posrules}
  \end{table}

\section{Increasing Coverage}
Having considered for each rule where \verb+Degree+ and \verb+Standard+ may occur, coverage may be evaluated by also finding those cases where a LU is used as an adjective but hasn't been tagged, eg.

\small
\begin{verbatim}
define TOP Size1 | Size2 | [LU].t(NonmatchingLU);
\end{verbatim}
\normalsize

The valid match is always the longest possible one, so \verb+NonmatchingLU+ will be the tag only if no subsuming \verb+SizeN+ rule applies.

For example in

\small
\begin{framed}
\begin{verbatim}
the moving human body is <NonmatchingLU>large</NonmatchingLU>,
obtrusive and highly visible
\end{verbatim}
\end{framed}
\normalsize

We see another realisation of the frame: the \verb+Entity+ being followed by a
copula, and the \verb+LU+ appearing to the right. We could write the rule
\verb+Size2+ to capture this, adding positions for non-core elements either by
linguistic reasoning or by searching the corpus.


\section{Weighted Regular Expressions}
% Sam & Miikka (3 p.)

\subsection{Syntax}

Weights can be assigned to individual transitions or to any regular expression surrounded by brackets
with the $::$ operator. The weights are most often from the tropical semiring. The tropical weight is
represented as a float, i.e. one or more digits that may be preceded by a minus or plus sign and followed
by a comma followed by at least one digit. For example the regular expression

\begin{framed}
\begin{verbatim}
[ a b:c::0.5 d::0.3 ]::0.2
\end{verbatim}
\end{framed}

will produce a transducer that maps abd to acd with weight 0.5 + 0.3 + 0.2 = 1.0. In this example,
we basically have a transition a:a with no weight followed by a transition b:c with weight 0.5
followed by transition d:d with weight 0.3 leading to a final state with weight 0.2. However, it is
possible that operations that are called afterwards, e.g. minimization, modify the exact positions
of weights in the transducer. 

In replace rules weighted expressions can be used on both sides of the rule.
For example the rule

\begin{framed}
\begin{verbatim}
[a::3]+ -> [b::5] || c __ c
\end{verbatim}
\end{framed}

will map one or more a:s between two c:s into one b weighting each a with 3
and the b with 5. It will map 'cac', 'caac' and 'caaac' all into 'cbc' with
corresponding weights 3 + 5 = 8, 3 + 3 + 5 = 11 and 3 + 3 + 3 + 5 = 14.

\subsection{A Weighted Edit Distance Model}

Parallel weighted replace rules offer a systematic representation of various
regular transformations. Consider an edit distance transducer that accepts
any string and outputs all the strings that result from making up to $n$ edits
to it. An abbreviated example:

\begin{framed}
\begin{verbatim}
"a" -> "a" | ["" | "b" | "c" | ...]::1.0 ,,
"b" -> "b" | ["" | "a" | "c" | ...]::1.0 ,,
...
"" -> "" | ["a" | "b" | ...]::1.0 ;
\end{verbatim}
\end{framed}

the \verb+,,+ indicates that the replacements are to be compiled to operate
in parallel. Completed, this example results in a transducer that rewrites
its input with any number of edits, and gives it a cumulative weight of
$1.0$ per edit (assuming that weights are added, ie.\@ that the weight semiring
has addition as its multiplication operation, as in the log or tropical
semirings).

Suppose we have a different notion of edit, and further that we want to weight
different edits differently. We can incorporate swaps of consecutive
characters as follows:

\begin{framed}
\begin{verbatim}
{ab} -> {ab} | {ba}::1.5 ,,
{ac} -> {ac} | {ca}::1.1 ,,
... 
\end{verbatim}
\end{framed}

The replacements are quite arbitrary. They can also include context conditions,
with a number of different replacements operating in different contexts with
different weights all compiled together in parallel into one rule.

The edits can be constrained to occur a given number of times with an edit
filter:

\begin{framed}
\begin{verbatim}
define edit_distance_with_markers
  "a" -> "a" | [ ["b"::1.1 | "c"::1.2 ...] "EDIT"] ,,
  "b" -> "b" | [ ["a"::0.9 | "c"::1.0 ...] "EDIT"] ,,
  ...
define filter [[? - "EDIT"]* ["EDIT":""] [? - "EDIT"*]]^{1,3}
define 1_to_3_edits edit_distance_with_markers .o. filter;
\end{verbatim}
\end{framed}

Here each rule making an edit appends a special marker, which is removed
by composing with \verb+filter+, which allows (in this case) between 1 and 3
edits to pass through, replacing them with the empty string.

\section{Background}\label{sec:background}
% Erik (2 p.)
During the past two years, there have been many improvements to the HFST 
interface. We now have a native lexc compiler and the possibility to 
hyperminimize the lexc lexicons. There have also been many bugfixes and 
improvements to the native XFST compiler. Flag diacritics are fully supported 
in all HFST command line tools. Weights can be defined for all regular expressions,
including replace rules.

We have a download page (http://hfst.sourceforge.net/downloads/) that tells what 
we offer and where it can be found. For Linux users, we recommend a Debian installation
which is offered via Apertium. For Mac and Windows users,
we offer binaries for eight tools: hfst-xfst, hfst-lexc, hfst-pmatch,
hfst-pmatch2fst, hfst-twolc, hfst-proc, hfst-lookup and hfst-optimized-lookup.
There is also a separate MacPorts installation available.

We currently have a Python interface for HFST for Linux and Mac, generated with SWIG directly
from our C++ API. We are working to create an alternative, more Python-like interface
and implement it also for Windows.

We have tentatively added Xerox's xfsm library as one implementation to HFST.
In Table~\ref{operationtimes} we whow preliminary benchmarking results for different back-end
implementations for two opearations, minimization and composition. As a test transducer we use an 
unweighted OMorFi intermediate transducer with ca 250 000 states and 600 000 transitions (in composition
the transducer is composed with itself).

\begin{table}[h!]
  \centering
  \caption{Times for two transducer operations with different back-end implementations.
    Times are given in seconds and averaged over 10 runs.}
  \begin{tabular}{c c c }
    \hline
    Back-End & Minimization & Composition \\ \hline
    xfsm & 0,74 & 0,49 \\
    foma & 0,53 & 2,00 \\
    SFST & 1,32 & 1,78 \\
    OpenFst & 5,09 & 2,34 \\ \hline
  \end{tabular}
  \label{operationtimes}
\end{table}

In minimization, the openfst implementation is clearly the slowest. We tested minimizing
the transducer with native openfst command line tools, but the result was still 5,36 seconds.
The few extra milliseconds come from reading and writing files because not all openfst tools support reading
and writing in a pipeline. As we have an old version (1.3.1) of openfst as our back-end, we also
used the command line tools of the newest version (1.4.1), but minimization was not any faster.
The slowness is probably due to the fact that openfst must be ready to handle weights.

In composition, the xfsm implementation is the fastest. In xfsm composition, unknown and identity
symbols are handled as a part of the composition itself, but for other back-ends some post- and preprocessing is
needed. This probably explains why they are all slower.

\section{Discussion and Conclusion}\label{sec:discussion}
% Krister (1 p.)

\bibliographystyle{splncs03}
\bibliography{sfcm-2015}

\end{document}
% vim: set spell:
