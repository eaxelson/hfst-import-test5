\documentclass{llncs}

\usepackage{llncsdoc}
\usepackage{multirow}
\usepackage{caption}
\usepackage{url}

\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{expex}

%
\begin{document}
%
\title{HFST---a System for Creating NLP Tools}
%
\author{Krister Lind\'{e}n \and Erik Axelson \and Senka Drobac \and Sam Hardwick \and\\
Tommi A Pirinen \and Miikka Silfverberg \and ...}

\institute{University of Helsinki\\
Department of Modern Languages\\
Unioninkatu 40 A\\
FI-00014 Helsingin yliopisto, Finland\\
\email{\{krister.linden, erik.axelson, senka.drobac, sam.hardwick,\\
tommi.pirinen, miikka silfverberg, ...\}@helsinki.fi}}

\maketitle

% Removed the manual bibstyle in favor of splncs03.bst,
% if it's required fetch it from svn history

\begin{abstract}
%Krister
\keywords{keywords here}
\end{abstract}


\section*{Introduction}
% Krister

\section{Applications and Tests}\label{hfst:structural-layout}
% Miikka

\subsection{Language identification}
Language identification is the task of recognizing the language of a
text or text fragment. It is highly useful in applications, that need
to process documents written in various languages where the language
might not be overtly marked in the document. For example a translation
application might need to identify the language of a document in order
apply the correct translation model. Another example is a speller for
Finnish might need to identify paragraphs, that are written in
English, in order to not perform spell checking on the paragraphs.

In this section we outline how to use Hfst tagger tools and language
identification tools for creating language identifiers. We also
present an experiment on language identification for documents written
in Dutch, English, Estonian, Finnish, German or Swedish. The
experiment shows that Hfst language identifiers are highly accurate
(99.5\% of the input sentences were correctly classified).

There are several methods for performing languages
identification. Highly accurate language identification can be
accomplished by treating documents as letter sequences and training
Markov chain from training documents whose language is
known~\cite{cavnar/1994}. One Markov chain is trained for each
language that the system recognizes. Language identification consists
of applying each Markov chain on input and choosing the language whose
model gives the highest likelihood for the text. 

Hfst language identifiers adopt a Markov chain framework, which can be
implemented using weighted finite-state technology. Using Hfst tagger
tools~\cite{silfverberg/2011}, we train Markov models for all
languages. A separate program, {\tt hfst-guess-language}, reads the
models and input text and labels each sentence with the language,
whose model gave the highest likelihood for the sentence.

We present an experiment on applying Hfst language identifiers for
guessing the language of sentences written in six languages. For all
languages except Swedish, we used training data from corpora
containing newspaper text. For Swedish, we used more general text.

For Dutch we used the Alpino treebank~\cite{bouma/2000}, for English
we used the Penn Treebank~\cite{marcus/1993}, for Estonian we used the
Estonian National
Corpus~\footnote{http://www.cl.ut.ee/korpused/segakorpus/}, for
Finnish we used text from the largest Finnish newspaper Helsingin
Sanomat year 1995~\footnote{http://www.csc.fi/kielipankki/}, for
German we used the TIGER Corpus~\cite{brants/2002} and for Swedish we
used Talbanken~\cite{einarsson/1976}.

\begin{table}
\begin{center}
\begin{tabular}{l|cc}
Language & Train data (utf-8 chars) & Test data (utf-8 chars)\\
\hline
Dutch    & 245,000  & 24,000\\
English  & 265,000  & 26,000\\
Estonian & 238,000  & 23,000\\
Finnish  & 155,000  & 14,000\\
German   & 280,000  & 28,000\\
Swedish  & 164,000  & 16,000\\
\end{tabular}
\caption{For each language, we used 2000 sentences for training and
  200 sentence for testing. We give the sizes of the data sets in
  utf-8 characters.}\label{tab:lang-id-data}
\end{center}
\end{table}

For each language, we chose 2200 sentences for training and
testing. Of the sentences, every eleventh sentence was used for
testing and the rest for training. This totals 2000 sentences for
training and 200 sentences for testing for each language. The sizes of
the data sets in utf-8 characters are described in
Table~\ref{tab:lang-id-data}. The average length of a sentence in
characters was shorter for Finnish and Swedish than for the other
languages.

\begin{table}
\begin{center}
\begin{tabular}{l|l}
Language & Accuracy\\
\hline
Dutch    & ~~~99.0\%\\
English  & ~~~99.5\%\\
Estonian & ~~~99.5\%\\
Finnish  & ~~~99.5\%\\
German   & ~~100.0\%\\
Swedish  & ~~~99.5\%\\
\hline
ALL      & ~~~99.5\%
\end{tabular}
\caption{We give accuracy of the language guesser per language and for
  all languages.}\label{tab:lang-id-acc}
\end{center}
\end{table}

We ran the language identifier for test sentences from all six
languages (totally 1200 sentences) and computed the accuracy of the
language identification system as the $corr / all$, where $corr$ is
the number of sentences whose language was correctly guessed and $all$
is the number of all sentences. In Table~\ref{tab:lang-id-acc}, we
show results for each individual language and all languages
combined. 

Of all sentences, 99.5\% were correctly classified, which demonstrates
that the language identifiers are highly accurate. This is encouraging
because Finnish and Estonian have similar orthographies. This applies
to German, Swedish and Dutch as well.

Currently identification is limited to identifying the closest
language corresponding to a sentence. There is no option to label a
sentence as belonging to an unknown language. It could be possible to
apply some threshold likelihood $l(n)$ which would state that a model
has to give a sentence of $n$ characters at least likelihood $l(n)$,
in order for the sentence to be labeled as belonging to the language
of the model. 

In practice it has been very difficult to establish $l(n)$ in a
reliable way. It is likely to be dependent on the genre of the
document, which makes it less useful. Identifying unknown language
using Hfst language identifiers thus remains future work.

\subsection{Morphologies and Guessers}
\label{sec: morph-guessers}
% Juha, Miikka

Language technological applications for agglutinating languages like
Finnish and Hungarian benefit greatly from high coverage morphological
analyzer, which supply word forms with their morphological
analyses. This makes applications dependent on the coverage of the
morphological analyzer. Building a high coverage morphological
analyzer (with recall over 95\%) is a substantial task and even with a
high coverage analyzer domain specific vocabulary presents a
challenge. Therefore accurate methods for dealing with out of
vocabulary words are needed.

Using Hfst tools it is possible to use an existing morphological
analyzer to construct a morphological guesser based on word
suffixes. Suffix based guessing is sufficient for many agglutinating
languages such as Finnish~\cite{linden/2009/nodalida}, where most
inflection and derivation is marked using suffixes. Even if a word is
not recognized by the morphological analyzer, the analyzer is likely
to recognize some words which inflect similarly as the unknown
word. These can be used for guessing the inflection of the unknown
word.

Guessing of an unknown word like ``twiitin'' (the genitive form of
``twiitti'' tweet in Finnish) is based on finding the recognized word
forms like ``sviitin'' (genitive form of ``sviitti'' hotel suite in
Finnish), that have long suffixes such as ``-iitin'', which match the
suffixes of the unrecognized word. The longer the common suffix, the
likelier it is that the unrecognized word has the same inflection as
the known word. The guesser will output morphological analyses for
``twiitin'' in order of likelihood.

Besides the length of the matching suffix, guesses can also be ordered
based on the probability that a suffix matches a given analysis. This
can be estimated using a labeled training corpus. In addition, any
existing weighting scheme in the original morphological analyzer can
be utilized.

If the morphological analyzer marks declension class, the guesser can
also be used for guessing the declension class. If the declension
class is marked, the guesser can be used for generation of word forms
as well as analysis. This is described in
Section~\ref{sec:morph-generation}.

Constructing a morphological guesser from
OMorFi~\footnote{http://code.google.com/p/omorfi/} The open-source
Finnish morphology~\cite{pirinen/2008}, the three top guesses for
``twiitin'' are (the markup is slightly simplified): \small
\begin{verbatim}
  twiit  [POS=NOUN] [GUESS_CATEGORY=5]  [NUM=SG][CASE=GEN]
  twiiti [POS=NOUN] [GUESS_CATEGORY=33] [NUM=SG][CASE=NOM]
  twiit  [POS=VERB] [GUESS_CATEGORY=53] [VOICE=ACT][MOOD=INDV] ...
\end{verbatim}
\normalsize
The first field corresponds to the stem of the word, the second field
to its main part of speech and the third to its declension class. The
fourth field shows the inflectional and derivational information of
the guess. In this case, the first guess is correct. It is modeled
after declension class number 5, which is a class of nouns containing
among others the noun ``sviitti''.


More stuff to write about:
\begin{itemize}
\item Experiment for Finnish. How often is the first guess correct?
  How often is the correct answer among the three best guesses?
\item Using the guesser for extending the lexicon.
\item More?
\end{itemize}

\subsection{Spell-checking}
% Tommi

Weighted finite-state methods in performing spell-checking and correction is a
relatively recent branch of study in research of spell-checking. The concept is
simple: finite-state morphological analysers and such can be trivially ported
into spell-checking dictionaries providing a language model for the correctly
spelled words in the spell-checking system. A baseline finite-state model for
correcting spelling errors can be inferred from the language model by creating
a Levenshtein-Damerau automaton based on the alphabetic characters present in
the language. The language model can be simply trained to prefer more common
words when the Levenshtein-Damerau distance between to suggestions is the same.
This is done by basic unigram language model training that simply maximises
the frequency of the suggested word. To our experience even relatively moderate
training material will gain improvement in quality as the statistical training
improves the discriminative power of the model, and the likelihood of random
typing error is more likely in frequent words.

The practical process of creating a finite-state spell-checker and corrector
is really simple: given an analysator capable of recognising correctly spelled
word-forms of a language, take a projection to the surface forms to create a
single-tape automaton. The automaton is trained with corpus word-form list, 
where end-weight of each word-form is e.g. $-\log\frac{c(wf)}{CS}$, where 
$c(wf)$ is the count of word-forms, and $CS$ is the corpus size. Words not
found in the corpus are given some weight $w_{max} > -\log\frac{1}{CS}$ to
push them in the bottom of the suggestion list; this weighting can be done
in finite-state algebra by composition of weighted $\Sigma^{\star}$ language,
or by manually fixing the data structure.

The error model can be improved from the baseline Levenshtein-Damerau distance
metric as well. For this purpose we need an error corpus, that is, set of
errors with their frequencies. This can be semi-automatically extracted from
weakly annotated sources, such as Wikipedia. From wikipedia we get, among tons
of other things, word-to-word corrections. It is possible to use the specific
word-to-word corrections to create simple extension of common confusables to
error model. Another way is to re-align the corrections using the
Damerau-Levenshtein algorithm, and train the original distance measure with
frequencies of the corrections in same manner as we did for word-forms above.

As an example of simplicity of this process, we have obtained an open source
German morphological analyser
morphisto~\footnote{\url{http://code.google.com/p/morphisto/}} to generate a
spelling checker, trained it with word-forms extracted from German
Wikipedia~\footnote{\url{http://de.wikipedia.org}} and applied it to Wikipedia
data to find spelling errors and correct them with n~\% of precision. The whole
script to do this is in our version control~\footnote{\url{}}, and it took us
no more than one work day by one researcher to implement this application.

\subsection{Named Entity Recognition}
% Jyrki, Juha (Sam?)

\subsection{Language Generation for Unknown words}
\label{sec:morph-generation}
Natural language user interfaces like dialogue systems, need a
language generation component for generating messages for the
user. The aim is to supply the user with information about the
internal state of some data base such as airline connections or
weather phenomena.

Language generation systems for agglutinating languages will benefit
from morphological analyzers, because generating syntactically correct
sentences requires inflecting words according to syntactic
context. Depending on the domain and coverage of the morphological
analyzer, it might also be necessary to inflect words that are not
recognized by the morphological analyzer. 

Hfst morphological guessers presented in Section~\ref{sec:
  morph-guessers} can be used for generation as well as morphological
analysis. For example using the OMorFi morphology for Finnish, the
best morphological guess for the unknown word ``twiitin'' is
\begin{verbatim}
  twiit  [POS=NOUN] [GUESS_CATEGORY=5]  [NUM=SG][CASE=GEN]
\end{verbatim}
Replacing the inflectional information {\tt [NUM=SG][CASE=GEN]}
(singular geneitive case) by {\tt [NUM=PL][CASE=PAR]} (plural
partitive case) gives the analysis
\begin{verbatim}
  twiit  [POS=NOUN] [GUESS_CATEGORY=5]  [NUM=PL][CASE=PAR]
\end{verbatim}
which can be fed back to the guesser to generate the surface forms
``twiitteja'' and ``twiittejä''. The latter one is correct, though the
first one would also be possible in theory, since the variation
between ``-ja'' and ``-jä'' is governed by Finnish vowel harmony and
the stem ``twiit'' is neutral with respect to vowel harmony.

Besides language generation, morphological form generation is useful
when adding new entries to a morphological analyzer. For a human
expert it is easy to identify whether the word forms in a generated
list are the correct ones. It would also be possible to search a
corpus for the generated word forms, which can give a clue of whether
the generated forms constitute actual words and therefor also give a
clue about whether the guessed morphological analysis is the correct
one.

\section{Examples for User Environments}

\subsection{An Interface in Python}
% Erik, Tommi

In addition to API library and command line tools, HFST library can also be used
through SWIG-generated python bindings. Currently the bindings are offered for
python programming language versions 2 and 3. All HFST functionalities are available
via both versions, but the python interpreters themselves have some differences.
Python 2 allows HFST exceptions to be caught directly, but python 3 requires the use
of a special wrapper function written as a part of the bindings. On the other hand, 
python 3 has better support for unicode characters, so it is probably a better choice
for linguistic applications.

The python bindings in particular make it smooth to use language models
developed for HFST in rapid prototyping of advanced tools. For an example, in
my current project I developed a chunker for Finnish language by simply
bracketing adjacent agreeing cases and few other similar expressions with
basically few lines of code on top of existing morphological analysers. E.g.
given Finnish sentence ``miljoona kärpästä voi olla väärässä paikassa'' we get
bracketing all three phrases bracketed by really simple python based bracketing
that are illustrated in following figure gloss:

\ex
\begingl
\gla Miljoona$_1$ kärpästä$_1$ voi$_2$ olla$_2$ väärässä$_3$ paikassa$_3$//
\glb Million-{\sc Num} fly-{\sc Par} can-{\sc AuxV} be-{\sc Inf} wrong-{\sc Ine} place-{\sc Ine}//
\glft `Million flies can be in a wrong place' //
\endgl
\xe


\subsubsection{A Chatroom Morphology tool}
% Sam

\subsection{HFST on Unix, Mac and Windows}
% Erik, Tommi

Portability has been one of the design goals of HFST system. The current 
versions are available or compilable on all POSIX-supporting systems, including
Linuxes, Mac OS X and Cygwin under Windows system. Compilation is also possible 
on MinGW under Windows. Another layer of portability
is attained by HFST's programming language bindings via SWIG, it is possible to
use full HFST library through it's SWIG interfaces in all supported systems with
python versions 2 and 3.


\subsection{Other user-oriented items}
% Erik

\section{Under the Hood}

\subsection{An independent XFST module}
% Erik (Erik)

HFST command line tools include an XFST parser tool that can be used in interactive
mode or to compile scriptfiles. The tool implements the same functionalities as the
original XFST (Xerox Finite-State Tool) which is a general-purpose utility for computing
with finite-state networks.


\subsection{XFST regexp compilation}
% Senka, Miikka

\subsection{Pmatch with applications for NER}
% Sam

\subsection{Other HFST development tools}
% Erik

\section{Discussion}\label{hfst:discussion}

\subsection{HMM vs. CRF for English and Finnish taggers}
% Miikka

\subsection{Adding rules to statistical systems (taggers and parser)}
% Miikka

\section{Conclusion}\label{hfst:conclusion}

\subsubsection*{Acknowledgments}

\bibliographystyle{splncs03}
\bibliography{sfcm-2013}

\end{document}
% vim: set spell:
