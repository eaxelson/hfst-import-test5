\documentclass{llncs}

\usepackage{llncsdoc}
\usepackage{multirow}
\usepackage{caption}
\usepackage{url}

\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\usepackage{expex}

%
\begin{document}
%
\title{HFST---a System for Creating NLP Tools}
%
\author{Krister Lind\'{e}n \and Erik Axelson \and Senka Drobac \and Sam Hardwick \and\\
Jyrki Niemi \and Tommi A Pirinen \and Miikka Silfverberg \and ...}

\institute{University of Helsinki\\
Department of Modern Languages\\
Unioninkatu 40 A\\
FI-00014 Helsingin yliopisto, Finland\\
\email{\{krister.linden, erik.axelson, senka.drobac, sam.hardwick,\\
jyrki.niemi, tommi.pirinen, miikka silfverberg, ...\}@helsinki.fi}}

\maketitle

% Removed the manual bibstyle in favor of splncs03.bst,
% if it's required fetch it from svn history

\begin{abstract}
%Krister
\keywords{keywords here}
\end{abstract}


\section*{Introduction}
% Krister

\section{Applications and Tests}\label{hfst:structural-layout}
% Miikka

\subsection{Language identification}
Language identification is the task of recognizing the language of a
text or text fragment. It is highly useful in applications, that need
to process documents written in various languages where the language
might not be overtly marked in the document. For example a translation
application might need to identify the language of a document in order
to apply the correct translation model. Another example is a speller for
Finnish that might need to identify paragraphs that are written in
English, in order not to perform spell checking on the paragraphs.

In this section we outline how to use Hfst tagger tools and language
identification tools for creating language identifiers. We also
present an experiment on language identification for documents written
in Dutch, English, Estonian, Finnish, German or Swedish. The
experiment shows that Hfst language identifiers are highly accurate
(99.5\% of the input sentences were correctly classified).

There are several methods for performing language
identification. Highly accurate language identification can be
accomplished by treating documents as letter sequences and training
Markov chain from training documents whose language is
known~\cite{cavnar/1994}. One Markov chain is trained for each
language that the system recognizes. Language identification consists
of applying each Markov chain on input and choosing the language whose
model gives the highest likelihood for the text. 

Hfst language identifiers adopt a Markov chain framework, which can be
implemented using weighted finite-state technology. Using Hfst tagger
tools~\cite{silfverberg/2011}, we train Markov models for all
languages. A separate program, {\tt hfst-guess-language}, reads the
models and input text and labels each sentence with the language,
whose model gave the highest likelihood for the sentence.

We present an experiment on applying Hfst language identifiers for
guessing the language of sentences written in six languages. For all
languages except Swedish, we used training data from corpora
containing newspaper text. For Swedish, we used more general text.

For Dutch we used the Alpino treebank~\cite{bouma/2000}, for English
we used the Penn Treebank~\cite{marcus/1993}, for Estonian we used the
Estonian National
Corpus~\footnote{http://www.cl.ut.ee/korpused/segakorpus/}, for
Finnish we used text from the largest Finnish newspaper Helsingin
Sanomat year 1995~\footnote{http://www.csc.fi/kielipankki/}, for
German we used the TIGER Corpus~\cite{brants/2002} and for Swedish we
used Talbanken~\cite{einarsson/1976}.

\begin{table}
\begin{center}
\begin{tabular}{l|cc}
Language & Train data (utf-8 chars) & Test data (utf-8 chars)\\
\hline
Dutch    & 245,000  & 24,000\\
English  & 265,000  & 26,000\\
Estonian & 238,000  & 23,000\\
Finnish  & 155,000  & 14,000\\
German   & 280,000  & 28,000\\
Swedish  & 164,000  & 16,000\\
\end{tabular}
\caption{For each language, we used 2000 sentences for training and
  200 sentence for testing. We give the sizes of the data sets in
  utf-8 characters.}\label{tab:lang-id-data}
\end{center}
\end{table}

For each language, we chose 2200 sentences for training and
testing. Of the sentences, every eleventh sentence was used for
testing and the rest for training. This totals 2000 sentences for
training and 200 sentences for testing for each language. The sizes of
the data sets in utf-8 characters are described in
Table~\ref{tab:lang-id-data}. The average length of a sentence in
characters was shorter for Finnish and Swedish than for the other
languages.

\begin{table}
\begin{center}
\begin{tabular}{l|l}
Language & Accuracy\\
\hline
Dutch    & ~~~99.0\%\\
English  & ~~~99.5\%\\
Estonian & ~~~99.5\%\\
Finnish  & ~~~99.5\%\\
German   & ~~100.0\%\\
Swedish  & ~~~99.5\%\\
\hline
ALL      & ~~~99.5\%
\end{tabular}
\caption{We give accuracy of the language guesser per language and for
  all languages.}\label{tab:lang-id-acc}
\end{center}
\end{table}

We ran the language identifier for test sentences from all six
languages (totally 1200 sentences) and computed the accuracy of the
language identification system as the $corr / all$, where $corr$ is
the number of sentences whose language was correctly guessed and $all$
is the number of all sentences. In Table~\ref{tab:lang-id-acc}, we
show results for each individual language and all languages
combined. 

Of all sentences, 99.5\% were correctly classified, which demonstrates
that the language identifiers are highly accurate. This is encouraging
because Finnish and Estonian have similar orthographies. This applies
to German, Swedish and Dutch as well.

Currently identification is limited to identifying the closest
language corresponding to a sentence. There is no option to label a
sentence as belonging to an unknown language. It could be possible to
apply some threshold likelihood $l(n)$ which would state that a model
has to give a sentence of $n$ characters at least likelihood $l(n)$,
in order for the sentence to be labeled as belonging to the language
of the model. 

In practice it has been very difficult to establish $l(n)$ in a
reliable way. It is likely to be dependent on the genre of the
document, which makes it less useful. Identifying unknown language
using Hfst language identifiers thus remains future work.

\subsection{Morphologies and Guessers}
\label{sec: morph-guessers}
% Juha, Miikka

Language technological applications for agglutinating languages like
Finnish and Hungarian benefit greatly from high coverage morphological
analyzers, which supply word forms with their morphological
analyses. This makes applications dependent on the coverage of the
morphological analyzer. Building a high coverage morphological
analyzer (with recall over 95\%) is a substantial task and even with a
high coverage analyzer domain specific vocabulary presents a
challenge. Therefore accurate methods for dealing with out of
vocabulary words are needed.

Using Hfst tools it is possible to use an existing morphological
analyzer to construct a morphological guesser based on word
suffixes. Suffix based guessing is sufficient for many agglutinating
languages such as Finnish~\cite{linden/2009/nodalida}, where most
inflection and derivation is marked using suffixes. Even if a word is
not recognized by the morphological analyzer, the analyzer is likely
to recognize some words which inflect similarly as the unknown
word. These can be used for guessing the inflection of the unknown
word.

Guessing of an unknown word like ``twiitin'' (the genitive form of
``twiitti'' tweet in Finnish) is based on finding the recognized word
forms like ``sviitin'' (genitive form of ``sviitti'' hotel suite in
Finnish), that have long suffixes such as ``-iitin'', which match the
suffixes of the unrecognized word. The longer the common suffix, the
likelier it is that the unrecognized word has the same inflection as
the known word. The guesser will output morphological analyses for
``twiitin'' in order of likelihood.

Besides the length of the matching suffix, guesses can also be ordered
based on the probability that a suffix matches a given analysis. This
can be estimated using a labeled training corpus. In addition, any
existing weighting scheme in the original morphological analyzer can
be utilized.

If the morphological analyzer marks declension class, the guesser can
also be used for guessing the declension class. If the declension
class is marked, the guesser can be used for generation of word forms
as well as analysis. This is described in
Section~\ref{sec:morph-generation}.

Constructing a morphological guesser from
OMorFi~\footnote{http://code.google.com/p/omorfi/} The open-source
Finnish morphology~\cite{pirinen/2008}, the three top guesses for
``twiitin'' are (the markup is slightly simplified): \small
\begin{verbatim}
  twiit  [POS=NOUN] [GUESS_CATEGORY=5]  [NUM=SG][CASE=GEN]
  twiiti [POS=NOUN] [GUESS_CATEGORY=33] [NUM=SG][CASE=NOM]
  twiit  [POS=VERB] [GUESS_CATEGORY=53] [VOICE=ACT][MOOD=INDV] ...
\end{verbatim}
\normalsize
The first field corresponds to the stem of the word, the second field
to its main part of speech and the third to its declension class. The
fourth field shows the inflectional and derivational information of
the guess. In this case, the first guess is correct. It is modeled
after declension class number 5, which is a class of nouns containing
among others the noun ``sviitti''.


More stuff to write about:
\begin{itemize}
\item Experiment for Finnish. How often is the first guess correct?
  How often is the correct answer among the three best guesses?
\item Using the guesser for extending the lexicon.
\item More?
\end{itemize}

\subsection{Spell-checking}
% Tommi

Using weighted finite-state methods in performing spell-checking and correction 
is a relatively recent branch of study in spell-checking research. The concept
is simple: finite-state morphological analysers and such can be trivially ported
into spell-checking dictionaries providing a language model for the correctly
spelled words in the spell-checking system. A baseline finite-state model for
correcting spelling errors can be inferred from the language model by creating
a Levenshtein-Damerau automaton based on the alphabetic characters present in
the language. The language model can be simply trained to prefer more common
words when the Levenshtein-Damerau distance between to suggestions is the same.
This is done by basic unigram language model training that simply maximises
the frequency of the suggested word. To our experience, even relatively moderate
training material will gain improvement in quality as the statistical training
improves the discriminative power of the model, and the likelihood of random
typing error is more likely in frequent words.

The practical process of creating a finite-state spell-checker and corrector
is really simple: given an analyser capable of recognising correctly spelled
word-forms of a language, take a projection to the surface forms to create a
single-tape automaton. The automaton is trained with corpus word-form list, 
where end-weight of each word-form is e.g. $-\log\frac{c(wf)}{CS}$, where 
$c(wf)$ is the count of word-forms, and $CS$ is the corpus size. Words not
found in the corpus are given some weight $w_{max} > -\log\frac{1}{CS}$ to
push them in the bottom of the suggestion list; this weighting can be done
in finite-state algebra by composition of weighted $\Sigma^{\star}$ language,
or by manually fixing the data structure.

The error model can be improved from the baseline Levenshtein-Damerau distance
metric as well. For this purpose we need an error corpus, that is, a set of
errors with their frequencies. This can be semi-automatically extracted from
weakly annotated sources, such as Wikipedia. From Wikipedia we get, among tons
of other things, word-to-word corrections. It is possible to use the specific
word-to-word corrections to create simple extension of common confusables to
error model. Another way is to re-align the corrections using the
Damerau-Levenshtein algorithm, and train the original distance measure with
frequencies of the corrections in same manner as we did for word-forms above.

The application of the language and error model for spell-checking is
standard finite-state traversal or composition. The checking of correct spelling
is a composition $w \circ L$, where $w$ is a single path automaton containing
the word-form and $L$ is a single-tape automaton recognising the correct
word-forms of a language. The spelling correction is $(w \circ E \circ L)_1$,
where $E$ is an two-tape automaton containing the error model, and $_1$ is
projection to result language.

As an example of simplicity of this process, we have obtained an open source
German morphological analyser
morphisto~\footnote{\url{http://code.google.com/p/morphisto/}} to generate a
spelling checker, trained it with word-forms extracted from German
Wikipedia~\footnote{\url{http://de.wikipedia.org}} and applied it to Wikipedia
data to find spelling errors and correct them. The whole script to do this is
in our version
control~\footnote{\url{svn://svn.code.sf.net/p/hfst/code/trunk/articles/sfcm-2013-article}},
and it took us no more than one work day by one researcher to implement this
application.  The resulting system does spell-checking and correction with
baseline finite-state edit distance algorithm~\cite{pirinen2010finitestate}
applying up to 2 errors per word-form at speed of 77,500~word-forms per second.
For further evaluations on other language and error models, refer
to~\cite{pirinen2012improving}.

\subsection{Named Entity Recognition}
% Jyrki, Juha (Sam?)

\subsubsection{Background.}

An important application of the Pmatch tool is rule-based named entity
recognition (NER). Toy examples of named entity recognition with
his FST pattern matching tool (\texttt{pmatch}) were presented by Karttunen
\cite{karttunen/2011}. The HFST Pmatch tool was modelled after
Karttunen's, but it is a completely independent implementation with
some differences in features. We have
converted a full-scale named entity recognizer for Swedish to use
HFST Pmatch, and we are developing one for Finnish.

A named entity recognizer marks names in a text, typically with
information on the type (class) of the name \cite{nadeau/2007}. Major
types of names include persons, locations, organizations, events and
works of art. NER tools often also recognize temporal and numeric
expressions. Names and their types can be recognized based on internal
evidence, i.e. the structure of the name itself (e.g., \textit{ACME
  Inc.} probably denotes a company), or based on external evidence,
i.e. the context of the name (e.g., \textit{she works for ACME};
\textit{ACME hired a new CEO}) \cite{mcdonald/1996}. In
addition, NER tools typically use gazetteers, lists of known names, to
ensure that high-frequency names are recognized with the correct type.

\subsubsection{Named Entity Recognition with Pmatch.}

A key feature of Pmatch that makes it well-suited for NER is the ability
to add XML-style tags around substrings matching a regular expression,
as in \cite{karttunen/2011}. Such tagging regular expressions are
specified by suffixing the expression with
\texttt{EndTag(\textit{TagName})}. For example, the following regular
expression \texttt{CorpSuffix} (with auxiliary definitions) marks
company names ending in a company designator:

\begin{verbatim}
Define NSTag [? - [Whitespace|"<"|">"]] ;
Define Aa ["A"|"a"] ;
...
Define Zz ["Z"|"z"] ;
Define CorpSuffix
    [UppercaseAlpha NSTag+ " "]+
    ["HB" | Aa Bb | Aa "." Bb "." | Cc Oo | Ii "nc" | Ll Tt Dd
     | "AG" | "A.S." | "A/S" | "AS" | "ADR" | "Corp" | "Gmb" Hh
     | "Hld" | "Limited" | "N.V." | "Oy" | "S.A." | "SA" | "BV"]
    EndTag(EnamexOrgCrp) ;
\end{verbatim}

\noindent
\begin{sloppypar}
The built-in set \texttt{Whitespace} denotes any whitespace character
and \texttt{UppercaseAlpha} any uppercase letter. String literals are
enclosed in double quotation marks where Karttunen's FST uses curly braces
\cite{karttunen/2011}.
\end{sloppypar}

For matching, Pmatch considers the regular expression with the special
name \texttt{TOP}. Thus, to be able to tag the company names with the
expression \texttt{CorpSuffix} above, \texttt{TOP} must refer to it:
\begin{verbatim}
Define TOP ... | CorpSuffix | ... ;
\end{verbatim}
The expression can now tag the company names in the following input
text:
\begin{verbatim}
Computer Systems Corp announced a merger with Home Computers Inc .
\end{verbatim}
\noindent
The output is:
\begin{verbatim}
<EnamexOrgCrp>Computer Systems Corp</EnamexOrgCrp> announced a
merger with <EnamexOrgCrp>Home Computers Inc</EnamexOrgCrp> .
\end{verbatim}

If a part of the input does not match the \texttt{TOP} regular
expression or only matches a subexpression without an \texttt{EndTag},
Pmatch outputs it as such.

In general, a Pmatch expression set (file) contains a list of named
regular expression definitions of the form \texttt{Define
  \textit{name} \textit{regex} ;}. The regular expression
\texttt{\textit{regex}} may contain references to other named regular
expressions.

Pmatch considers leftmost longest matches of \texttt{TOP} in the input
and adds the tags specified in \texttt{TOP} or the expressions to
which it refers. If several subexpressions match the same leftmost
longest match, it is unspecified (but deterministic) which one Pmatch
chooses. To disambiguate between matches, context expressions can be
added to the matching regular expressions. This case may arise when
one expression is an exception to a more general rule. In such a case,
the exception expression can be subtracted from the general one.
For example, to
prevent capitalized \textit{järnväg} (`railway') from matching a more
general expression marking street names, it is subtracted from the
more general pattern:

\begin{verbatim}
Define LocStrSweExcept "Järnväg" ("en") | "Omväg" ("en") | ... ;
Define LocStrSwe
    [Capword2 ["väg" ("en") | "gata" ("n")]] - LocStrSweExcept
    EndTag(EnamexLocStr) ;
\end{verbatim}

\subsubsection{Regular Expression Contexts.}

An \texttt{EndTag} expression may be accompanied with a context
expression specifying that the tags should only be added if the
context of the match matches the context expression. For example, the
following expression tags the capitalized words following
\textit{rörelseresultatet för} (`operating profit of') with
\texttt{EnamexOrgCrp}:

\begin{verbatim}
Define CapWord2 UppercaseAlpha NSTag+ ;
Define OrgCrpOpProfit
    CapWord2 [" " CapWord2]*
    EndTag(EnamexOrgCrp)
    LC(Rr "örelse" ["resultatet" | "förlust"] " för ") ;
Define TOP ... | OrgCrpOpProfit | ... ;
\end{verbatim}

\noindent
For example:
\begin{verbatim}
Rörelseresultatet för <EnamexOrgCrp>Computer Systems</EnamexOrgCrp>
var ...
\end{verbatim}

As in \cite{karttunen/2011}, the regular expression in \texttt{LC()}
specifies a left context that must precede the actual match for the
match to be considered and the tags added. Similarly, \texttt{RC()}
specifies a right context that must follow the match. \texttt{NLC()}
and \texttt{NRC()} specify negative left and right context,
respectively: the match is considered only if not preceded or
followed, respectively, by the context.

Context expressions may be combined with conjunction and disjunction.
For example, the following expressions mark a capitalized word ending
in an \textit{s} as a sports organization (\texttt{EnamexOrgAth}) if
it is preceded by \textit{in} and followed by \textit{segermål}
(`winning goal'):

\begin{verbatim}
Define OrgAthWingoal
    CapWord2 "s" EndTag(EnamexOrgAth) LC(" in ") RC(" segermål") ;
Define TOP ... | OrgAthWingoal | ... ;
\end{verbatim}

\noindent
The following expressions mark the listed words as written media
(newspapers; \texttt{EnamexWrkWmd}) if either preceded by one of the
words in \texttt{LC()} or followed by an opening parenthesis, digits,
slash and a digit (\texttt{RC()}):

\begin{verbatim}
Define WrkWmdNewspaper
    "SvD" | "GP" | "DN" | "Sydsvenskan" | "Dagens " CapWord2
    EndTag(EnamexWrkWmd) ] ;
    [  LC(["av" | "enligt" | "till" | "dagens"] " ")
     | RC(" ( " Num+ "/" Num) ]
Define TOP ... | WrkWmdNewspaper | ... ;
\end{verbatim}

Conjunctive contexts can also be specified at several stages in the
expressions. For example, a name is marked as a sports event by the
following expressions only if it is followed by a space and
\textit{spelades} (`was played') (right context expression from
\texttt{EvnAtlIntl}) and preceded by a space or sentence boundary
(\texttt{\#}) (left context expression from \texttt{TOP}):

\begin{verbatim}
Define EvnAtlIntl
    [CapWord2 " "]+ "International "
    EndTag(EnamexEvnAtl)
    RC(" spelades") ;
Define TOP [ ... | EvnAtlIntl | ... ] LC(Whitespace | #) ;
\end{verbatim}

\noindent
In this case, the left context condition in \texttt{TOP} is considered
for all the \texttt{EndTag} expressions contained or included in
\texttt{TOP}.

\subsubsection{Transductions with Pmatch.}

HFST Pmatch regular expressions may contain transductions that can add
extra output or discard specified parts of the input. Even though they
are not in general used in tagging regular expressions, they can be
used in correction expressions that modify tags added by previous sets
of expressions. (Pmatch makes a single pass over its input, so a
transduction cannot modify tags added by the same set of expressions.) For
example, the following expressions move the start tag by removing the
existing tags and adding new ones using \texttt{EndTag}:

\begin{verbatim}
Define LowerWord LowercaseAlpha+ ;
Define NoTags [? - ["<"|">"]]+ ;
Define StartTagPrsHum "<EnamexPrsHum>" ;
Define RemoveStartTagPrsHum
    StartTagPrsHum> .o. [StartTagPrsHum -> ""] ;
Define EndTagEnamex "</Enamex" [? - ">"]+ ">" ;
Define RemoveEndTag [EndTagEnamex .o. [EndTagEnamex -> ""]] ;
Define TOP
    RemoveStartTagPrs LowerWord " " CapWord2 "s "
    [Vv Dd " " NoTags RemoveEndTag EndTag(EnamexPrsHum)] ;
\end{verbatim}
%
For example, this corrects the tagging
\begin{verbatim}
... <EnamexPrsHum>säger Computers vd Svensson</EnamexPrsHum>
\end{verbatim}
(`says Computer's CEO Svensson') to
\begin{verbatim}
... säger Computers vd <EnamexPrsHum>Svensson</EnamexPrsHum>
\end{verbatim}

\subsubsection{Converting the Swedish Named Entity Recognizer to Use
  Pmatch.}

The Swedish named entity recognizer works on tokenized running text
input: punctuation marks are separated from words by spaces but the
words are not annotated in any way. In contrast, the forthcoming
Finnish NER tool will work on annotated text, which makes it easier to
write more general rules, in particular for a morphologically rich
language such as Finnish.

The original implementation of the Swedish named entity recognizer
\cite{kokkinakis/2003} contained 24 different recognizers running in a
pipeline and a correction filter run after each stage. 21 of the
recognizers and the correction filter had been written using
Flex\footnote{http://flex.sourceforge.net/} rules; the remaining three
were Perl scripts recognizing names in gazetteers. The Flex rules
recognize regular expression matches in input, corresponding to names
and their possible context, and the actions of the rules mark the name
parts of the matches with XML elements in the output. The correction
filter modifies
the tags of the elements to remove incorrectly marked names, to
include words in the context in names, to exclude incorrectly included
parts of names, to split names into two, to mark new names based on
marked names in the context, and to change the type of a name.

Incentives for reimplementing the Swedish recognizer in Pmatch
included the slow compilation of some of the Flex rule sets (several
hours in the worst case), resulting in a long wait before being able to
test changes to the rules, and the wish to be able to use a single
tool or
formalism for all the components of the recognizer.

Since both Flex and Pmatch are based on regular expressions and
recognizing the leftmost longest match, we were able to automate a
large part of the conversion from Flex rules to Pmatch rules. The
conversion script also analysed the Flex actions to find out which
part (words) of the recognized match was to be marked as a name and
which was context. The correction filter was converted by hand, since
its actions were more varied and would have required more elaborate
processing than those in the recognizers.

As an example of the conversion, consider the Flex rule in the rule
file marking organization names:
\begin{verbatim}
sång(erska|are)" i "{U}[^\n ]+(" "{U}[^ \n]*)+    {
    printCLT(yytext,2);}
\end{verbatim}
Here, \texttt{\{U\}} in the regular expression denotes an uppercase
letter, and the function call in the action indicates that the name to
be tagged begins from the character following the second space in the
match and that it should be tagged as a cultural organization. This
rule was converted to the following Pmatch expression:
\begin{verbatim}
Define EnamexOrgClt006
    CapWord2 [" " CapWord]+
    LC("sång" ["erska" | "are"] " i ") EndTag(EnamexOrgClt) ;
\end{verbatim}
Here the words not included in the match (`singer in') are moved to
the left context. \textsf{[Should we present a more elaborate example
  of the conversion?]}

However, differences in the semantics of Flex NER rules and Pmatch
meant that the Pmatch expressions generated by the automatic
conversion had to be edited by hand to work correctly. Firstly, the
Flex rules were written so that the matched regular expressions cover
the contexts in addition to the name to be recognized, whereas Pmatch
excludes contexts from the leftmost longest match. Consequently, the
leftmost longest match at a certain point in text may be found by
different patterns in Flex and Pmatch.

% TODO: Try to find a real example
For example, consider the text \textit{vd vid Computer Systems ab}
(`CEO at Computer Systems ab') and one Flex rule that matches
\textit{vd vid Computer Systems} but only marks \textit{Computer
  Systems} as a corporate organization, and another rule that matches
\textit{Computer Systems ab} and marks it as a whole. In this text,
the first rule provides the leftmost longest match. In the
automatically converted Pmatch expressions, however, \textit{vd vid}
is considered as a context and not included in the match, so both
expressions match at the same point but since the second expression is
longer, it is chosen. To get the same result as with the Flex rules, a
negative left context matching \textit{vd vid} should be added to the
latter expression. However, in this case, the marking \textit{Computer
  Systems ab} would in fact be better.

Secondly, Flex rules are ordered whereas Pmatch expressions are not.
This means that the Flex patterns can be ordered from the most
specific to the most general, so that the most specific pattern is
chosen even if also a more general pattern would match. In contrast,
Pmatch cannot guarantee any specific order: if more than one
expression have the same leftmost longest match, an arbitrary
expression is chosen, though always the same for the same set of
expressions. The ordering has to be replaced with more detailed
contexts or regular language difference or both. An example of the
regular language difference was presented above.

With some modifications to account for the lack of ordering, the
Pmatch rules were able to recognize and classify the same names as the
original Flex rules. However, many rules would be more natural if
written from scratch to utilize the features of Pmatch. Existing rules
could be combined and generalized. For example, rules marking the same
names but with contexts with different number of words could be
combined into a single Pmatch regular expression. \textsf{[Examples?]}

The gazetteer lookup of the original implementation of the Swedish NER
marks words found in the gazetteer not only as such but also, for
example, names with an inflectional suffix, names with another name
prefixed with a dash or slash, and lowercased names of at least five
letters. The gazetteers of the original implementation contain pairs
of names and their types, but to make the Pmatch implementation more
efficient, the names have been divided into files by name type. The
Pmatch rules read these files as disjunctions of strings with the
construct \texttt{@txt"\textit{filename}"}. The basic Pmatch rules for
the gazetteer lookup are as follows:

\begin{verbatim}
Define NSTag [Alpha|Num] | [? - [Whitespace|"<"|">"]] ;
! Names as such
Define LocStrBasic @txt"gaz-LocStr.txt" EndTag(EnamexLocStr) ;
Define PrsHumBasic @txt"gaz-PrsHum.txt" EndTag(EnamexPrsHum) ;
...
! Names with an inflectional suffix
Define Suffix ["s" | ":" LowercaseAlpha+] ;
Define LocStrSuffix
    @txt"gaz-LocStr.txt" Suffix EndTag(EnamexLocStr) ;
...
! Words with a prefixed word
Define PrefixWord UppercaseAlpha NSTag+ ["/"|"-"] ;
Define LocStrPrefixWord 
    PrefixWord @txt"gaz-LocStr.txt" EndTag(EnamexLocStr) ;
...
! Lowercased names of at least five letters
Define LowerWord5p LowercaseAlpha^{5,} ;
Define LocStrLower
    LowerWord5p .o. LowCase(@txt"gaz-PrsHum.txt")
    EndTag(EnamexPrsHum) ;
...
Define Boundary [" " | #] ;
Define TOP
    [  LocStrBasic | PrsHumBasic | ... | LocStrSuffix | ... 
     | LocStrPrefixWord | ... | LocStrLower | ... ]
    LC(Boundary) RC(Boundary);
\end{verbatim}

\noindent
The function \texttt{LowCase(\textit{regex})} in the expression
\texttt{LocStrLower} converts \texttt{\textit{regex}} to recognize
only lowercased strings.
The context conditions in \texttt{TOP} allow names to be recognized
only at word boundaries.

The original Swedish NER system marks names with fairly generic XML
elements, with attributes in the start tag that specify the more
precise type (\texttt{TYPE}) and subtype (\texttt{SBT}) of the name;
for example:
\begin{verbatim}
<ENAMEX TYPE="ORG" SBT="CRP">Computer Systems Corp</ENAMEX>
\end{verbatim}
However, since Pmatch lacks support for element attributes, their
values are encoded in the tag names:
\begin{verbatim}
<EnamexOrgCrp>Computer Systems Corp</EnamexOrgCrp>
\end{verbatim}
The Pmatch-style elements can be converted to the attribute-style ones
with a Pmatch expression set with appropriate transductions, basically
as follows:

\begin{verbatim}
Define AlphaToUpper a:A|b:B|...|z:Z|UppercaseAlpha ;
Define MainTagName "ENAMEX" | "NUMEX" | "TIMEX" ;
Define ConvertStartTag
    "<" MainTagName ["" -> " TYPE=\""] AlphaToUpper^{3}
    ["" -> "\" SBT="\""] AlphaToUpper^{3} ["" -> "\""] ">" ;
Define ConvertEndTag
    "</" MainTagName [Alpha+ .o. [Alpha+ -> ""]] ">" ;
Define TOP [ConvertStartTag | ConvertEndTag] ;
\end{verbatim}

\textsf{[Should we omit the above, as we currently use a simple Python
  script instead, because the replacement operations do not (yet) work
  correctly?]}

\subsubsection{Performance.}

Compiling the Pmatch version of the Swedish NER was about ten times
faster than that of the Flex version, so the goal of speeding up the
compilation was largely achieved. We will also investigate further
ways to improve compilation speed. In contrast, it seems to take about
three times as much time to process text with Pmatch NER than with
Flex NER. This speed required the use of a left context condition
of a whitespace or sentence boundary; without the context, the running
times were many times longer. The total size of the current Pmatch
FSTs is over three gigabytes, about eight times as large as the
executables compiled from the Flex files. However, the FST sizes
should be reduced as soon as expression caching and sequentialization
are implemented in Pmatch. \textsf{[How about the INS() of Karttunen?
  Or is it the same as caching?]}

\subsection{Language Generation for Unknown words}
\label{sec:morph-generation}
Natural language user interfaces like dialogue systems, need a
language generation component for generating messages for the
user. The aim is to supply the user with information about the
internal state of some data base such as airline connections or
weather phenomena.

Language generation systems for agglutinating languages will benefit
from morphological analyzers, because generating syntactically correct
sentences requires inflecting words according to syntactic
context. Depending on the domain and coverage of the morphological
analyzer, it might also be necessary to inflect words that are not
recognized by the morphological analyzer. 

Hfst morphological guessers presented in Section~\ref{sec:
  morph-guessers} can be used for generation as well as morphological
analysis. For example using the OMorFi morphology for Finnish, the
best morphological guess for the unknown word ``twiitin'' is
\begin{verbatim}
  twiit  [POS=NOUN] [GUESS_CATEGORY=5]  [NUM=SG][CASE=GEN]
\end{verbatim}
Replacing the inflectional information {\tt [NUM=SG][CASE=GEN]}
(singular genitive case) by {\tt [NUM=PL][CASE=PAR]} (plural
partitive case) gives the analysis
\begin{verbatim}
  twiit  [POS=NOUN] [GUESS_CATEGORY=5]  [NUM=PL][CASE=PAR]
\end{verbatim}
which can be fed back to the guesser to generate the surface forms
``twiitteja'' and ``twiittejä''. The latter one is correct, though the
first one would also be possible in theory, since the variation
between ``-ja'' and ``-jä'' is governed by Finnish vowel harmony and
the stem ``twiit'' is neutral with respect to vowel harmony.

Besides language generation, morphological form generation is useful
when adding new entries to a morphological analyzer. For a human
expert it is easy to identify whether the word forms in a generated
list are the correct ones. It would also be possible to search a
corpus for the generated word forms, which can give a clue of whether
the generated forms constitute actual words and therefor also give a
clue about whether the guessed morphological analysis is the correct
one.

\section{Examples for User Environments}

\subsection{An Interface in Python}
% Erik, Tommi

In addition to an API library and command line tools, HFST library can
also be used through SWIG-generated python bindings. The bindings are
offered for python programming language versions 2 and 3. All HFST
functionalities are available via both versions, but the python
interpreters themselves have some differences. For example, Python 2
allows HFST exceptions to be caught directly, but python 3 requires
the use of a special wrapper function written as a part of the
bindings. On the other hand, python 3 has better support for unicode
characters, so it is probably a better choice for most linguistic
applications.

Below is an example of iterating through the states and transitions of
an HFST transducer using python bindings:

\begin{verbatim}
# Go through all states in fsm       
for state in fsm.states():
    # Go through all transitions                                               
    for transition in fsm.transitions(state):
        # do something
\end{verbatim}

And the same using HFST API directly:

\begin{verbatim}
// Go through all states in fsm
for (HfstBasicTransducer::const_iterator it = fsm.begin();
       it != fsm.end(); it++ )      
    {      
     // Go through all transitions    
    for (HfstBasicTransducer::HfstTransitions::const_iterator tr_it  
           = it->begin(); tr_it != it->end(); tr_it++) 
        {
        // do something
        }
    }
\end{verbatim}

The python bindings in particular make it smooth to use language models
developed for HFST in rapid prototyping of advanced tools. For an example, a
chunker for Finnish language was developed by simply bracketing adjacent
agreeing cases and few other similar expressions with basically few lines of
code on top of existing morphological analysers. E.g.  given Finnish sentence
``miljoona kärpästä voi olla väärässä paikassa'' we get bracketing all three
phrases bracketed by really simple python based bracketing that are illustrated
in following figure gloss:

\ex
\begingl
\gla Miljoona$_1$ kärpästä$_1$ voi$_2$ olla$_2$ väärässä$_3$ paikassa$_3$//
\glb Million-{\sc Num} fly-{\sc Par} can-{\sc AuxV} be-{\sc Inf} wrong-{\sc Ine} place-{\sc Ine}//
\glft `Million flies can be in a wrong place' //
\endgl
\xe

The rules governing chunking are all about pairs of words here: measurement
phrase is a numeral followed by a partitive nominal, verbal phrase is an
auxiliary followed by a lexical verb and noun phrase is an adjective and a noun
in any agreeing case.  The three pairs of words can be identified as common
chunks of Finnish language and having specific rules for these specific pairs
will give reasonable baseline surface syntax for applications where more
elaborate syntax structure is not required.

\subsubsection{A Chatroom Morphology tool}
% Sam

\subsection{HFST on Unix, Mac and Windows}
% Erik, Tommi

Portability has been one of the design goals of HFST system. The
current versions are available or compilable on all POSIX-supporting
systems, including Linuxes, Mac OS X and Cygwin under Windows
system. Compilation is also possible on MinGW under Windows.

Fresh version of HFST source code can be fetched from our svn
repository at Sourceforge. We also offer, usually twice a month, new
release packages that include source tarball (compilable on all
aforementioned platforms), Debian binaries (for Linux), MacPort
distribution (for MacOS X) and NSIS installer (for Windows).

Another layer of portability is attained by HFST's programming
language bindings via SWIG. It is possible to use full HFST library
through it's SWIG interfaces in all supported systems with python
versions 2 and 3.


\subsection{Other user-oriented items}
% Erik

We have four new command line tools. The most important are the XFST
parser hfst-xfst and the tagging tool hfst-tagger. Also two functions
that were earlier available only through the API can now be used via
command line tools: hfst-shuffle and hfst-prune-alphabet. The existing
tools also have new features: hfst-fst2txt can write to dot/graphviz
and PCKIMMO format and hfst-fst2strings has a new parameter that
controls its output to achieve better interoperability with other
command line tools.

Special symbols can be controlled better as we have added a parameter
to binary operators that controls whether unknown/identity transitions
are expanded, the default being true. We also have a new special
symbol, the default symbol that matches any symbol if no other
transition in a given state matches.

We have kept the number of dependencies in HFST as low as possible.
All back-ends (SFST, OpenFst and foma) are now bundled with
HFST. There is no more need to install them separately or worry about
having the right version. We can also make modifications to the
back-end libraries, for instance some of the warnings in back-end
libraries that are given during compilation are now fixed or
suppressed. GNU- and bash-specific commands were also removed from
scripts to make them more portable.

\section{Under the Hood}

\subsection{An independent XFST module}
% Erik (Erik)

HFST command line tools include an XFST parser tool that can be used
in interactive mode or to compile scriptfiles. The tool implements the
same functionalities as the original XFST (Xerox Finite-State Tool)
which is a general-purpose utility for computing with finite-state
networks. There are over 100 commands in hfst-xfst, the same as in the
Xerox tool. Below is an example of using hfst-xfst in interactive mode
where we define two transducer variables, use them in a regular
expression and print random words recognized by the expression.

\begin{verbatim}
$ hfst-xfst2fst 
hfst[0]: define Foo foo;
hfst[0]: define Bar bar;
hfst[0]: regex [[Foo|0] baz [Bar|0]];
424 bytes. 4 states, 4 arcs, 4 paths
hfst[1]: print random-words
baz
bazbar
foobaz
foobazbar
hfst[1]: 
\end{verbatim}

To test hfst-xfst, we have compiled 17 out of the 22 XFST exercises
that are found in the homepage of Beesley and Karttunen's book Finite
State Morphology. We have omitted the exercises that do not include an
answer and are not trivial to solve. We have compiled the exercises
using both Xerox's XFST and HFST's hfst-xfst and compared the results
for equivalence. For some exercises, we also have an additional
solution that uses hfst command line tools (other than hfst-xfst).

\subsection{XFST regexp compilation}
% Senka, Miikka

Regex parser tool \verb+hfst-regexp2fst+ was developed. General backward
compability with XFST regular expression parser was demonstrated through the
fsm-book test set, where original tests were altered in a way that they could be
run through the parser tool. Since the parser tool alone doesn't have stack
operations as XFST and Foma do, the biggest change in the test cases was
operating with defined variables.

For example, in the Monish Guesser Analyser test, front vowels are defined:
\begin{verbatim}
define FrontV [ i | e | é | ä ];
\end{verbatim}  
Instead of using XFST's \verb+define+ function, we save intermediate results
to files:
\begin{verbatim}
echo '[ i | e | é | ä ]' | hfst-regexp2fst > FrontV
\end{verbatim}  
Transducer saved in the FrontV file is later simply used as \verb+@"FrontV"+:
\begin{verbatim}
$ echo '[ [ ? - [ @"BackV" | @"MorphoV" ] ]+ & $[ @"FrontV" ] ] | \
[ [ ? - [ @"FrontV" | @"MorphoV" ] ]+ & $[ @"BackV" ] ]'
|  hfst-regexp2fst > Root
\end{verbatim}

In order to compare the XFST's and Foma's result with the result from the
\verb+hfst-regexp2fst+ tool, we wrote out the XFST's result as prolog script,
transformed it to HFST type transducer and then used \verb+hfst-compare+ tool
for comparison. Unfortunately, for some tests, the stack was too big to be saved
as a text file. For those tests we used only foma compiler to get the result
transducer we can compare our results to.

Although we aimed to get complete backward compability with XFST and Foma, we
noticed that in some border line cases the results might differ.

Differences might occur when using replace rules in regular expressions.
One example where we found that XFST and \verb+hfst-regexp2fst+ in certain cases
give different results is the longest match.

Both regular expressions: 
\begin{verbatim}
regex a+ b+ | b+ a+ @-> x \\ _ x ;
regex a+ b+ | b+ a+ @-> x \/ _ x ;
\end{verbatim}
result in transducer which for input \verb+aabbaax+ gives two outputs:
\verb+aaxx+ and \verb+xxx+.

However, the same regular expression compiled with \verb+hfst-regexp2fst+ tool
will for the same input give only one output \verb+xxx+:
\begin{verbatim}
$ echo 'a a b b a a x .o. a+ b+ | b+ a+ @-> x \\ _ x;' \
    | hfst-regexp2fst | hfst-fst2strings
aabbaax:xxx
$ echo 'a a b b a a x .o. a+ b+ | b+ a+ @-> x \// _ x;' \
    | hfst-regexp2fst | hfst-fst2strings
aabbaax:xxx
\end{verbatim} 
It is likely that in this case difference is caused by different compilation approaches.
In \verb+hfst-regexp2fst+ tool, replace rules are compiled using preference
operator~\cite{drobac/2012}, which in this case successfully finds that output string
\verb+aaxx+ is less preferable in comparison to the output string \verb+xxx+ and therefore is excluded from the final result.

Furthermore, we have noticed that there are some differences in pruning alphabet
after performing certain operations.
These two examples will give different results if compiled with XFST:  
\begin{verbatim}
regex [a | b | c ] & $[a | b] ;
resulting alphabet: a, b
\end{verbatim}
\begin{verbatim}
regex [a | b | c ] & [a | b] ;
resulting alphabet: a, b, c
\end{verbatim}

Hfst regex parser prunes alphabet always after following operations:
\textbf{replace rules} (contexts are also pruned before being compiled with the
rule), \textbf{complement}, \textbf{containments}, \textbf{intersection},
\textbf{minus}.
Therefore, if upper comannds were run in \verb+hfst-regexp2fst+ tool, the
resulting albphabet would always be \verb+a, b+.

\subsection{Pmatch with applications for NER}
% Sam

\subsection{Other HFST development tools}
% Erik

\section{Discussion}\label{hfst:discussion}

\subsection{HMM vs. CRF for English and Finnish taggers}
% Miikka

\subsection{Adding rules to statistical systems (taggers and parser)}
% Miikka

\section{Conclusion}\label{hfst:conclusion}

\subsubsection*{Acknowledgments}
The research leading to some of these results has received funding from the
European Commission's 7th Framework Program under grant agreement n° 238405 (CLARA).


\bibliographystyle{splncs03}
\bibliography{sfcm-2013}

\end{document}
% vim: set spell:
