% This is the Authors' notes demonstration file with content stripped out
% and sections from Krister's email substituted.
% 
% It might be a good idea to verify that Authors' Instructions.pdf
% (in this directory) is adhered to before submitting.
%
% llncs.doc is actually a tex file, it's the source for the Authors'
% Instructions. There are good examples of eg. tables and graphs there.
\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{multirow}
\usepackage{caption}
%
\begin{document}
%
\title{HFST---Framework for Compiling and Applying Morphologies}
%
\author{Krister Lind\'{e}n \and Erik Axelson \and Sam Hardwick \and\\
Tommi A Pirinen \and Miikka Silfverberg}

\institute{University of Helsinki\\
Department of Modern Languages\\
Unioninkatu 40 A\\
FI-00014 Helsingin yliopisto\\
\email{\{krister.linden, erik.axelson, sam.hardwick,\\
tommi.pirinen, miikka silfverberg\}@helsinki.fi}}

\maketitle

% Removed the manual bibstyle in favor of splncs03.bst,
% if it's required fetch it from svn history

%
\begin{abstract}
% KL
\keywords{finite-state morphology}
\end{abstract}

\section*{Introduction}
% KL


\section{Structural Layout}
% MS


\section{Structural Layout cont.}
% SH
% Comment to remove before submission:
% the relatively insignificant material here should really be incorporated into
% another section..
There has been considerable progress in achieving HSFT's goal of acting as a
compatibility layer between different representations of finite-state
transducers and, more importantly, the operations and formalisms (eg.
\verb+lexc/twolc+, \verb+xfst+, \verb+sfst+) that have been implemented
for them. HFST is now independent of any particular library and requires no
custom extensions to the libraries it uses.

\subsection{Dynamic Linking to Underlying Libraries}
Previously HFST relied on custom extensions to the libraries it supported,
namely OpenFst and SFST, and it was necessary to statically link them into
\verb+libhfst+. This was obviously also rather restrictive in terms of
new versions and different use cases (eg. experimentation with local changes to
the underlying libraries.

HFST3 supports conditional compilation of all its elements that provide
interfaces to underlying libraries, and dynamic linking is done to whichever
libraries the user configures HFST3 to use.

\subsection{Stand-alone Use and Flexibility}
Due to these improvements, HFST3 can also be built without any external
libraries (in which case only the operations for a simple internal
representation and optimized-lookup (see section \ref{optimized-lookup}),
building from text representation and fast lookup will be supported) or only
the user's self-made library, which will then support compilation to
optimized-lookup format.

\section{Coding Principles}
% EA

Exceptional situations occur sometimes in computer programs when the
user does something wrong or there is a bug in the code.
Possible user-originating situations in a finite-state library could
be for example:

\begin{enumerate} 
\item The user tries to read a binary transducer from a
file that contains a text document or does not exist.
\item A transducer in AT \& T text format has a typo on one line and the 
line cannot be parsed. 
\item The user calls a function without checking the preconditions,
e.g. tries to extract all paths from a cyclic transducer.
\end{enumerate}

Throwing an exception on such occasions gives the user a possibility
to catch the exception and recover from the situation. In HFST version
2.0, exceptional situations were handled by printing to standard error
a short message and exiting with an error code. In HFST version 3, 
exceptions are classes that have a figurative name and can contain an 
error message.

For the above scenarios, HFST will throw the following exceptions:

\begin{enumerate} 
\item NotTransducerStreamException or StreamCannotBeReadException and
the name of the file or stream in the error message.
\item NotValidAttFormatException and the line that could not be parsed
in the error message.
\item TransducerIsCyclicException.
\end{enumerate} 

The user could react to the exceptions in the following ways, for
example:

\begin{enumerate} 
\item Check that the file exists and contains transducers and try again
with the correct file.
\item Fix the typo in the text format.
\item Call another function that limits the number of paths extracted
from the transducer.
\end{enumerate} 

Exceptions are also used internally in HFST code for reporting to the 
calling function that something unexpected happened. The calling
function can handle the situation or inform the user of it. 
So the execution of the program does not need to stop or lead to an
erroneous result or even cause later a segmentation fault.

An HfstFatalException is thrown when it is supposed that the user
cannot handle that exception. The user should instead report the error
to the developers of HFST and in which circumstances it happened, because that
is essentially a bug that must be fixed. Sometimes asserts are also
used for internal checks. When an assertion fails the user should similarly
report the assert message and from which file and which line it happened.


\section{Data Format}
% EA

\subsection{Transducer binary format}

An HFST transducer in binary format consists of an HFST header followed by the
backend implementation in binary 
format\footnote{\url{https://kitwiki.csc.fi/twiki/bin/view/KitWiki/HfstTransducerHeader}}. 
In version 3.0, the header
format is less error-prone than in the previous version and gives more 
information both for users of HFST (even when seen on a screen or in a
text editor in binary format) and for the HFST library itself.
 
The current header format is somewhat similar to foma where pieces of 
information are separated by newline characters to make them more
readable. In HFST version 2, we represented the properties of a
transducer in a cryptic two-byte bit vector akin to OpenFst's header
format. The type of the transducer and the existence of an optional
alphabet were also encoded with two characters in the beginning of the
binary transducer and they were impossible to interpret without
looking at the HFST code itself.

The beginning of an HFST version 3.0 header contains an identifier
'HFST', a separating zero byte, two bytes signifying the length of
the rest of the header in bytes and a separating zero byte. The rest
of the header contains pairs of attributes and their values. The pairs
are separated by newlines and attributes and values by zero bytes. 
HFST version 3.0 header must contain at least the attributes
'version', 'type' and 'name' (in that order) and their values. 
Additional attributes can follow after these obligatory ones. 
For instance, we could include information on the minimality or
cyclicity of a transducer even if the backend implementation did not
store these properties in its binary format (e.g. SFST). 
For more information on the header format, see (ref. to HfstTransducerHeader).

The new header format makes it easier to react to unexpected
situations and inform the user of them. When we start to read an HFST
binary transducer, we first see whether the identifier 'HFST' is
found. If it is not found, we know that the user has given a wrong
type of file and can throw a appropriate exception. 
Next we see what kind of transducer is coming from the stream. 
If the backend transducer library is not linked to HFST, we can handle
the situation by throwing an exception again. 
Next we see the HFST version used and know how to process the header
and the backend implementation correctly. 
If the user has requested a verbose mode for a tool that reads the
transducer, it is possible to print the name of each transducer 
before or after reading it.

\subsection{Conversion between different backend formats}

In HFST version 3.0, the conversion between different backend formats 
(SFST, foma, OpenFst with tropical and logarithmic semiring) is
carried out through HFST's own transducer format, HfstTransitionGraph, 
a simple transition graph datatype that consists of states (unsigned
integers) and transitions between those 
states\footnote{\url{http://hfst.sourceforge.net/hfst3/index.html}}.
 
We have chosen to implement HfstTransitionGraph for two
reasons. Firstly, it serves as an intermediate transducer format in
conversions, thus reducing the number of conversion functions from 2 x
N to N x (N - 1), where N is the number of different transducer
backend formats. Secondly, it is easy to implement functions for 
HfstTransitionGraph that allow the user to construct transducers from 
scratch and iterate through their states and transitions. Implementing
such features for some transducer libraries (e.g. SFST and foma) can
be difficult because they are designed to be used on a higher level of 
abstraction. Accordingly, the functions that operate on states and 
transitions are often declared protected and not necessarily well
documented.

HfstTransitionGraph is a class template with template parameters T and
W. T defines the type of transition data that a transition uses and W
the weight type that is used in transitions and final states. 
HfstTransitionGraph contains two maps. One maps each state to a set of
that state's transitions (that are of type class HfstTransition\textless class
T\textgreater). The other maps each final state to its final weight 
(that is of type class W). Class T must use the weight type W. 
A state's transition (class HfstTransition\textless class T\textgreater) 
contains a target state and a transition data field (that is of type class T).

Actually, HfstTransitionGraph is not a transducer but a more
generalized transition graph that can contain many kinds of data in
its transitions. Currently, the HFST library offers the specializations 
HfstBasicTransducer and HfstBasicTransition for HfstTransitionGraph
and HfstTransition. These specializations are designed for weighted 
transducers. The weight class W is a float and the transition data
class T contains an input string, an output string and a weight of
type float. The specializations HfstBasicTransducer and
HfstBasicTransition are used when converting between different
transducer backend formats.

The class template HfstTransitionGraph is designed so that it can
easily be extended for different kinds of transition datatypes. For
example, if HFST tools are used in text-to-speech or speech-to-text
conversion, the symbol type of transitions will probably be something
else than strings.



\section{Alphabet}
% EA

The alphabet of a transducer means all symbols (strings) that are
known to that transducer. The alphabet includes all symbols that occur 
or have occurred in a transition of the transducer unless explicitly
removed from the alphabet. 
If we apply a binary operation (e.g. disjunction or composition) on 
transducers A and B, the resulting transducer's alphabet will include
all symbols that were in the alphabets of A and B. 

In HFST version 2.0, we were not especially interested in alphabets 
since the interface did not offer any way for the user to access a 
transducer's alphabet. It was up to the backend implementation to take
care of alphabets. In SFST the transducers always have an alphabet,
but in OpenFst their use is optional so we did not use them in our
OpenFst implementation.

In HFST version 3.0, we need to be aware of alphabets because two new
special symbols are included, \texttt{unknown} and \texttt{identity}. 
These special symbols are a part of the Xerox Finite-State Tool (XFST) 
formalism~\cite[page x]{beesley/2003} and they are also implemented in
foma~\cite{hulden/2009}. \texttt{unknown} and \texttt{identity} symbols 
are useful when we want to refer to all symbols that are not currently 
known to a transducer but which the transducer can later become aware of. 

Supporting \texttt{unknown} and \texttt{identity} symbols in all HFST backend
implementations has enabled us to make an XFST compiler that can be used
with all backend implementations. 
In this way we can offer the users of HFST new regular expression
formalism in addition to the SFST programming language. Next we
describe what these new special symbols require for HFST's
part. Because they are already implemented in foma, we have to care
only for the SFST and OpenFst implementations.

Besides keeping track of all symbols known to a transducer, we also
have to expand each transition involving \texttt{unknown} and 
\texttt{identity} symbols into a set of transitions every time we apply 
a binary operation on two transducers. 
This is because the transducer becomes aware of new symbols that are
no longer unknown and thus no more included in \texttt{unknown} or 
\texttt{identity} symbols.
Fortunately, this expansion can be done before the operation itself 
(and for composition before and after the operation itself), so we do
not have to make changes in the operations of the backend transducer
libraries. 
The operations can freely (and will) handle these special symbols just
like any ordinary symbols. 

First we iterate through the alphabets of both transducers and find
out what symbols in the alphabet of one transducer are not found in
the alphabet of the other transducer and vice versa. 
Then we add beside each transition involving \texttt{unknown} or 
\texttt{identity} symbols a set of transitions where \texttt{unknown} 
and \texttt{identity} symbols are
replaced with all symbols that the transducer just got aware of. 
For more information on expanding special symbols, 
see~\cite{hulden/2009}~and~\cite{beesley/2003}.

It is also possible to switch off special symbol handling if we know
for sure that they are not used in transducers. 
In this way we can optimize performance for instance for the tool 
hfst-calculate that processes SFST programming language which does not
support unknown or identity symbols.


\section{Algorithmic Improvements}
% MS
\subsection{Intersecting Composition}

Intersecting composition is used to apply a grammar of two-level
phonological rules on a two-level lexicon. The result of the operation
is a morphological analyzer, which maps word forms to
analyzes. Compiling the analyzer using conventional methods requires
computing the intersection of the rule transducers. This can lead to a
prohibitively large intermediate result. Intersecting composition
avoids computing the entire intersection of the rules thus reducing
both memory and time requirement. The operation was introduced by
Karttunen \cite{Karttunen/1994} and later extended to weighted
transducer in hfst2 by Silfverberg and Lind\'{e}n
\cite{silfverberg/2009/2}.

The intersecting composition operation was implemented in hfst2, but
we used techniques adopted from OpenFst \cite{openfst/2007} to improve
the implementation and the current implementation is significantly
faster than the old one. The current implementation computes a lazy
pairwise intersection of the rule transducers. The lexicon can be
composed with this intersection using a standard composition
algorithm.

\subsubsection{Previous implementation}

The implementation of intersecting composition in hfst2 can be
characterized as the composition of the lexicon transducer $L$ with a
parallel structure $P$ containing all rule transducer. For simplicity,
we assume that the lexicon and rule transducers are deterministic as
automata.

Outwards the structure $P$ resembles an ordinary transducer with
states and transitions. The states of $P$ correspond internally to
vectors of rule transducer states, which we call state
configurations. The vectors have equally many indices as there are rules
and each rule corresponds to a unique index, where its state is
stored. E.g. the start state of $P$ corresponds to the vector
containing the start states of the rules.

Initially only the start state of $P$ is computed. More states in $P$
are computed according to the transitions in $L$. E.g. the lexicon $L$
might have a transition with output-symbol {\tt a} in its initial
state. In order to compute the intersecting composition, it would be
necessary to create transitions and corresponding target states in $P$
for all symbol pairs {\tt a:b}, where each of the rules has
transitions from its initial state with symbol pair {\tt
  a:b}. Outwards $P$ would have one target state $t$ for the
transition with pair {\tt a:b} from its initial state. Internally $t$
would correspond to a configuration of the target states of the
transitions with symbol pair {\tt a:b} in each of the indiviual rules.

There is no caching of transitions in the states of $P$. Thus the
transitions in states have to be recomputed every time the algorithm
visits a given configuration of rule states. This requires more work
than if the transitions were cached, since there usually exist state
configuration which are very frequently visited.

Even if the transitions in states were cached, this implementation
would still be suboptimal, since a new state configuration always
requires re-examining the transitions in all rules. This is true even
though only one rule state would differ from a previous configuration.

\subsubsection{Current implementation}

Phonological two-level rules usually track sound changes in fairly
specific contexts. This means that when composing two-level rule
transducers with a lexicon, the rules will occupy a limited state set
during the majority of the time of the composition. The current
implementation of intersecting composition capitalizes on this
property.

Instead of a parallel lazy intersection like we used in hfst2, we
recursively build up an intersection of the rules by intersecting them
lazily pairwise. The first and second rule are intersected, the result
is intersected with the third rule and so on. The current
implementation caches the transitions in a give state pair, so there
is no need to recompute it when the state is revisited.

This leads to significant improvement in performance. The improvement
results from caching transitions, but it is improved by the fact that
computing the transitions in a previously unseen state configuration
does not require recomputing the transitions in all of the
rules. E.g. if rule number $n$ moves to a new state with symbol pair
{\tt a:b}, but the rest of the rules remain in a familiar state
configuration, we only need to recompute the transitions of the rules
having greater index than $n$. This results from the fact that we have
already cached the target state of the subset of rules $1$ to $n-1$ in
their lazy intersection structure.

Like the old implementation, the current implementation of
intersecting compose is equivalent with flag Xerox style diacritics
and IDENTITY symbols.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{l|c|c}
Language  & hfst3 & hfst2 \\
\hline
Northern S\'{a}mi  & 63s   & 364 s \\
\hline
Finnish   &       &
\end{tabular}
\vskip.2cm
\caption{Runtimes for intersecting composition of the Finnish and Northern S\'{a}mi morphological analyzers in hfst2 and hfst3.}
\end{center}
\end{table}

\section{Xerox Compatibility}
% TP

Among the one goals of HFST framework has always been to retain legacy support
for Xerox line of tools for building finite-state
morphologies~\cite{beesley/2003}. The support should also be trivial for
end-users converting from the Xerox tools. For this purpose we have aimed to
create clones of the most important Xerox tools as accurate as possible.
Previous open source implementations of Xerox tool clones have included the
lexc and twolc~\cite{linden/2009/sfcm} and lexc and xfst~\cite{hulden/2009}, in
HFST 3 we have combined these contributions to one uniform package capable of
handling full line of Xerox tools for morphology, which for most intents and
purposes means that end users will require no other work than changing program
names to start using HFST tools for their Xerox-style language description.


The actual implementation of xfst scripting language support makes heavy
use of the new foma backend that already included good coverage of the
xfst support. The only additions made in HFST are once required to support
interoperability between other back ends and HFST internals, specifically
care was taken not to duplicate the work already present in foma and its
tools. In similar effect the HFST's own lexc parsing engine was mainly
replaced by the faster parser in foma, again with HFST interoperability
tweaks bridged in.

For examples of specific previously implemented Xerox style finite-state
language descriptions we provide a wiki-based web page\footnote{\url{https://kitwiki.csc.fi/twiki/bin/view/KitWiki/HfstExamples}}. Another repository of
such language descriptions is located at university of Tromsø's subversion
repository\footnote{\url{http://divvun.no/doc/infra/anonymous-svn.html}}.
Of these at least variants of Sámi and Greenlandic are regularly
used to regress and stress test HFST tools.

For specific functionality, the Xerox tools have lots of specific special
processing of finite-state automata, that is not part of standard finite-state
algorithms. Primary example of this is handling of special symbols such as
flag diacritics~\cite{beesley/1998}, which would require support from underlying
libraries for many finite-state operations to work as they do in Xerox tools
when using \texttt{flag-is-epsilon} and \texttt{obey-flags} settings. In
HFST tools we have provided support for same options and provided fall-back
processing where it is possible to have backend libraries support required
operations. The fallback commonly means converting the end library
automata to HFST's internal format, calculating the operation out internally
and convert back to end libraries format. 

\section{Compilation Performance}
% EA

The performance of HFST has improved from version 2.0 to 3.0. 
We compiled two finite-state morphologies in SFST programming language format
with HFST versions 2.0 and 3.0. 
The morphologies were OMorFi~\cite{pirinen/2008} for Finnish and 
Morphisto~\cite{zielinski/2009} for German.
In table~\ref{tab:compilation_times} are the compilation times 
for both morphologies with 
different backend implementations with both versions of HFST. 
Note that the foma implementation was not available in version 2.0.

\begin{table}
\centering
  \begin{tabular}{ c | c | c | c }
  \multicolumn{4}{c}{Compilation times} \\ \hline
  Backend & version & Finnish & German \\ \hline
  \multirow{2}{*}{SFST} & 2.0 & 25:16 & 107:47 \\
  & 3.0 & 5:02 & 6:39 \\ \hline
  \multirow{2}{*}{OpenFst} & 2.0 & 7:54 & 6:23 \\
  & 3.0 & 6:51 & 6:28 \\ \hline
  \multirow{2}{*}{foma} & 2.0 & - & - \\
  & 3.0 & 1:49 & 1:29 \\
  \end{tabular}
  \caption{Compilation times for Finnish and German morphologies with
    HFST. The times are expressed in minutes and seconds.}
  \label{tab:compilation_times}
\end{table}

We can clearly see that the compilation time has improved dramatically
for the SFST implementation.
This is mainly because the new version of SFST, 1.4.2, uses Hopcroft's
minimization algorithm~\cite{hopcroft/1971} instead of 
Brzozowski's~\cite{brzozowski/1964}. 
We noticed how the minimization algorithm affects performance
already when we were testing HFST version 2.0; 
OpenFst was clearly faster because it uses Hopcroft's algorithm. 
Based on this observation, Helmut Schmid could improve his SFST by 
writing a minimization function that uses Hopcroft's algorithm.

When comparing the compilation times for OpenFst, we see that the
Finnish morphology compiles faster but the German one slightly slower
on HFST version 3.0 than on version 2.0. This is because there are two
factors that contribute to the difference in performance. Firstly, we
are currently using OpenFst version 1.2.7 that is faster than the
previous versions. Secondly, in HFST version 3.0 we no longer use the
same number-to-symbol encodings for all transducers during the same
session. Every time we perform a binary operation on two transducers,
we must harmonize the encodings of the transducers. Nevertheless, 
it seems that the newer, more efficient version of
OpenFst compensates well for this slowness caused by harmonization. 

We did not have the foma implementation available in HFST version 2.0,
but it is evident that it is much faster than the other
implementations in either version of HFST. Foma does not either use
the same symbol-to-number encodings in its transducers, but it still
performs well. It is presumable that symbol harmonization is not a big
factor in the compilation times of morphologies. 


\section{Application Areas}
% TP

After initial release of HFST platform it has been used in several end-product
application software. The two most prominent uses are as a part of rule-based
machine translation platform in apertium\footnote{\url{http://apertium.sf.net}}
and spell-checking library voikko\footnote{\url{http://voikko.sf.net}}. Both of
these linguistic applications take a huge benefit from the fact that there
were available previous language descriptions written in Xerox finite-state
morphology formalism and integrating HFST to these applications gave
developers of those language descriptions direct conversion path to two
application types that had not been available in the Xerox framework. Since
both applications as well as HFST framework are free and libre open source
software, the integration of the existing language descriptions to the
projects was easily possible.

One of the most pressing reason for extending finite-state support to these
applications is the lack of language support and lack of even theoeretical
possibility of language support for morphologically more complex languages
in those application fields. For example in field of spell-checkers the
theoretical bounds of hunspell---de facto standard in open source market---
is mere 4 affixes. For polyXXXXXX languages like Greenlandic, it simply is
not possible to precompute enough combinations out, like has been done with
e.g. Hungarian. The Xerox style finite-state morphology demonstrably supports
at least Hungarian and a wide variety of other morphologically varying
languages~\cite{beesley/2003}.

Another rationale for extending finite-state methods to these application areas
is that the efficiency and expressiveness of finite-state automata has been
well-known and researched e.g. in \cite{aho/2007}, which makes it a good choice
for various text-processing tasks.

\subsection{Apertium interoperability---corpus processing tools and I/O formats}

In apertium the finite-state automata are used to provide language models for
both parsing the running text and generating the translations after performing
a mid-shallow rule-based transfer. The HFST is only one possible language
model, so the crucial part for inclusion was to get HFST language models
work as others. This includes two functions: reliable tokenization based on
the dictionary data and support for apertium I/O formats. The total
contribution of this corpus processing functionality is contained in tool
called \texttt{hfst-proc} included in HFST toolkit. The name originates from
the interoperability with other toolkits' corpora processing tools, specifically
\texttt{cg-proc} from vislcg3\footnote{\url{http://beta.visl.sdu.dk/cg3.html}} and
\texttt{lt-proc} from apertium itself.

For tokenization the FST-based dictionaries are useful, since the process of
analysis and lookup can be both performed by basic FST traversal. The specific
implementation of analysis and tokenisation with single FST traversal was
implemented as Google summer of code project, based on previous studies on
topic~\cite{garrido-alenda/2002}. The basic program logic of automata traversal
for longest matches is trivially extended here by processing of flag diacritics
and weights.

The I/O format requirements for apertium platform are based on the needs to
translate existing documents containing all kinds of markup and rich text
formats, such as html for web pages or mediawiki codes from wikipedia. To
achieve this apertium uses text encoding and decoding mechanisms and
interchange format called apertium stream format, whose input and output
was implemented in hfst's corpus processing tools.

\subsection{Voikko and HFST based spell-checker formulation}\label{spellcheck}

The application of finite-state language models for spell-checking applications
is also based on new development of finite-state algorithms. The application
framework including HFST spell-checkers is voikko library, which provides
spell-checkers for OpenOffice.org/LibreOffice, the GNOME desktop (via
enchant), Mac OS X (via SpellService) and Mozilla application suite.

The finite-state formulation of spell-checking system was developed based on
previous research. In this research it has been shown that a finite-state based
natural language description is usable as spell-checker with specialized fuzzy
traversal algorithms~\cite{oflazer/1996,hulden/2009} or by using a special
(weighted) two-tape automaton as error model and regular finite-state
composition to map misspelled words to their possible
corrections\cite{agata/2002,pirinen/2010/lrec}.
 
The basic finding here is that typical finite-state language descriptions are
usable as spell-checking dictionaries with minor to no modifications.
Furthermore it has been shown that pre-existing non-finite-state spell-checking
dictionaries from hunspell and myspell can be converted to finite-state
form~\cite{pirinen/2010/cla} providing full backwards compatibility for
traditional spell-checking systems.

Furthermore we have optimised the application of error models by composition
using parallel composition with both the dictionary and the misspelled word in
one operation. This reduces the weight and time requirements by leaving the
intermediate results out of the system.

\section{Optimized Lookup}\label{optimized-lookup}
% SH
\emph{Optimized-lookup} is a HFST-specific binary format for finite-state
transducers providing fast lookup. First documented in \cite{silfverberg/2009},
its implementation has evolved somewhat to meet the requirements of specific
applications. The format has also found new application in on-the-fly
operations in transducers, eg. composing lookup for spell-checking and
correction (see section \ref{spellcheck}).

\subsection{Implementation and Integration in HFST2 and HFST3}
Optimized-lookup was supported in HFST2 by the standalone utilities
\verb+hfst-lookup-optimize+ (compilation to the format) and
\verb+hfst-optimized-lookup+ (non-tokenizing lookup). This was partly
in service of the goal to give optimized-lookup the widest possible range
of uses; \verb+hfst-optimized-lookup+ was released under the
Apache License~\cite{apache-license}, whereas HFST2 proper was released
under the potentially more restrictive GNU Lesser Public
License~\cite{lgpl-license}.

Provision was later made for both unweighted and weighted (with log weights)
transducers, and flag diacritics~\cite{beesley/2003} (also see section
\ref{flag-diacritics}). Demonstrations of the lookup facility were also
produced in Java and Python, two popular and accessible programming languages,
in the hope of facilitating and spreading use of the format. This effort
bore fruit in the incorporation of the Java code in a project for anonymizing
identities in legal decisions at Aalto University.

Over the course of furthering research goals and maintaining HFST2,
the format saw additional uses and implementations. Hyphenators and spell
checking transducers were primarily used in this format for its speed, and
in 2010 a Google Summer of Code project by Brian Croom produced
\verb+hfst-proc+, a tokenizing lookup application for optimized-lookup which
was put to use in various text stream processing scripts (eg.
\verb+-analyze+ for analysis, \verb+-generate+ for generation and
\verb+-hyphenate+ for hyphenation).

Originally compilation to the format was only possible from SFST and OpenFst
formats, and as uses and applications proliferated, it became desirable to
provide some API access to optimized-lookup in the HFST library. In HFST3
this has been accomplished by implementing compilation from HFST's internal
transducer format, allowing for a great degree of integration with
HFST3-supported tools.

\subsection{Index Table Compaction}
The critical idea behind optimized-lookup is Liang compaction, as described in
\cite{silfverberg/2009}. It allows for the representation of a transducer as
a lookup table, with entries for each symbol in the alphabet for each state
in the transducer, without growing to the prohibitive sizes such a design
would imply (a multiple of the number of states and the number of symbols for
the state indexing table alone). Liang in his PhD thesis on hyphenation
\cite{liang/1983} didn't specify a generalized compaction scheme, only the
requirements for its correctness. In realistic transducers, finding the optimal
packing is in any case computatinally infeasible, and so some approaches to
producing a ``good enough'' compaction have been attempted.

For the purposes of this article, the task of compacting the index table may
be summarized as the following. Given $N$ arrays $s$ of length $L$, the
entries of which are $0$ or $1$, calculate a list of starting indices
$I_1, I_2 \ldots I_N$ such that a result array with cells
\begin{equation}
R_i = \displaystyle\sum\limits_{p, q} s_p(q) [I_p + q = i]
\end{equation} will also have
entries $0$ or $1$. An optimally compacted index table corresponds to the
shortest result array $R$.

A simple strategy is to iterate through the arrays in some order, assigning
the lowest possible starting index to each one. For transducers of an
appreciable size this process can become slow, as the result array
will typically have some zeros in practically all its regions, so a large
number of possibilities have to be checked. This problem can be mitigated by
applying a head filter, disregarding the largest region of the result array
$R(1 \ldots r)$ with that region having a density of $1$ entries greater than
some predifined limit. A limit of $1.0$ corresponds to having no filter at all;
in practice limits in the region $0.8 \ldots 0.9$ have proved reasonable.

For the order in which the arrays are traversed, ordering the states from
greatest density of $1$ entries to lowest and simply ordering them in
numerical order have been tried. These approaches don't appear to produce
dramatically different results\footnote{Of course, in practice these orderings
will often be similar.}; also in theory, either approach could
produce better results than the other.

\begin{table}[htb!]
\begin{center}
\begin{tabular}{c|r}
Head filter limit  & Number of index entries \\
\hline
(No compaction) & 4,541,879 \\
\hline
0.0 & 1,107,321 \\
\hline
0.1 & 408,843 \\
\hline
0.3 & 206,152 \\
\hline
0.5 & 170,789 \\
\hline
0.7 & 155,703 \\
\hline
0.8 & 148,740 \\
\hline
0.9 & 140,795 \\
\hline
1.0 & 135,285 \\
\end{tabular}
\vskip.2cm
\caption{Index table sizes for various values of the head filter limit as applied to the Morphalou project's French morphology and released on HFST's site on 2010-04-14.}
\end{center}
\end{table}

Work is ongoing in finding the best practical filter strategies, filter
limits and traversal orders, and potentially different compaction strategies.

A representation of the index table with no compaction would require
$N \times L$ entries. For the sake of comparison, in the case of an analyzing
transducer from OmorFi \footnote{We used the binary released on HFST's site on
2010-10-14.}, a Finnish finite-state morphology, this is
$203851 \times 155 = 31,596,905$ and would produce a binary of almost 200
megabytes, whereas the compacted index table has 364,980 entries, or about
one percent of the uncompacted form, for a binary of 7 megabytes.

\subsection{Flag Diacritics and Related Optimization Tricks}\label{flag-diacritics}
Support for flag diacritics was added to the \verb+optimized-lookup+ utility
with an eye to efficiency, prompting some refinements to HFST's implementation
of the format itself. Flag diacritics are parsed prior to lookup and during
it they restrict the lookup search tree. This is critical for speed, as the
alternative of calculating all the outputs first and then removing outputs
with conflicting flag diacritics can, in the case of transducers with liberal
use of flags, involve several times the work.

For the purposes of traversing transitions, flag diacritics are essentially
a special case of the epsilon symbol. If the configuration of flags
that have been traversed up to a certain point allow it, each transition
with a flag diacritic is traversed without reading an input symbol. With
this in mind, it was desirable to avoid checking for transitions with each
flag symbol in each state. The optimized-lookup format was therefore amended
to treat flag diacritics as epsilon for purposes of constructing the index
table and to list their transitions out of the normal order, after the epsilon
transitions. Thus it is possible to check only those flag transitions that
are present in a given state.

This allows further reductions in the size of the index table; as
the indexes for flag diacritics are no longer in use, it is possible to reduce
the effective size of the input alphabet by the number of different flag
diacritic operations. These improvements may appear minor, but are not
insignificant in transducers that make substantial use of flag diacritics. In
the previously discussed version of OmorFi, we achieved a reduction of $12.4%$
in index table size.

\section{Discussion}
%KL

\section{Conclusion}
% KL

%
%
%

\bibliographystyle{splncs03}
\bibliography{sfcm-2011}

\end{document}
% vim: set spell
