\documentclass[11pt]{article}

\usepackage{acl2012}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{ifpdf}

\ifpdf
\pdfinfo{}
\fi

\title{Effect~of~Language~and~Error~Models on Efficiency~of~Finite-State
Spell-Checking~and~Correction}

\author{
Tommi A Pirinen\\
University of Helsinki\\
Department of Modern Languages\\
FI-00014 Univ. of Helsinki, PO box 24\\
{\tt tommi.pirinen@helsinki.fi} \\\And
Sam Hardwick\\
University of Helsinki\\
Department of Modern Languages\\
FI-00014 Univ. of Helsinki, PO box 24\\
{\tt sam.hardwick@helsinki.fi} \\
}

\date{\today (draft)}

\begin{document}

\maketitle
\begin{abstract}
In this paper we inspect the effects of different finite-state language and
error models to speed of finite-state spell-checking. We evaluate the speed and
memory consumption of full-scale real-world finite-state spell checkers for
three languages covering the broadest available spectrum of morphological
complexity; English, Finnish and Greenlandic. We evaluate the effects of
different finite-state error models and optimisations against contemporary
software-driven spell-checkers with same or equivalent spell-checking features.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

In most applications of spell-checking, efficiency is the key factor for
selecting or discarding spell-checking solutions. In case of finite-state
spell-checking it is well-known and well-researched, that finite-state language
models can efficiently encode dictionaries of natural
languages~\cite{beesley/2003}, even for languages with rich and complex
morphological systems, such as polysynthetic languages. The efficiency of
finite-state approaches to spelling correction, however, has often been seen as
a problem, and therefore most of the contemporary spell-checking and correction
system are still based on programmatic solutions (e.g.
hunspell\footnote{\url{http://hunspell.sf.net}}, and its *spell relatives), or
at most specialised algorithms for implementing error-tolerant traversal of the
finite-state dictionaries~\cite{oflazer/1996,hulden/2009}. There has also been
few implementations of using fully finite-state implementations for both the
error-detection and corrections~\cite{schulz/2002,pirinen/2010/lrec}. In this
paper we further evaluate the use of finite-state dictionaries with two-tape
finite-state automatons for mechanism to correct misspellings, and how to
optimise the finite-state error models.

To evaluate general usability and efficiency of finite-state spell-checking we
test the system with three languages of typologically different morphological
features and different reference implementations for contemporary
spell-checking applications: We use English as a sample for morphologically
poor language with basically plain word-list approach to spell-checking; for
reference implementation we use the hunspell's en-US
dictionary\footnote{\url{http://wiki.services.openoffice.org/wiki/Dictionaries}}
and for finite-state version we use weighted word-list
from \newcite{norvig/2010}.  The second language we test is Finnish, a
morphologically more rich language which is still agglutinative. Finnish is
among the morphologically simpler languages that have no working implementation
under hunspell, and for reference implementation we use
voikko\footnote{\url{http://voikko.sf.net}}, with LAG-based dictionary using
malaga\footnote{\url{http://home.arcor.de/bjoern-beutel/malaga/}}. Our third
test language is Greenlandic, a polysynthetic language which is implemented as
finite-state system using Xerox's original finite-state morphology
formalism~\cite{beesley/2003}. For open source implementation we use
HFST\footnote{\url{http://hfst.sf.net}}. There are no free software for
reference implementation apart from our solution, as far as we know.

The baseline feature-set and the efficiency of spell-checking we are targeting
is defined by the currently de facto standard spelling suite in open source
systems, hunspell. As neither Finnish nor Greenlandic have been successfully
implemented in hunspell formalism, we mainly use them to evaluate how complexity
of language model affects the efficiency of finite-state spell-checking. For
full-scale survey on the state-of-the-art non-finite-state spell-checking we
refer to \newcite{mitton/2009}.

The efficiency results are contrasted to the existing research on the
finite-state spell-checking in \newcite{hassan/2008} and the theoretical
results on finite-state error-models in \newcite{mitankin/2005} with the
additions of the morphologically complex languages with actual cyclic
dictionary automata and more complex structure in general, than those of
English and Arabic. In particular we show, that the previous approaches have
neglected to simultaneously constrain the error model and the dictionary with
each other in on-line composition, which affords a significant speed benefit.

The rest of the paper is organised as follows. In Section~\ref{sec:methods} we
introduce methods used by current non-finite-state spell-checkers and
previously used finite-state methods for spell-checking and correction and
propose some possible speed optimisations for the error models. In
Section~\ref{sec:materials} we present the language models, error models, the
testing corpora and the earlier work. In Section~\ref{sec:evaluation} we
present the comparisons of speed and quality with combinations of different
language and error models and corpora for spell-checking. In
Section~\ref{sec:conclusions} we summarise our findings and results for use
of finite-state spell-checking.


\section{Methods}
\label{sec:methods}

A finite-state spell-checker is typically composed of at least two finite-state
automata; one for dictionary of the language, or the \emph{language model},
which contains valid strings of the language, and one automaton to map misspelt
words into correct strings, or the \emph{error model}. Both the language models
and the error models are usually weighted as finite-state automata, in which
case it is probability of word being correctly spelled in the language model
and the probability of specific misspellings that are assigned by the automata.
We evaluate here the effect of both the language model automaton's structure
and complexity, and the error models structure and complexity to the efficiency
of the finite-state spelling.\footnote{The methods introduced in this research
as well as all materials are free/libre open source. Please see our svn
%\url{https://hfst.svn.sf.net/svnroot/trunk/fsmnlp-2012-spellers/}
repository for detailed implementation and scripts to reproduce all the
results.}

\subsection{Language Models}
\label{subsec:language-models}

The most basic language model for spell-checking dictionary is a list of
correctly spelled word-forms. One of the easiest way to create such 
spell-checker is collecting the word-forms from reasonably large corpus of
mostly correctly spelt texts; additionally we can count the frequency of words
and use that as likelihood $P=\frac{c(\mathrm{wordform})}{\mathrm{Corpus size}}$
For morphologically poor languages such as English, this is often sufficient
approach~\cite{norvig/2010}, and we use it to create dictionary for our English
spell-checking dictionary as well.

For morphologically slightly richer languages like Finnish, for which the
word-list approach is likely to miss much greater amount of words, one of the
commonest approaches is to use right-linear grammars, possibly combined with
finite-state rule languages to implement morphophonological
alterations~\cite{koskenniemi/1983}. This approach also applies to the newest
available free open source and full fledged finite-state Finnish morphological
dictionary we found~\cite{pirinen/2011/nodalida}. This language model features
productive derivations, compounding and rudimentary probabilistic models from.
The reference non-finite state language model for Finnish we found from
currently used spell-checker is implemented in left-associative grammar
formalism, a potentially slightly less efficient system with more expressive
power and similar feature set as finite-state formulation in terms of
linguistic coverage.

For polysynthetic languages it will be obvious that coverage of any
word-list-based approach will be even lower, furthermore most simple extensions
to it such as affix stripping of hunspell are not enough for describing
word-forms of the language. To our knowledge, the only approaches that have
been in wide use for spell-checking and morphological analysis of Greenlandic
have been based on traditional finite-state solutions like the Xerox formalisms
as with Finnish finite-state dictionaries.

\subsection{Error Models}
\label{subsec:error-models}

The ubiquitous formula for modeling typing errors since the beginning of the
spelling correction by computer has been the edit-distance algorithm sometimes
attributed to \newcite{levenshtein/1966} and/or
\newcite{damerau/1964}. What the algorithm does is it maps four most
typical slips of fingers on keyboard to events in the fuzzy matching of the
misspelt word-forms to correct ones, that is, deletion of character (i.e. not
pressing a key), addition of a character (i.e. pressing extra key
accidentally), changing a character (i.e. pressing wrong key) and swapping
adjacent characters (i.e. pressing two keys in wrong order). When modeling edit
distance as finite-state automaton, a relatively simple two-tape automaton is
enough to implement the algorithm~\cite{hassan/2008}.  Basically the automaton
will consist one arc for each type of error, additionally one state for each
swapping of character type of error is needed to keep the character pair to be
swapped in memory. This means that the trivial NFA implementing the algorithm
is of space complexity $S(V, E, \Sigma) = O(|\Sigma|^2 |V| + |\Sigma|^2 |E|)$,
where $\Sigma$ is the alphabet of language, $V$ is the set vertices in
automaton and $E$ is the set of edges in automaton.  This edit distance
formulation is roughly feature equivalent to hunspell's \texttt{TRY} mechanism.

To further fine-tune this finite-state formulation of edit distance algorithm,
it is possible to attach a probability to each of the error events as a weight
in weighted finite-state automaton, which corresponds the likelihood of error
or confusion factor. This can be used to implement features like keyboard
adjacency or OCR confusion factor to the error correction model. This will not
modify the structure of the finite-state error models or the search space, but
introduction of non-homogenous weights to resulting finite-state network may
have an effect on search time. This addition is equivalent to hunspell's
\texttt{KEY} mechanism.

For English language spelling correction there is also additional form of
error model to deal with competence related misspellings, as opposed to these
other models that mainly deal with mistypings, implemented in form of phonemic
folding and unfolding. This type of error is very specific to certain type of
English texts and is not dealt with in scope of this experiment. This is what is
the \texttt{PHON} part of the hunspell's correction mechanism.

After fine-tuning the error models to reimplement hunspell's feature set, we
use propose different variations of this edit-distance scheme to optimise the
speed of error correction with little or no negative effect to the quality of
the correction suggestions. The factors that make up the time of the
finite-state spelling correction algorithm are the size of the search space,
i.e. the complexity of resulting network when the error model is applied to the
misspelt string and intersected with the dictionary\footnote{correspondingly
for non-finite-state solution, the search space is simply amount of the
possible strings given the error corrections made in the algorithm, for
finite-state system the amount of generated strings with cyclic language and
error models is infinite so complexity calculation is theoretically slightly
more complex, however for basic edit distance implementations used in this
article the search space complexities are always the same and amount of
suggestions generated finite}.

To optimise the application of edit distance by limiting the search space, many
traditional spell checkers will not attempt to correct the very first letter of
the word-form. We investigated whether this decision is a particularly
effective way to limit the search space, but it does not appear to
significantly differ from restricting edits at any other position in the input.
Presumably, the rationale is a belief that errors predominately occur at other
positions in the input. As far as we know, the complete justification for this
belief remains to be made with a high-quality, hand-checked error corpus.
On the error-model side this optimisation has been justified by findings where
between 1.5~\% and 15~\% of spelling-errors happen in the first character of
the word, depending on the text type~\cite{Bhagat/2007};
the 1.5~\% from small corpus of academic texts~\cite{Yannakoudakis/1983}
and 15~\% from dictated corpora~\cite{Kukich/1992}. We also performed a
rudimentary classification of the errors in the small error corpus of
333 entries from~\newcite{pirinen/2011/cicling}, and found errors at
first position in 1.2~\% of the entries, furthermore we noticed that
when evenly splitting the word-forms in three parts, 15~\% of the errors are
in the first third of the word-form, while second has 47~\% and third 58~\%,
which would be in favor of selecting the error model to discard initial
errors\footnote{by crude classification we mean, that all errors were
forced to one of the three classes at weight of one, e.g. series of
three same letters was counted as deletion at first s position.}.

Second form of optimisation that is used by many traditional spell-checking
systems as well, is to apply the lower order edit distances separately before
trying higher order ones. This is based on the assumption that vast majority
of spelling errors will be of lower order, as is given in the original
description of the edit distance for spell-checking, 80~\% of the spelling
errors can be corrected~\cite{Pollock/1984}.

Third form of the optimisation that we test is an attempt to decrease the
number of redundant corrections in error models of higher order edit distances
than one. This means that things like  adding and deleting the
same character in successive moves will not be performed. This makes the error
model more complicated but reduces the search space, and does not affect the
quality of results at all.

\subsection{Algorithms}
\label{sec:algorithms}
The obvious baseline algorithm for the task of finding which strings can be
altered by the error model in such a way that the alteration is present in the
language model is generating all the possible alterations and checking which
ones are present in the language model. This was done in \newcite{hassan/2008}
by first calculating the composition of the input string with the error
model and then composing the result with the language model.

If we simplify the error model to one in which only substitutions occur, it can
already be seen that this method is quite sensitive to input length and
alphabet size. The composition explores each combination of edit sites in the
input string (i.e. $\sum_{i=1}^d n \choose i$, where $n$ is the input length
and $d$ is the edit distance) and each edit for each site ($|\Sigma| - 1$, the
entire alphabet except for the actual input). This results in an $O(n^d \cdot
|\Sigma|)$ algorithm, or for an English 26-letter lowercase alphabet, swap
distance $2$ and the $8$-letter word ``spelling'', $700$ strings that are
stored in a transducer. With transpositions, deletions, insertions and edit
weights this grows to $100,215$ different combinations of output and weight. We
have simulated this algorithm in our results by generating the edited strings
by lookup, and performing another lookup with the language model on these
strings.

Plainly, it would be desirable to improve on this. The intuition behind our
improvement is that when editing an input string, say ``spellling'', it is a
wasted effort to explore the remainder after generating a prefix that is not
present in the lexicon. For example, after changing the first character to ``z''
and not editing the second characted, we have the prefix ``zp-'', which does
not occur in our English lexicon. So the remaining possibilities - performing
any remaining edits on the remaining $7$-character word - can be ignored.

This is accomplished with a three-way composition, in which the input, the
error model and the language model simultaneously constrain each other to
produce the legal correction set. This algorithm is presented in some detail
in \newcite{hfst/2012/cla}.

\section{Material}
\label{sec:materials}

For language models we have selected to get suitable dictionaries from the
Internet by seeking freely available dictionaries for languages of
typologically varied languages.

For creating error models we have created new implementations of the algorithms
to create and modify finite-state error models. For baseline we have created
the basic edit-distance error models by hand and then modified them
automatically to test different variants.

To test the effect of correctness of the source text to speed of
spelling-checker we have fetched of largest freely available open source text
materials from the internet, i.e. Wikipedia. The Wikipedia text as itself is
good for real-world material as it does contain wide variety of spelling
errors. For a material with more errors, we have used simple script to
introduce the (further) errors at uniform probability of 1/33 per character;
using this method we can also obtain a corpus of errors with correct
corrections along them.  Finally we have used a text corpus from language
different than the one being spelled to ensure that majority of words are not
in vocabulary and not fixable by standard error models, which should provide to
be the worst case scenario test for the spell-checking.

The corpora statistics are as follows: the English Wikipedia
dump\footnote{\url{http://dumps.wikimedia.org/enwiki/20120211/enwiki-20120211-pages-articles.xml.bz2}}
is 34 GiB, the Finnish
Wikipedia\footnote{\url{http://dumps.wikimedia.org/fiwiki/20120213/fiwiki-20120213-pages-articles.xml.bz2}}
is 1 GiB, and the Greenlandic
Wikipedia\footnote{\url{http://dumps.wikimedia.org/klwiki/20120214/klwiki-20120214-pages-articles.xml.bz2}}
is only 7 MiB. From these we have extracted the contents of the articles and
picked 100,000 first word tokens for evaluation purposes.

In Table~\ref{table:dictionary-sizes} we summarize the sizes of automata in
terms of structural elements. On the first row, we give the effective size of
alphabet needed to represent the whole dictionary, this is all the Unicode
code points that have been used in the dictionary, including alphabets, 
punctuation and spaces. Next we give the sizes of automata as nodes and
arcs of the finite-state automaton encoding the dictionary. Last we give the
size of the automaton as serialised on the hard disk, while this is not the
same amount of memory as its loaded data structures, it gives some indication
of memory usage of the automaton in running program. As can be clearly
seen from the table, the morphological complexity correlates quite directly
in all of structural elements of automaton.

\begin{table}[h]
\begin{center}
\begin{scriptsize}
\begin{tabular}{|l|rrr|}
\hline
\bf Automaton & \bf En & \bf Fi & \bf Kl  \\ 
\hline
$\Sigma$ set size &
 43& 117& 133
\\
Dictionary FSM nodes &
 49,778& 286,719& 628,177
\\
Dictionary FSM arcs &
 86,523& 783,461& 11,596,911
\\
Dictionary FSM on HDD & 
2.3 MiB &
43 MiB &
290 MiB
\\
\hline
\end{tabular}
\end{scriptsize}
\end{center}
\caption{\label{table:dictionary-sizes}
The sizes of dictionaries as automata}
\end{table}

In the Table~\ref{table:error-sizes} we give the same calculations for the
sizes of error models we've generated. The sigma set size row here shows the
amount of alphabets left, when we have removed the parts of alphabet from the
error models that is usually not considered to be part of spell-checking
mechanism, such as all punctuation that does not occur word-internally and
white-space characters\footnote{The method described here does not handle
run-on words or extraneous spaces, as they introduce lot of programmatic
complexity which we believe is irrelevant to the results of this experiment.}.
Note, that sizes of error models can be directly computed from its parameters;
i.e., the distance, the $\Sigma$ set size and the optimisation, this table
is provided for reference only.

\begin{table}[h]
\begin{center}
\begin{scriptsize}
\begin{tabular}{|l|rrrr|}
\hline
\bf Automaton & \bf En & \bf Fi & \bf Kl  \\ 
\hline
$\Sigma$ set size &
 28& 60& 64
\\
Edit distance 1 nodes &
 652& 3308& 3784
\\
Edit distance 1 arcs &
 2081& 10209& 11657
\\
Edit distance 2 nodes &
 1303& 6615& 7567
\\
Edit distance 2 arcs &
 4136& 20360& 23252
\\
Edit distance 3 nodes &
 1954& 9922& 11350
\\
Edit distance 3 arcs &
 6191& 30511& 34847
\\
No first edits ed 1 nodes & 
 652& 3308& 3784
\\
No first edits ed 1 arcs & 
 2107& 10267& 11719
\\
No first edits ed 2 nodes &
 1303& 6615& 7567
\\
No first edits ed 2 arcs &
 4162& 20418& 23314
\\
No first edits ed 3 nodes &
 1954& 9922& 11350
\\
No first edits ed 3 arcs &
 6217& 30569& 34909
\\
Avoid redundancy and firsts ed 2 nodes &
 1303& 6615& 7567
\\
Avoid redundancy and firsts ed 2 arcs &
 4162& 20418& 23314
\\
Avoid redundancy and firsts ed 3 nodes &
 3256& 16536& 18916
\\
Avoid redundancy and firsts ed 3 arcs &
 46102& 444342& 538474
\\
Lower order first ed 1 to 2 arcs &
add & above & rows
\\
Lower order first ed 1 to 2 nodes &
add & above & rows
\\
\end{tabular}
\end{scriptsize}
\end{center}
\caption{\label{table:error-sizes}
The sizes of error models as automata}
\end{table}


\section{Evaluation}
\label{sec:evaluation}

The evaluations in this section are performed on quad-core Intel Xeon E5450
running at 3~GHz with 64~GiB of RAM memory. The times are averaged over five
test runs in stable server environment with no server processes or running
graphical interfaces or other uses. The test results are measured using the
\texttt{getrusage} C function on a system that supports the maximum resident
stack size \texttt{ru\_maxrss} and user time \texttt{ru\_utime} fields. The
times are also verified with GNU \texttt{time} command. The results for
hunspell, voikkospell and foma processes are only measured with time and top.
The respective versions of the software are voikkospell 3.3, hunspell
1.2.14, and foma 0.9.16alpha.

In the Table~\ref{table:error-model-vs-language-speed} we measure the speeds of
the spell-checking process of the native language Wikipedia with real-world
spelling errors and unknown strings. The error model rows are defined as
follows: on the \emph{Reference impl.} row, we test the spell-checking speed of
the hunspell tool for English, and voikkospell tool for Finnish. On the
\emph{edit distance 2} row we use the basic traditional edit distance 2 without
any modifications. On the row \emph{lower order first}, we test applying the
lower order edit distance models first, then iff no results are given advancing
to higher orders up to edit distance 3. On the \emph{No first edits} row we
use the error model that may not modify the first character of the word or the
first character after compound word boundary. On the \emph{Avoid redundancy}
row we use the error model edit distance 2 with the redundant edit combinations
removed. On the \emph{Avoid redundancy and first edits} rows we use combined
error model of \emph{No first edits} and \emph{Avoid redundancy}
functionalities.  In the tables and formul\ae we routinely use the language
codes to denote the languages: \emph{en} for English, \emph{fi} for Finnish and
\emph{kl} for Greenlandic (Kalaallisut). 

\begin{table}[h]
\begin{center}
\begin{scriptsize}
\begin{tabular}{|l|rrr|}
\hline
\bf Error model & \bf En & \bf Fi & \bf Kl \\ 
\hline
Reference impl. &
9.93&7.96&11.42
\\
Generate all edits 2 & 
37.75&1177.55&362.03
\\
\hline
Edit distance 1 &
0.26&6.78&4.79
\\
Edit distance 2 &
7.55&220.42&568.36
\\
Edit distance 3 &
140.96&---&---
\\
No first edits ed 1 & 
0.44&3.19&3.52
\\
No first edits ed 2 &
1.38&61.88&386.06
\\
No first edits ed 3 &
8.25&---&---
\\
Avoid redundancy ed 2 &
7.52&4230.94&6420.66
\\
Avoid redundancy ed 3 &
143.64&7.91&---
\\
Avoid redundancy and first edits ed 2 &
1.51&62.05&386.63
\\
Avoid redundancy and first edits ed 3 &
---&---&---
\\
Lower order first ed 1 to 2 &
4.31&157.07&545.91
\\
\end{tabular}
\end{scriptsize}
\end{center}
\caption{\label{table:error-model-vs-language-speed} Effect of language and 
error models to speed (time in seconds per 10000 word forms)}
\end{table}

The results show that not editing the first position does indeed give
significant boost to the speed, regardless of language model, which is of course
caused by the significant reduction in search space. However, the
redundancy avoidance does not seem to make reasonable difference, this is most
likely because the amount of duplicate paths in the search space is not so
proportionally large and their traversal will be relatively fast. The separate
application of error models gives the expected timing result between its
relevant primary and secondary error models.

In the Table~\ref{table:error-model-vs-language-memory} we measure the memory
consumption when performing the same tests. The tests here are also performed
using unmodified native language corpora and with same language and error
models as with previous tests.

\begin{table}[h]
\begin{center}
\begin{scriptsize}
\begin{tabular}{|l|rrr|}
\hline
\bf Error model & \bf En & \bf Fi & \bf Kl \\ 
\hline
Edit distance 1 &
13.5 MiB&0.2 GiB&1.6 GiB
\\
Edit distance 2 &
13.6 MiB&0.2 GiB&1.6 GiB
\\
Edit distance 3 &
17.1 MiB&0.3 GiB&---
\\
No firsts ed 1 & 
13.5 MiB&0.2 GiB&1.6 GiB
\\
No firsts ed 2 &
13.6 MiB&0.2 GiB&1.6 GiB
\\
No firsts ed 3 &
13.7 MiB&0.2 GiB&---
\\
No redundancy or firsts ed 2 &
13.6 MiB&0.2 GiB&1.6 GiB
\\
No redundancy or firsts ed 3 &
---&---&---
\\
Lower order first ed 1 to 2 &
13.8 MiB&0.2 GiB&1.6 GiB
\\
\hline
\end{tabular}
\end{scriptsize}
\end{center}
\caption{\label{table:error-model-vs-language-memory} Effect of language and 
error models to memory consumption}
\end{table}

The memory measurements show an interesting feature of our method of applying
the spell-checking and correction here; the memory usage stays nearly same
regardless of the selected error model. We assume that the memory figures shown
in the table are dominated by the memory stamp of the dictionary automaton and
the error model automaton loaded into memory, and the intermediate structures
used by the error-applying traversal of the automaton will not typically
contribute much (approx. half mebibyte for each larger edit distance), since
they are created and deleted on-the-fly. Notable discrepancy to this pattern is
Greenlandic with larger edit distances, it seems to exhaust whole memory which
may be caused an inadvertent (epsilon) loop in the morphology. One practical
thing to note here is, the memory footstamp does give suggestion of how much
available RAM is needed to get speed measurements of the
table~\ref{table:error-model-vs-language-speed}, as our experience shows that
if the dictionary automaton is partially in swap memory, the speed will
decrease by order of magnitude in current operating systems.

To measure the degradation of quality when using different error models we
count the proportion of suggestion sets that contain the correct correction
among the corrected strings. For this test we use automatically generated corpus
of spelling errors to get the large-scale results. For English and Finnish we
have also used smaller set of original errors in the unmodified corpora that
were hand-corrected by native speaker; for Greenlandic we did not have a native
speaker available for this purpose. The numbers from real-world errors are 
indicated in parentheses.

\begin{table}[h]
\begin{center}
\begin{scriptsize}
\begin{tabular}{|l|rrr|}
\hline
\bf Error model & \bf En & \bf Fi & \bf Kl\\ 
\hline
\hline
\hline
Edit distance 1 &
0.89&0.83&0.81
\\
Edit distance 2 &
0.99&0.95&0.92
\\
Edit distance 3 &
1.00&0.96&---
\\
No first edits ed 1 & 
0.74&0.73&0.60
\\
No first edits ed 2 &
0.81&0.82&0.69
\\
No first edits ed 3 &
0.82&---&---
\\
\end{tabular}
\end{scriptsize}
\end{center}
\caption{\label{table:error-model-vs-language} Effect of language and 
error models to quality}
\end{table}

This test with automatically introduced errors shows us that with uniformly
distributed errors the penalty of changing error model to ignore the
word-initial could be significant.

Finally we measure how the text type used will effect the speed of the
spell-checking. As the best-case scenario we use the unmodified texts of
Wikipedia, which contain probably the most realistic native language
speaker-like typing error distribution. For text with some more errors, where
majority of errors should be recoverable we use our automatically generated
errors in the Wikipedia texts. Finally to see the performance on worst case
scenario where most of the words have unrecoverable spelling errors we use the
texts from other language, in this case English texts for Finnish and 
Greenlandic spell-checking and Finnish texts for English spell-checking, which
will likely give us the nearly lower bounds of the performance. All of the
tests in this category were performed with error models under row
\emph{avoid redundancy and firsts ed 2} in previous tables, which gave us 
the best speed/quality ratio in the previous tests.

\begin{table}[h]
\begin{center}
\begin{scriptsize}
\begin{tabular}{|l|rrr|}
\hline
\bf Error model & \bf En & \bf Fi & \bf Kl  \\ 
\hline
\hline
Native Lang. Corpus &
1.38&61.88&386.06
\\
Added automatic errors &
6.91&946.19&551.81
\\
Text in another language &
22.40&148.86&783.64
\\
\end{tabular}
\end{scriptsize}
\end{center}
\caption{\label{table:language-vs-text-type} Effect of text type on
error models to speed}
\end{table}

The interesting thing to note here is, that this table doesn't really tell us
anything about anything. 


\section{Conclusions}
\label{sec:conclusions}

In this paper we have shown that finite-state spell-checking is suitable for
general use even with those language models that have been deemed
unimplementable for other contemporary methods of implementing spell-checking.

% \section*{Acknowledgements} 

% We thank the anonymous reviewers for their comments and the HFST research
% team for fruitless discussions on the article's topics.

\bibliographystyle{acl2012}
\bibliography{fsmnlp2012}

\end{document}
% vim: set spell:
